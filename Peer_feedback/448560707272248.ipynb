{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Group number 30\n",
    "Hróbjartur Höskuldsson\n",
    "vdh-406"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "The ER schema can be seen here: https://ibb.co/6vhnH6W \n",
    "\n",
    "When designing the schema I first took a look at a possible primary key. There are a couple of attributes that \n",
    "springs to mind: Id, Url, Title and Content. These are the only fields that will not often be repeated through \n",
    "out the data set. The Url can not be repeated which might be a good option, however, the Url can change, and \n",
    "there is no way to stop a website from changing its domain for instance. The title can also not be the primary key \n",
    "since there might be two articles with the same title. Content has the same problem, although extremely unlikely, \n",
    "it might happen that two authors write the excact same article.So now we are left with Id, which will never change, \n",
    "and never be the same. Now we have found a primary key and now we set up entity sets. The schema used in the data \n",
    "set contains three entity sets. Authors, Articles, and additional_information. The reason behind this is to seperate \n",
    "the article to only include information about the content it self, not the domain or authors etc. This will help us\n",
    "to look at the article it self only by the content it self. The same for Authors. We can look at the authors, \n",
    "see what, domain they post on, and their type of writing, without all the information on their specific articles. \n",
    "Since the data sets contains a list of news articles it only makes sence to have the entity set Articles to contain \n",
    "the primary key, Id. Looking at the ER model, we can see that the other two entity sets are weak entity sets. \n",
    "This is because their attributes can not distictively be a key in their own relation. So they need the help of their\n",
    "host entity set, Articles. In the set Authors, we have multiple soft keys, this is because the relation of the \n",
    "relationship Wrote, will now show a better picture of their relationship. The entity set additional_information \n",
    "only contains a single soft key, this is because none of its attributes could act as a better key. And the \n",
    "relationship Contains does not necciserally show a better picture then just the entity set additional_information.\n",
    "It would have been possible to have more entity sets then in this schema, but making it as simple as possible might\n",
    "have helped later, while doing queries. The data set was set to a single table in our data management system and \n",
    "then seperated to multiple tables according to the ER model. \n",
    "\n",
    "ER Model to relations:\n",
    "\n",
    "These are the queries used to upload create the relations from the ER model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import psycopg2\n",
    "\n",
    "connect = psycopg2.connect(\"dbname=testBase user=postgres password=******** host=localhost\")\n",
    "cur = connect.cursor()\n",
    "\n",
    "#Entity sets:\n",
    "#*articles entity set to relation*\n",
    "cur.execute(\n",
    "'CREATE TABLE articles'\n",
    "\t'AS(SELECT id, title, content, url, summary, tags, keywords'\n",
    "\t  'FROM news' \n",
    "\t   'WHERE id between 13 and 700013)'\n",
    "\n",
    "#*weak authors entity set to relation (includes ID)*\n",
    "'CREATE TABLE authors'\n",
    "\t'AS(SELECT id, authors, type, domain'\n",
    "\t  'FROM news' \n",
    "\t   'WHERE id between 13 and 700013)'\n",
    "    \n",
    "#*weak additional_data entity set to relation (includes ID)*\n",
    "'CREATE TABLE additional_data'\n",
    "\t'AS(SELECT id, meta_description, meta_keywords, scraped_at, inserted_at, updated_at'\n",
    "\t  'FROM news' \n",
    "\t   'WHERE id between 13 and 700013)'\n",
    "    \n",
    "# Relationships:\n",
    "'CREATE TABLE wrote'\n",
    "\t'AS(SELECT id, title, authors, type, domain'\n",
    "\t  'FROM news'\n",
    "\t   'WHERE id between 13 and 700013)'\n",
    "\n",
    "'CREATE TABLE contains'\n",
    "\t'AS(SELECT id, meta_description'\n",
    "\t  'FROM news' \n",
    "\t   'WHERE id between 13 and 700013)'\n",
    "           )\n",
    "\n",
    "# When trying out queries, the database would often loose connection. So the amount of rows was reduced from \n",
    "# 1.000.000 to 700.000. This will let use do more complex queries without the risk of loosing connection \n",
    "# to the database.\n",
    "\n",
    "# We can show the amount of rows in every table with the command:\n",
    "\n",
    "cur.execute(\n",
    "'select count(*)'\n",
    "'from articles'\n",
    ")\n",
    "\n",
    "# Wich returns 700.001, the amount of rows in the database. We can also perform some simple queries such as:\n",
    "\n",
    "cur.execute(\n",
    "'select * from wrote'\n",
    "'where type = \"bias\"'\n",
    ")\n",
    "\n",
    "cur.execute(\n",
    "'select authors.id, authors.domain, authors.type'\n",
    "'from authors' \n",
    "'INNER JOIN articles on authors.id = articles.id'\n",
    "'where authors.id between 500 and 1000'\n",
    ")\n",
    "\n",
    "cur.execute(\n",
    "'select * from wrote'\n",
    "'where type = \"bias\"'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1:\n",
    "cur.execute(\n",
    "'SELECT *'\n",
    "  'FROM ('\n",
    "        'SELECT  wrote.id,' \n",
    "            'wrote.domain,' \n",
    "            'additional_data.scraped_at,' \n",
    "            'ROW_NUMBER() OVER(PARTITION BY wrote.domain) rn'\n",
    "        'from wrote inner join additional_data on additional_data.id = wrote.id'\n",
    "\t  \t\t\t'where wrote.id between 13 and 700013 and wrote.type = \"reliable\"'\n",
    "\t\t\t\t'and scraped_at > '2018-01-15''\n",
    "              ') a'\n",
    "'WHERE rn = 1'\n",
    ")\n",
    "\n",
    "# Which returns:\n",
    "# 96\t\"christianpost.com\"\t\"2018-01-25 16:17:44.789555\"\t1\n",
    "# 165034\t\"consortiumnews.com\"\t\"2018-01-25 20:13:50.426130\"\t1\n",
    "# 418108\t\"nutritionfacts.org\"\t\"2018-01-25 20:13:50.426130\"\t1\n",
    "# -----------------------\n",
    "\n",
    "# Query 2:\n",
    "cur.execute(\n",
    "'Select authors, count(authors)''\n",
    "'from wrote'\n",
    "'where id between 13 and 700013 and type = 'fake''\n",
    "'and authors is not NULL'\n",
    "'group by authors'\n",
    "'ORDER BY COUNT(*) DESC'\n",
    "'LIMIT 5;'\n",
    ")\n",
    "\n",
    "# This return the five most profilic authors:\n",
    "# \"John Rolls\"\t1107\n",
    "# \"Zacks Investment Research\"\t942\n",
    "# \"Freedom Bunker\"\t859\n",
    "# \"Alton Parrish\"\t774\n",
    "# \"Reason Magazine\"\t685\n",
    "# -----------------------\n",
    "\n",
    "# Query 3:\n",
    "'SELECT'\n",
    "    'meta_keywords, count(*)'\n",
    "'FROM additional_data'\n",
    "\t'where id between 13 and 700013'\n",
    "\t'and length(meta_keywords) > 4'\n",
    "\t'group by meta_keywords'\n",
    "\t'having count(*) >= 2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1:\n",
    "cur.execute(\n",
    "    'select *\n",
    "\t'from('\n",
    "\t'select type, count(*)'\n",
    "\t'from wrote'\n",
    "\t'where id between 13 and 10013'\n",
    "\t'and authors is null'\n",
    "\t'group by type) as q1'\n",
    "\t','\n",
    "\t'('\n",
    "\t'select type, count(*)'\n",
    "\t'from wrote'\n",
    "\t'where id between 13 and 700013'\n",
    "\t'group by type)'\n",
    "\t'as q2'\n",
    "'where q1.type = q2.type'\n",
    "'order by (q1.count* 100 / q2.count)'\n",
    ")\n",
    "\n",
    "# This query finds the amount of authors that have the assignment Null and then compares it to the total amount of \n",
    "# authors. Then it orders the result on how low the percantage is. So the type with the lowest percentage of \n",
    "# assigned authors is first and the highets on the bottom. So here we can see that satire ranked lowest with only \n",
    "# 6% of their articles have an assigned author. On the other end, the conspiracy type is \n",
    "\n",
    "# \"satire\"\n",
    "# \"rumor\"\n",
    "# \"hate\"\n",
    "# \"political\"\n",
    "# \"fake\"\n",
    "# \"clickbait\"\n",
    "# \"junksci\"\n",
    "# \"unreliable\"\n",
    "# \"unknown\"\n",
    "# \"bias\"\n",
    "# \"reliable\"\n",
    "# \"Conspiracy\"\n",
    "# -----------------------\n",
    "\n",
    "# Query 2:\n",
    "cur.execute(\n",
    "'select wrote.type, round(avg(length(content)))'\n",
    "'from articles, wrote'\n",
    "'where articles.id between 13 and 100013'\n",
    "'and articles.id = wrote.id'\n",
    "'group by type'\n",
    "'order by round'\n",
    ")\n",
    "\n",
    "# This query finds the avarage character count from articles of each type. So we can see how each type tends to have \n",
    "# different article lengths. For this query I saw right away that the query took longer than usual to compute, or \n",
    "# around 4 seconds. After experimenting with bigger amounts of articles the database would loose connection, so \n",
    "# the article amount was lowered to only 100.000 articles. So the numbers would probably differ if we had the whole \n",
    "# 1.000.000 articles, but it still shows somewhat of a pattern already since the values from 50.000 and 100.000 did \n",
    "# not seem to differ too much. Both resulting in rumour and satire having the shortest avarage article length, and \n",
    "# reliable having the longest. This might hint that the length of articles hints at the type of article it represents.\n",
    "\n",
    "# Which returns:\n",
    "# \"rumor\"\t\t1962\n",
    "# \"satire\"\t\t1966\n",
    "# \"clickbait\"\t2254\n",
    "# \"hate\"\t\t2277\n",
    "# “Null”\t\t2739\n",
    "# \"bias\"\t\t3351\n",
    "# \"fake\"\t\t3355\n",
    "# \"unreliable\"\t3417\n",
    "# \"political\"\t3627\n",
    "# \"junksci\"\t\t4147 \n",
    "# \"conspiracy\"\t4778\n",
    "# \"unknown\"\t\t4782\n",
    "# \"reliable\"\t4993\n",
    "# -----------------------\n",
    "\n",
    "# Query 3:\n",
    "cur.execute(\n",
    "'select *'\n",
    "'from\t'\n",
    "'\t(select domain, count(*)'\n",
    "'\tfrom wrote'\n",
    "'\twhere id between 13 and 700013'\n",
    "'\tand authors is not null'\n",
    "'\tgroup by domain'\n",
    "'\torder by count desc'\n",
    "'\tfetch first 100 rows only) as total'\n",
    "'\t,'\n",
    "'\t('\n",
    "'\tselect domain, count(*)'\n",
    "'\t\tfrom'\n",
    "'\t\t(\t\tselect authors, domain, count(*)'\n",
    "'\t\t\t\tfrom wrote'\n",
    "'\t\t\t\twhere id between 13 and 700013'\n",
    "'\t\t \t\tand authors is not null'\n",
    "'\t\t\t\tgroup by authors, domain) as num'\n",
    "'\tgroup by domain'\n",
    "'\torder by count desc) as auths'\n",
    "'where auths.domain = total.domain'\n",
    "'order by (auths.count * 100 / total.count)'\n",
    ")\n",
    "\n",
    "# The final query is a bit longer. It takes a look at the domains with the highest amount of articles, and sees how \n",
    "# many different authors write articles for that domain. The idea is to find if there is a corrilation between how \n",
    "# many articles a domain produces and the amount of authors that write for the domain. Note that, the author can \n",
    "# appear again if there was another author working on the same article. Also, the query does not look at Null values \n",
    "# since they are completely unknown. The result does not show any particular corrolation between the amount of \n",
    "# articles and authors a domain has. Since the domain with the highest ratio per author was thevictoryreport.org \n",
    "# with 1450 articles and only 4 authors so each author has written 362.5 articles on avarage, whereas the lowest \n",
    "# authors-to-domain ration was southfront.org with 785 articles and 703 authors so each author has written 1,1 \n",
    "# articles on avarage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "\n",
    "For task five, Scrapy was used to scrape the web, and the group of letters was 'HIJKLMNOPR'.\n",
    "I am not familiar with web scarping and css in general so using scrapy was quite though, since there\n",
    "was not alot of clear information about its syntax online. It helped to first create a scrapy shell and find the\n",
    "code needed, and then rewriting it. \n",
    "Since some of the categories have subcategories and some dont, the program should be able to navigate through all \n",
    "the pages. The solution was to have 3 different parsers. The first one start at the front page and gets the urls\n",
    "of all categories from H to R. The second parser gets all urls of subcategories and urls of articles and seperates them. Then it sends the article urls to the third parser and recursively sends the subcategories to it self. That way it can scan through all the subcategories. The third parser yields all the data. The fields it produces is the title, date, content and categories.\n",
    "\n",
    "The code is below and is excecuted in to a .csv with the following code:\n",
    "scrapy crawl articles -o articles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"category\"\n",
    "    \n",
    "\n",
    "    start_urls = [\n",
    "        'https://en.wikinews.org/wiki/Category:Politics_and_conflicts',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        letters = ['H','I','J','K','L','M','N','O','P']\n",
    "        for auth in response.css('div.mw-category-group'):\n",
    "            if auth.css('h3::text').get() in letters:\n",
    "                allHrefsInCategory = auth.css('a::attr(href)').getall()\n",
    "                for href in allHrefsInCategory:\n",
    "                    nextPageUrl = response.urljoin(href)\n",
    "                    yield scrapy.Request(url = nextPageUrl, callback = self.parse_middle)\n",
    "                    \n",
    "    def parse_middle(self, response):\n",
    "        subcat = response.css('div[id=\"mw-subcategories\"] a::attr(href)').getall()\n",
    "        if len(subcat) > 0:\n",
    "            for hrefs in subcat:\n",
    "                nextPageUrl = response.urljoin(hrefs)\n",
    "                yield scrapy.Request(url = nextPageUrl, callback = self.parse_middle) \n",
    "            \n",
    "        for pages in response.css('div[id=\"mw-pages\"]'):\n",
    "            allHrefsInCategory = pages.css('a::attr(href)').getall()\n",
    "            for href in allHrefsInCategory:\n",
    "                nextPageUrl = response.urljoin(href)\n",
    "                yield scrapy.Request(url = nextPageUrl, callback = self.parse_articles)                \n",
    "            \n",
    "                \n",
    "    def parse_articles(self, response):\n",
    "        yield{\n",
    "                'title': response.css('h1::text').get(),\n",
    "                'date': response.css('span[id=\"publishDate\"]').xpath(\"@title\").get(),\n",
    "                'content': response.css('p::text,a[class=\"mw-redirect\"]::text').extract(),\n",
    "                'Categories': response.css('div[class=\"mw-normal-catlinks\"] a::attr(title)').extract()\n",
    "                \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our .csv file we need to clean it. I used the cleaning program from the original data set and tweaked it a little. For example, the categories field needed alot of cleaning since the text had a lot of extra strings that did not contribute much. The code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import re\n",
    "\n",
    "def cleanNumbers(text):\n",
    "    text = \" \"+text+\" \"\n",
    "    for number in string.digits:\n",
    "        if number in text:\n",
    "            count = text.count(number)\n",
    "            for x in range(count):\n",
    "                if number in text:\n",
    "                    placement = text.find(number)\n",
    "                    if placement != -1:\n",
    "                        nextIsDigit = True\n",
    "                        substring = \" <NUM>\"\n",
    "                        while nextIsDigit:\n",
    "                            if text[placement + 1] in string.digits:\n",
    "                                text = text[:placement] + text[1 + placement:]\n",
    "                            else:\n",
    "                                text = text[:placement - 1] + substring + text[1 + placement:]\n",
    "                                nextIsDigit = False\n",
    "    return(text)\n",
    "\n",
    "def cleanEmails(text):\n",
    "    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    for i in emails:\n",
    "        text = text.replace(i,\"<EMAIL>\")  \n",
    "    return text\n",
    "        \n",
    "def cleanUrls(text):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    for i in urls:\n",
    "        text = text.replace(i,\"<URL>\")  \n",
    "    return text\n",
    "\n",
    "def cleanDates(text):\n",
    "    dates = re.findall(r'\\d{4}-\\d{2}-\\d{2}', text)\n",
    "    for i in dates:\n",
    "        text = text.replace(i,\"<DATE>\")  \n",
    "    dates2 = re.findall(r'\\d{4}.\\d{2}.\\d{2}', text)\n",
    "    for i in dates2:\n",
    "        text = text.replace(i,\"<DATE>\")  \n",
    "    dates3 = re.findall(r'\\d{4}_\\d{2}_\\d{2}', text)\n",
    "    for i in dates3:\n",
    "        text = text.replace(i,\"<DATE>\")  \n",
    "    return text\n",
    "\n",
    "def cleanCategories(text):\n",
    "    if text is not None:\n",
    "        cats = re.findall('Categories|Category|Special, ', text)\n",
    "        for i in cats:\n",
    "                text = text.replace(i,\" \")  \n",
    "        colon = re.findall(':', text)\n",
    "        for i in colon:\n",
    "                text = text.replace(i,\",\")  \n",
    "    return text\n",
    "\n",
    "\n",
    "def cleanText(fileName):\n",
    "    r = csv.reader(open(fileName)) \n",
    "    lines = list(r) \n",
    "    id = 1\n",
    "    for e in lines:\n",
    "        e.insert(0, str(id))\n",
    "        id = id + 1\n",
    "        for x in range(len(e)):\n",
    "            if \",\" in e[x]:\n",
    "                e[x] = e[x].replace(\",\",\"\")\n",
    "            if \"\\n\" in e[x]:\n",
    "                e[x] = e[x].replace(\"\\n\",\"\")\n",
    "        e[3] = e[3].lower()\n",
    "        e[3] = cleanDates(e[3])\n",
    "        e[3] = cleanEmails(e[3])\n",
    "        e[3] = cleanUrls(e[3])\n",
    "        e[3] = cleanNumbers(e[3])\n",
    "        if len(e) == 4:\n",
    "            e.append(None)\n",
    "        e[4] = cleanCategories(e[4])\n",
    "    lines[0][4] = 'categories'\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "finalText = cleanText('news.csv')\n",
    "with open('scrapedCleanedNews.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(finalText)\n",
    "    \n",
    "print(\"FINISHED\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we only have insert the file to postgres, with the following code.\n",
    "cur.execute(\n",
    "'COPY newsScraped(id,title,date,content,categories)'\n",
    "'FROM \"/private/tmp/scrapedCleanedNews.csv\" DELIMITER ',' CSV HEADER;'\n",
    ")\n",
    "\n",
    "# and we can see that it supports basic queries like:\n",
    "\n",
    "cur.execute(\n",
    "'select count(*)'\n",
    "'from newsscraped'\n",
    ")\n",
    "\n",
    "# which returns 5959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
