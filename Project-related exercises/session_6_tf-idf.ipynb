{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /Users/Master/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/Master/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import pyodbc\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#Tilføjede kommentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execQuery(query):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"root\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\",\n",
    "                                      database = \"postgres\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        record = cursor.fetchall()\n",
    "        return record\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        connection = False\n",
    "        print (\"Error while connecting to PostgreSQL\", error)\n",
    "    finally:\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            # Det er vigtigt at tilføje 'curser.commit()' hvis man vil lave andre oparationer end at læse fra databasen\n",
    "            print(\"Executed query and closed connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesContent = execQuery(\"\"\"Select content\n",
    "from article\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection = psycopg2.connect(user = \"postgres\",\n",
    "#                                      password = \"root\",\n",
    "#                                      host = \"localhost\",\n",
    "#                                      port = \"5432\",\n",
    "#                                      database = \"postgres\")\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "filepath = 'small.csv'\n",
    "#filepath = 'news_sample.csv' # <- overwrite for setup\n",
    "s = 250                    # desired sample size(seems to have slack ie. not exact)\n",
    "seed = 1                     # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_csv(filepath, index_col = [0]).sample(n=s, random_state=seed)\n",
    "df[\"content\"] = df[\"content\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> The United States has slashed its contribution to the UN Relief and Works Agency for Palestinian refugees by more than NUM million but still gave the agency NUM million Thats NUM million too much UNRWA has become the poster child for UN bureaucratic bloat mission creep and twisted morality\n\nUNRWA was founded in NUM to help Palestinian refugees displaced by Israels war of independence Even that decision was questionable After all putting Israels creation aside in the NUMs alone wars created more than NUM million refugees There was World War II of course but the NUM partition of India also displaced NUM million persons and created millions of refugees In the wake of Israels creation Arab states expelled hundreds of thousands of their Jews This is why the UN created the United Nations High Commissioner for Refugees to be the permanent agency charged with refugee assistance and protection\n\nIndividual Palestinian Arab families suffered dislocation and tragedy but they were hardly alone They were alone however in having a UN agency created to cater specifically to them Still when the UN created UNRWA it was meant to be a temporary agency to address a temporary problem Once it resettled Palestinian refugees its job was done and it was to shut its doors forever and dissolve\n\nIn a NUM report to the UN General Assembly UNRWA reported There must be a firm goal of terminating relief operations Sustained relief operations inevitably contain the germ of human deterioration It was right but Arab state rejectionism got in the way Arab states like Egypt Syria and Iraq quickly concluded that they could use UNRWA to perpetuate the problem to undercut normalization with Israel and to transform an aid agency into a weapon against Israel\n\nThe rest is history Both current UNRWA Commissioner General Pierre Krahenbuhl and UNRWA spokesman Chris Gunness were born years after UNRWA was meant to dissolve\n\nNor does UNRWAs track record make it worth saving While UNHCR has resettled refugees and allowed them to get on with their lives UNRWA has actively undercut settlement It has embraced a unique definition of refugee that differs with UNHCR and that is used everywhere else in the world Consider If India and Pakistan used the same definition of refugee from their partition that UNRWA embraces there would be more than NUM million refugees in South Asia today UNRWA has helped make the Palestinians the largest per capita recipient of aid on earth but has the least to show for it\n\nThe problem isnt just waste of money however Almost a quartercentury after the Palestinian Authority began UNRWA has eroded rather than supported the foundations of good governance After all if UNRWA promises to take care of housing education and support why shouldnt both the Palestinian Authority and their sometimepartner Hamas spend money on terror pensions terror tunnels and an arsenal of rockets\n\nAdd into the mix UNRWA incompetence schools used as arsenals school books teaching incitement UNRWA employees moonlighting as bomb makers and terror recruitment in UNRWA high schools and the problem is even worse UNRWA denials of complicity in such activities if taken at face value are just acknowledgment of its own poor oversight\n\nIts long past time to cut off UNRWA and ask UNHCR to take over The Palestinians deserve an apolitical agency rather than an amplifier for radicalism and waste UNRWA has become a cancer not only for those who seek peace and reconciliation but also for the entire UN system\n\nUNRWA in NUM was right Permanence is not a virtue its an impediment\n\nMichael Rubin MrubinNUM is a contributor to the Washington Examiners Beltway Confidential blog He is a resident scholar at the American Enterprise Institute and a former Pentagon official\n\nIf you would like to write an oped for the Washington Examiner please read our guidelines on submissions here\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "67     the united states has slashed its contribution...\n249    former us president bill clinton on monday cal...\n230    the real story behind marijuana prohibition\\n\\...\n161    audio sensors market analysis by current indus...\n91     headline bitcoin  blockchain searches exceed t...\nName: content, dtype: object"
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "# working with content as a series\n",
    "series_content = df['content']\n",
    "\n",
    "### preprocessing - https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing ###\n",
    "# try creating super regex ie. combine alle into one - might be alot faster\n",
    "\n",
    "### replace [name] with tokens in text ###\n",
    "\n",
    "# replace URLs  with token\n",
    "regexURL= r\"(?:https?:\\/\\/)?(?:www\\.)?([^@\\s]+\\.[a-zA-Z]{2,4})[^\\s]*\"\n",
    "series_content = series_content.replace(to_replace=regexURL, value='<URL>', regex=True)\n",
    "\n",
    "# replace emails with token\n",
    "regexEmail = r\"[a-zA-Z_-]+@[a-zA-Z_-]+(?:\\.[a-zA-Z]{2,4}){1,3}\"\n",
    "series_content = series_content.replace(to_replace=regexEmail, value='<EMAIL>', regex=True)\n",
    "\n",
    "# replace dates with token\n",
    "regexDate = r\"(((19[7-9]\\d|20\\d{2})|(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|(nov|dec)(?:ember)?)|(([12][0-9])|(3[01])|(0?[1-9])))[\\/. \\-,\\n]){2,3}\"\n",
    "series_content = series_content.replace(to_replace=regexDate, value='<DATE>', regex=True)\n",
    "\n",
    "# repalce numbers with token\n",
    "regexNum = r\"[0-9][0-9,\\.]+\" # type of nums with [0-9] and [,.]\n",
    "series_content = series_content.replace(to_replace=regexNum, value='<NUM>', regex=True)\n",
    "\n",
    "### remove chars in text ###\n",
    "# remove \"\\n\"\n",
    "regexNL = r\"(\\\\n)\"\n",
    "series_content = series_content.replace(to_replace=regexNL, value='', regex=True)\n",
    "\n",
    "# remove punctuations - remove \"<\" and \">\"\n",
    "regexPunkt = r\"[!“”\\\"#$%&()*+,\\-–.\\/:;<=>?@[\\\\\\]^_`{|}~'\\’\\']\" # also removes: \" ' \" which means it's -> its - remove \\' from regex if not needed\n",
    "series_content = series_content.replace(to_replace=regexPunkt, value='', regex=True)\n",
    "print(\"->\", series_content.iloc[0])\n",
    "\n",
    "# lower Casing - IMP lower casing is DONE my most of the modern day vecotirzers and tokenizers \n",
    "series_content = series_content.str.lower()\n",
    "\n",
    "# remove stopwords - NOT DONE\n",
    "stopWords = set(stopwords.words('english'))\n",
    "series_content.str.split().explode().isin(stopWords) == False\n",
    "\n",
    "\n",
    "series_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0         [the, sky, is, blue]\n1    [blue, not, is, sky, the]\nName: content, dtype: object\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0        the sky is blue\n1    blue not is sky the\nName: content, dtype: object"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# READ ABOUT TOKENIZERS\n",
    "# https://medium.com/biaslyai/beginners-guide-to-text-preprocessing-in-python-2cbeafbf5f44\n",
    "# https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "# https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f\n",
    "# https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7\n",
    "\n",
    "# COMAPRE TOKENIZERS\n",
    "# https://miro.medium.com/max/1400/1*FLVWAVL1pkAOpN9CoVBehA.png\n",
    "\n",
    "# series_of_str -> [ nltk.word_tokenize(string) for string in words ]\n",
    "\n",
    "df_1 = pd.DataFrame({'content': [\n",
    "                        'the sky is blue',\n",
    "                        'blue not is sky the']})\n",
    "\n",
    "# tokenization of strings - [not fast if using apply: maybe use library for faster apply or list comwprehension]\n",
    "def str_tokenizer(str_words):\n",
    "    # Word Tokenization\n",
    "    list_words = word_tokenize(str_words)\n",
    "\n",
    "    # Word Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_words = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "\n",
    "    # Word Stemming\n",
    "    # snowball = SnowballStemmer(language = 'english')\n",
    "    # stem_words = [snowball.stem(w) for w in list_words]\n",
    "        \n",
    "    return lem_words\n",
    "\n",
    "# choose df-column to tokenize\n",
    "series_1 = df_1['content'].apply(str_tokenizer)\n",
    "print(series_1)\n",
    "\n",
    "\n",
    "# list comprehension\n",
    "[ nltk.word_tokenize(string) for string in df_1['content'] ]\n",
    "\n",
    "df_1['content'] = df_1['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas.core.series.Series"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# df_1 = pd.DataFrame({'content': [\n",
    "#                      'the sky is blue',\n",
    "#                      'the sky is not blue']})\n",
    "\n",
    "# series_1 = df_1['content'].str.split()\n",
    "\n",
    "series_content\n",
    "\n",
    "# used to to tokenize\n",
    "    # # Tokenize and generate count vectors\n",
    "    # word_vec = series_of_lists_of_tokens.str.split().apply(pd.value_counts).fillna(0)\n",
    "\n",
    "\n",
    "def tf_idf(series_of_lists_of_tokens):\n",
    "    \"\"\" Returns tf-idf matrix(dataframe) for a given df-column/series with list of tokens for each row. \"\"\"\n",
    "\n",
    "    # Generate count vectors\n",
    "    word_vec = series_of_lists_of_tokens.apply(pd.value_counts).fillna(0) # intuitive but slow\n",
    "\n",
    "\n",
    "    # Compute term frequencies\n",
    "        ### Assuming from http://www.tfidf.com/ -> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "    tf = word_vec.divide(np.sum(word_vec, axis='columns'), axis='index')\n",
    "\n",
    "    # Compute inverse document frequencies\n",
    "        ### Slides seems wrong s.18: https://docs.google.com/presentation/d/1rXhewxbmLX7mzDcy8aucI8OLIikfrLLiz8ocN5GS06I/edit#slide=id.g77508fe5d3_0_19\n",
    "        ### Assuming from http://www.tfidf.com/ -> log_e(Total number of documents / Number of documents with term t in it)\n",
    "    idf = np.log(len(tf) / word_vec[word_vec > 0].count()) \n",
    "\n",
    "    # Compute TF-IDF vectors and return\n",
    "    tfidf = np.multiply(tf, idf.to_frame().T)\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "# print(tf)\n",
    "# print(idf)\n",
    "# stemming = PorterStemmer()\n",
    "# test1 = df_1['content'].iloc[0]\n",
    "# test2 = 'frightening, frightened frightens'\n",
    "# series_content_tokens = df_1['content'].str.split()\n",
    "# def tokenize(text):\n",
    "#     list_tokens = nltk.word_tokenize(text)\n",
    "#     a = [stemming.stem(word) for word in list_tokens]\n",
    "#     return a\n",
    "\n",
    "# df_tokens = series_content_tokens.explode().to_frame().reset_index().rename(columns={'index':'content_index'})\n",
    "# df_tokens.groupby('content_index')[df_tokens.columns[1]].value_counts(normalize=True).unstack(level=1)\n",
    "\n",
    "\n",
    "# tf_idf(series_content_tokens)\n",
    "tf_idf(series_content)\n",
    "# df_tokens.apply(tuple).value_counts()\n",
    "\n",
    "df_content = pd.DataFrame(series_content)\n",
    "df_content\n",
    "\n",
    "type(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'the united states has slashed its contribution to the un relief and works agency for palestinian refugees by more than 65 million but still gave the agency 60 million thats 60 million too much unrwa has become the poster child for un bureaucratic bloat mission creep and twisted morality\\n\\nunrwa was founded in 1949 to help palestinian refugees displaced by israels war of independence even that decision was questionable after all putting israels creation aside in the 1940s alone wars created more than 40 million refugees there was world war ii of course but the 1947 partition of india also displaced 14 million persons and created millions of refugees in the wake of israels creation arab states expelled hundreds of thousands of their jews this is why the un created the united nations high commissioner for refugees to be the permanent agency charged with refugee assistance and protection\\n\\nindividual palestinian arab families suffered dislocation and tragedy but they were hardly alone they were alone however in having a un agency created to cater specifically to them still when the un created unrwa it was meant to be a temporary agency to address a temporary problem once it resettled palestinian refugees its job was done and it was to shut its doors forever and dissolve\\n\\nin a 1951 report to the un general assembly unrwa reported there must be a firm goal of terminating relief operations sustained relief operations inevitably contain the germ of human deterioration it was right but arab state rejectionism got in the way arab states like egypt syria and iraq quickly concluded that they could use unrwa to perpetuate the problem to undercut normalization with israel and to transform an aid agency into a weapon against israel\\n\\nthe rest is history both current unrwa commissioner general pierre krahenbuhl and unrwa spokesman chris gunness were born years after unrwa was meant to dissolve\\n\\nnor does unrwas track record make it worth saving while unhcr has resettled refugees and allowed them to get on with their lives unrwa has actively undercut settlement it has embraced a unique definition of refugee that differs with unhcr and that is used everywhere else in the world consider if india and pakistan used the same definition of refugee from their partition that unrwa embraces there would be more than 100 million refugees in south asia today unrwa has helped make the palestinians the largest per capita recipient of aid on earth but has the least to show for it\\n\\nthe problem isnt just waste of money however almost a quartercentury after the palestinian authority began unrwa has eroded rather than supported the foundations of good governance after all if unrwa promises to take care of housing education and support why shouldnt both the palestinian authority and their sometimepartner hamas spend money on terror pensions terror tunnels and an arsenal of rockets\\n\\nadd into the mix unrwa incompetence schools used as arsenals school books teaching incitement unrwa employees moonlighting as bomb makers and terror recruitment in unrwa high schools and the problem is even worse unrwa denials of complicity in such activities if taken at face value are just acknowledgment of its own poor oversight\\n\\nits long past time to cut off unrwa and ask unhcr to take over the palestinians deserve an apolitical agency rather than an amplifier for radicalism and waste unrwa has become a cancer not only for those who seek peace and reconciliation but also for the entire un system\\n\\nunrwa in 1951 was right permanence is not a virtue its an impediment\\n\\nmichael rubin mrubin1971 is a contributor to the washington examiners beltway confidential blog he is a resident scholar at the american enterprise institute and a former pentagon official\\n\\nif you would like to write an oped for the washington examiner please read our guidelines on submissions here'"
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(series_content)\n",
    "series_content.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'item' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e227ae1c644d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEMMER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTOKENIZER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'frightening, frightened frightens'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtextprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-e227ae1c644d>\u001b[0m in \u001b[0;36mtextprocessing\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mTOKENIZER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtextprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEMMER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTOKENIZER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'frightening, frightened frightens'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtextprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-e227ae1c644d>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mTOKENIZER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtextprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEMMER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTOKENIZER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'frightening, frightened frightens'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtextprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'item' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "STEMMER = nltk.stem.SnowballStemmer('english')\n",
    "TOKENIZER = RegexpTokenizer(r'\\w+')\n",
    "def textprocessing(text):\n",
    "    return ''.join(STEMMER.stem(item)  for token in TOKENIZER.tokenize(text.lower()) if len(token) > 1)\n",
    "a = 'frightening, frightened frightens'\n",
    "textprocessing(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}