{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /home/daniel/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pyodbc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execQuery(query):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"root\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\",\n",
    "                                      database = \"postgres\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        record = cursor.fetchall()\n",
    "        return record\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        connection = False\n",
    "        print (\"Error while connecting to PostgreSQL\", error)\n",
    "    finally:\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Executed query and closed connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesContent = execQuery(\"\"\"Select content\n",
    "from article\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection = psycopg2.connect(user = \"postgres\",\n",
    "#                                      password = \"root\",\n",
    "#                                      host = \"localhost\",\n",
    "#                                      port = \"5432\",\n",
    "#                                      database = \"postgres\")\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "filepath = '/home/daniel/OneDrive/KUuni/DataScience/Python/small.csv'\n",
    "#filepath = 'news_sample.csv' # <- overwrite for setup\n",
    "s = 250                    # desired sample size(seems to have slack ie. not exact)\n",
    "seed = 1                     # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_csv(filepath, index_col = [0]).sample(n=s, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package wordnet to /home/daniel/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['the', 'sky', 'is', 'blue'], ['blue', 'not', 'is', 'sky', 'the']]"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# https://medium.com/biaslyai/beginners-guide-to-text-preprocessing-in-python-2cbeafbf5f44\n",
    "# https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "# https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f\n",
    "# https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7\n",
    "\n",
    "# COMAPRE TOKENIZERS\n",
    "# https://miro.medium.com/max/1400/1*FLVWAVL1pkAOpN9CoVBehA.png\n",
    "\n",
    "# series_of_str -> [ nltk.word_tokenize(string) for string in words ]\n",
    "\n",
    "df_1 = pd.DataFrame({'content': [\n",
    "                     'the sky is blue',\n",
    "                     'blue not is sky the']})\n",
    "\n",
    "# tokenization of strings - [not fast if using apply: maybe use library for faster apply or list comwprehension]\n",
    "def str_tokenizer(str_words):\n",
    "    # Word Tokenization\n",
    "    list_words = word_tokenize(str_words)\n",
    "\n",
    "    # Word Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_words = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "\n",
    "    # Word Stemming\n",
    "    # snowball = SnowballStemmer(language = 'english')\n",
    "    # stem_words = [snowball.stem(w) for w in list_words]\n",
    "        \n",
    "    return lem_words\n",
    "\n",
    "# choose df-column to tokenize\n",
    "series_1 = df_1['content'].apply(str_tokenizer)\n",
    "\n",
    "# list comprehension\n",
    "[ nltk.word_tokenize(string) for string in df_1['content'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "sent  blue   is       not  sky  the\n0      0.0  0.0  0.000000  0.0  0.0\n1      0.0  0.0  0.138629  0.0  0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>sent</th>\n      <th>blue</th>\n      <th>is</th>\n      <th>not</th>\n      <th>sky</th>\n      <th>the</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.138629</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "def tf_idf(series_of_lists_of_tokens):\n",
    "    \"\"\" Returns tf-idf matrix(dataframe) for a given df-column/series with list of tokens for each row. \"\"\"\n",
    "\n",
    "    # ### DONT REMOVE Understandable/simple term frequencies\n",
    "    # # Tokenize and generate count vectors\n",
    "    # word_vec = series_of_lists_of_tokens.apply(pd.value_counts).fillna(0)\n",
    "    # # Compute term frequencies\n",
    "    # tf = word_vec.divide(np.sum(word_vec, axis=1), axis=0)\n",
    "    # ### DONT REMOVE Understandable/simple term frequencies\n",
    "\n",
    "    # Compute count vectors\n",
    "    # explode: flattern -> goupby: apply to each \"list_of_tokens\" -> unstack: dataframe with row as content and column as token [change unstack(level=X) to transpose]\n",
    "        ### Assuming from http://www.tfidf.com/ -> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "    word_vec = series_of_lists_of_tokens.explode().groupby(level=0).value_counts().unstack(level=1).fillna(0)\n",
    "\n",
    "    # Compute term frequencies\n",
    "    tf = word_vec.divide(np.sum(word_vec, axis='columns'), axis='index')\n",
    "\n",
    "    # Compute inverse document frequencies\n",
    "        ### Assuming from http://www.tfidf.com/ -> log_e(Total number of documents / Number of documents with term t in it)\n",
    "    idf = np.log(len(tf) / word_vec[word_vec > 0].count()) \n",
    "\n",
    "    # Compute TF-IDF vectors and return\n",
    "    return np.multiply(tf, idf.to_frame().T) # <- tf * idf\n",
    "\n",
    "tf_idf(series_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BELOW IS DEBUG CODE and scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    1\nthe   0.25  0.2\nblue  0.25  0.2\nis    0.25  0.2\nsky   0.25  0.2\nnot   0.00  0.2 \n\n         0    1\nsent           \nblue  0.25  0.2\nis    0.25  0.2\nnot   0.00  0.2\nsky   0.25  0.2\nthe   0.25  0.2 \n\n"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame({'sent': [\n",
    "                     'the sky is blue',\n",
    "                     'blue not is sky the']})\n",
    "series_1 = df_1.sent.str.split()\n",
    "# should be equal but maybe different order\n",
    "# simple\n",
    "word_vec = series_1.apply(pd.value_counts).fillna(0)\n",
    "tf1 = word_vec.divide(np.sum(word_vec, axis='columns'), axis='index')\n",
    "\n",
    "print(tf1.T, \"\\n\")\n",
    "\n",
    "# complex but fast with large datasets\n",
    "tf2 = series_1.explode().groupby(level=0).value_counts(normalize=True).unstack(level=1).fillna(0)\n",
    "\n",
    "print(tf2.T, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "sent           blue    is  not   sky   the\ncontent_index                             \n0              0.25  0.25  0.0  0.25  0.25\n1              0.20  0.20  0.2  0.20  0.20",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>sent</th>\n      <th>blue</th>\n      <th>is</th>\n      <th>not</th>\n      <th>sky</th>\n      <th>the</th>\n    </tr>\n    <tr>\n      <th>content_index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.20</td>\n      <td>0.20</td>\n      <td>0.2</td>\n      <td>0.20</td>\n      <td>0.20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df_tokens = series_of_lists_of_tokens.explode().to_frame().reset_index().rename(columns={'index':'content_index'})\n",
    "tf2 = df_tokens.groupby('content_index')[df_tokens.columns[1]].value_counts(normalize=True).unstack(level=1).fillna(0)\n",
    "tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sent  blue   is  not  sky  the\n0      1.0  1.0  0.0  1.0  1.0\n1      1.0  1.0  1.0  1.0  1.0\nsent  blue    is  not   sky   the\n0     0.25  0.25  0.0  0.25  0.25\n1     0.20  0.20  0.2  0.20  0.20\n"
    }
   ],
   "source": [
    "word_vec = series_1.explode().groupby(level=0).value_counts().unstack(level=1).fillna(0)\n",
    "print(word_vec)\n",
    "\n",
    "print(word_vec.divide(np.sum(word_vec, axis=1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.04 ms ± 105 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "series_explode = series_of_lists_of_tokens.explode()\n",
    "\n",
    "tf2 = series_explode.groupby(level=0).value_counts(normalize=True).unstack(level=1).fillna(0)\n",
    "tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "sent  blue    is  not   sky   the\n0     0.25  0.25  0.0  0.25  0.25\n1     0.20  0.20  0.2  0.20  0.20",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>sent</th>\n      <th>blue</th>\n      <th>is</th>\n      <th>not</th>\n      <th>sky</th>\n      <th>the</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.20</td>\n      <td>0.20</td>\n      <td>0.2</td>\n      <td>0.20</td>\n      <td>0.20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "series_explode = series_of_lists_of_tokens.explode()\n",
    "\n",
    "tf2 = series_of_lists_of_tokens.explode().groupby(level=0).value_counts(normalize=True).unstack(level=1).fillna(0)\n",
    "tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sent\n0  blue    0.25\n   is      0.25\n   sky     0.25\n   the     0.25\n1  blue    0.20\n   is      0.20\n   not     0.20\n   sky     0.20\n   the     0.20\nName: sent, dtype: float64"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "series_explode = series_of_lists_of_tokens.explode()\n",
    "\n",
    "tf2 = series_of_lists_of_tokens.explode().groupby(level=0).value_counts(normalize=True).unstack(level=1).fillna(0)\n",
    "tf2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('DS': conda)",
   "language": "python",
   "name": "python37764bitdsconda217f41063fe24cf89ccdf8aa73f962cf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}