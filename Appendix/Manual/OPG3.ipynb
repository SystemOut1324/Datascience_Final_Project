{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execQuery(query):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"root\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\",\n",
    "                                      database = \"postgress\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        record = cursor.fetchall()\n",
    "        return record\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        connection = False\n",
    "        print (\"Error while connecting to PostgreSQL\", error)\n",
    "    finally:\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Executed query and closed connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain_id</th>\n",
       "      <th>type_id</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>inserted_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13842</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>http://beforeitsnews.com/military/2017/12/russ...</td>\n",
       "      <td>['russia', 'responds', 'to', 'us', 'provocatio...</td>\n",
       "      <td>Russia Responds to US Provocation: Open Skies ...</td>\n",
       "      <td>['']</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2350</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>http://breakpoint.org/category/signs-and-wonde...</td>\n",
       "      <td>['signs', 'and', 'wondersr', 'r', 'snap', 'out...</td>\n",
       "      <td>Signs and Wonders Archives</td>\n",
       "      <td>['']</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>27373</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.undergroundhealth.com/understandin...</td>\n",
       "      <td>['by', 'giovani', '', '', 'URL', 'r', 'r', 'ar...</td>\n",
       "      <td>Understanding What Authentic Happiness Is</td>\n",
       "      <td>['']</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>26536</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.alternet.org/activism/if-we-dont-a...</td>\n",
       "      <td>['if', 'we', 'dont', 'act', 'now', 'fascism', ...</td>\n",
       "      <td>If We Don't Act Now, Fascism Will Be on Our Do...</td>\n",
       "      <td>['']</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6045</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>http://beforeitsnews.com/opinion-conservative/...</td>\n",
       "      <td>['greg', 'hunter', 'big', 'banks', 'in', 'big'...</td>\n",
       "      <td>Greg Hunter: Big Banks in Big Trouble, Syria/N...</td>\n",
       "      <td>['']</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  domain_id  type_id  \\\n",
       "0  13842          1        1   \n",
       "1   2350          2        2   \n",
       "2  27373          3        3   \n",
       "3  26536          0        0   \n",
       "4   6045          1        1   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://beforeitsnews.com/military/2017/12/russ...   \n",
       "1  http://breakpoint.org/category/signs-and-wonde...   \n",
       "2  https://www.undergroundhealth.com/understandin...   \n",
       "3  https://www.alternet.org/activism/if-we-dont-a...   \n",
       "4  http://beforeitsnews.com/opinion-conservative/...   \n",
       "\n",
       "                                             content  \\\n",
       "0  ['russia', 'responds', 'to', 'us', 'provocatio...   \n",
       "1  ['signs', 'and', 'wondersr', 'r', 'snap', 'out...   \n",
       "2  ['by', 'giovani', '', '', 'URL', 'r', 'r', 'ar...   \n",
       "3  ['if', 'we', 'dont', 'act', 'now', 'fascism', ...   \n",
       "4  ['greg', 'hunter', 'big', 'banks', 'in', 'big'...   \n",
       "\n",
       "                                               title meta_description  \\\n",
       "0  Russia Responds to US Provocation: Open Skies ...             ['']   \n",
       "1                         Signs and Wonders Archives             ['']   \n",
       "2          Understanding What Authentic Happiness Is             ['']   \n",
       "3  If We Don't Act Now, Fascism Will Be on Our Do...             ['']   \n",
       "4  Greg Hunter: Big Banks in Big Trouble, Syria/N...             ['']   \n",
       "\n",
       "   scraped_at  updated_at inserted_at  \n",
       "0  2018-01-25  2018-02-02  2018-02-02  \n",
       "1  2018-01-25  2018-02-02  2018-02-02  \n",
       "2  2018-01-25  2018-02-02  2018-02-02  \n",
       "3  2018-01-25  2018-02-02  2018-02-02  \n",
       "4  2018-01-25  2018-02-02  2018-02-02  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load dataset\n",
    "connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"root\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\")\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "filepath = '../Data_git_ignore/clean_csv/article_clean.csv' # 250 rows of FakeNewsCorpus\n",
    "s = 250                                            # desired sample size\n",
    "seed = 1                                           # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM fakenews.article\", connection)\n",
    "#df[\"content\"] = df[\"content\"].astype(str)\n",
    "# c*reate type_id\n",
    "#df['type_id'] = df.groupby(['type']).ngroup()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = int(df.shape[0] * 1)\n",
    "\n",
    "x_train = df['content'].str[:ratio]\n",
    "x_test = df['content'].str[ratio:]\n",
    "\n",
    "y_train = df['type_id'][:ratio]\n",
    "y_test = df['type_id'][ratio:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'signs'\",\n",
       " \"'and'\",\n",
       " \"'wondersr'\",\n",
       " \"'r'\",\n",
       " \"'snap'\",\n",
       " \"'out'\",\n",
       " \"'of'\",\n",
       " \"'it'\",\n",
       " \"'the'\",\n",
       " \"'bible'\",\n",
       " \"'is'\",\n",
       " \"'clear'\",\n",
       " \"'that'\",\n",
       " \"'christians'\",\n",
       " \"'should'\",\n",
       " \"'have'\",\n",
       " \"'compassion'\",\n",
       " \"'for'\",\n",
       " \"'the'\",\n",
       " \"'poor'\",\n",
       " \"'however'\",\n",
       " \"'the'\",\n",
       " \"'federal'\",\n",
       " \"''\",\n",
       " \"'URL'\",\n",
       " \"''\",\n",
       " \"'more'\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laver modellen\n",
    "model = tf.keras.models.Sequential()\n",
    "# tilføjer et input på modellen\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# relu er default aktiverings funktion. Lav den om hvis resultatet ikke er godt nok\n",
    "model.add(tf.keras.layers.Dense(6, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(6, activation=tf.nn.relu))\n",
    "\n",
    "#antallet (10) er antal output. Det er 10 tal i datasettet derfor skal der være et 10 tal\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n",
    "\n",
    "# Dette er den mest komplexe del. adam er goto. Hvis der kun er 2 løsninger så brug binary_categorical_crossentropy eller binary_crossentropy i stedet for sparse_categorical_crossentropy\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'str'>\"}), <class 'numpy.int64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-54ad8b97353e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    531\u001b[0m                      'at same time.')\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m   \u001b[1;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 998\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    999\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'str'>\"}), <class 'numpy.int64'>"
     ]
    }
   ],
   "source": [
    "model.fit(x_train[1], y_train[1], epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 59us/sample - loss: 8.5717 - accuracy: 0.6176\n",
      "8.57165673199822 0.61764705\n"
     ]
    }
   ],
   "source": [
    "# dette for for at trække om modellen er overfittet. val_loss og val_acc skal minde meget om det man får i fittet. \n",
    "#Det er ok at loss og acc er lidt højre på testen. De må ikke være fortæt eller for længt væk fra hinaden\n",
    "val_loss, val_acc = model.evaluate(x_test,y_test)\n",
    "print(val_loss, val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
