{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           id  domain_id  type_id  \\\n22476  808451          1        0   \n9055    40330          1        0   \n3394   821607          1        0   \n5552    23931          1        0   \n80575  407069        117        3   \n\n                                                     url  \\\n22476  http://beforeitsnews.com/forex/2014/08/gbpusd-...   \n9055   http://beforeitsnews.com/stories/category/diy....   \n3394   http://beforeitsnews.com/environment/2010/11/f...   \n5552   http://beforeitsnews.com/libertarian/2013/12/t...   \n80575  http://www.abovetopsecret.com/forum/thread1171...   \n\n                                        content_tokenize  \\\n22476  ['gbpusd', 'daily', 'forecast', 'DATE', 'headl...   \n9055   ['NUM', 'by', 'show', 'tell', 'on', 'tuesday',...   \n3394   ['following', 'the', 'money', 'of', 'readers',...   \n5552   ['tampon', 'earrings', 'parody', 'of', 'reader...   \n80575  ['edit', 'on', 'DATE', 'by', 'tonycodes', 'bec...   \n\n                                                 content  \\\n22476  gbpusd daily forecast DATE headline bitcoin bl...   \n9055           NUM by show tell on tuesday DATE NUM NUM    \n3394   following the money of readers think this stor...   \n5552   tampon earrings parody of readers think this s...   \n80575  edit on DATE by tonycodes because no reason gi...   \n\n                                  title meta_description  scraped_at  \\\n22476  GBPUSD Daily Forecast: August 22             None  2018-01-25   \n9055      Stories in the \"DIY\" category             None  2018-01-25   \n3394               Following the money…             None  2018-01-25   \n5552             Tampon Earrings Parody             None  2018-01-25   \n80575   Help Mexico's Children!, page 1             None  2018-01-25   \n\n       updated_at inserted_at  \n22476  2018-02-02  2018-02-02  \n9055   2018-02-02  2018-02-02  \n3394   2018-02-02  2018-02-02  \n5552   2018-02-02  2018-02-02  \n80575  2018-02-02  2018-02-02  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain_id</th>\n      <th>type_id</th>\n      <th>url</th>\n      <th>content_tokenize</th>\n      <th>content</th>\n      <th>title</th>\n      <th>meta_description</th>\n      <th>scraped_at</th>\n      <th>updated_at</th>\n      <th>inserted_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22476</th>\n      <td>808451</td>\n      <td>1</td>\n      <td>0</td>\n      <td>http://beforeitsnews.com/forex/2014/08/gbpusd-...</td>\n      <td>['gbpusd', 'daily', 'forecast', 'DATE', 'headl...</td>\n      <td>gbpusd daily forecast DATE headline bitcoin bl...</td>\n      <td>GBPUSD Daily Forecast: August 22</td>\n      <td>None</td>\n      <td>2018-01-25</td>\n      <td>2018-02-02</td>\n      <td>2018-02-02</td>\n    </tr>\n    <tr>\n      <th>9055</th>\n      <td>40330</td>\n      <td>1</td>\n      <td>0</td>\n      <td>http://beforeitsnews.com/stories/category/diy....</td>\n      <td>['NUM', 'by', 'show', 'tell', 'on', 'tuesday',...</td>\n      <td>NUM by show tell on tuesday DATE NUM NUM</td>\n      <td>Stories in the \"DIY\" category</td>\n      <td>None</td>\n      <td>2018-01-25</td>\n      <td>2018-02-02</td>\n      <td>2018-02-02</td>\n    </tr>\n    <tr>\n      <th>3394</th>\n      <td>821607</td>\n      <td>1</td>\n      <td>0</td>\n      <td>http://beforeitsnews.com/environment/2010/11/f...</td>\n      <td>['following', 'the', 'money', 'of', 'readers',...</td>\n      <td>following the money of readers think this stor...</td>\n      <td>Following the money…</td>\n      <td>None</td>\n      <td>2018-01-25</td>\n      <td>2018-02-02</td>\n      <td>2018-02-02</td>\n    </tr>\n    <tr>\n      <th>5552</th>\n      <td>23931</td>\n      <td>1</td>\n      <td>0</td>\n      <td>http://beforeitsnews.com/libertarian/2013/12/t...</td>\n      <td>['tampon', 'earrings', 'parody', 'of', 'reader...</td>\n      <td>tampon earrings parody of readers think this s...</td>\n      <td>Tampon Earrings Parody</td>\n      <td>None</td>\n      <td>2018-01-25</td>\n      <td>2018-02-02</td>\n      <td>2018-02-02</td>\n    </tr>\n    <tr>\n      <th>80575</th>\n      <td>407069</td>\n      <td>117</td>\n      <td>3</td>\n      <td>http://www.abovetopsecret.com/forum/thread1171...</td>\n      <td>['edit', 'on', 'DATE', 'by', 'tonycodes', 'bec...</td>\n      <td>edit on DATE by tonycodes because no reason gi...</td>\n      <td>Help Mexico's Children!, page 1</td>\n      <td>None</td>\n      <td>2018-01-25</td>\n      <td>2018-02-02</td>\n      <td>2018-02-02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "### load dataset\n",
    "connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"root\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\",\n",
    "                                      database='data_science')\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "filepath = '../Data_git_ignore/clean_csv/article_clean.csv' # 250 rows of FakeNewsCorpus\n",
    "f = 0.2                                            # desired frac\n",
    "seed = 1                                           # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM fakenews.article\", connection).sample(frac=f, random_state=seed)\n",
    "#df[\"content\"] = df[\"content\"].astype(str)\n",
    "# c*reate type_id\n",
    "#df['type_id'] = df.groupby(['type']).ngroup()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# v = TfidfVectorizer()\n",
    "\n",
    "# #df['content'] = df['content'].apply(' '.join)\n",
    "\n",
    "# x = v.fit_transform(df[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c60fa94dd1c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"type_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x.toarray(), df[\"type_id\"], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> 0 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 1 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 2 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 3 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 4 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 5 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 6 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 7 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 8 (5, 2) (3, 2) (1, 2) (1, 2)\n=> 9 (5, 2) (3, 2) (1, 2) (1, 2)\n\n[Final split]\ntrain, test, validate ==> (30, 2) (10, 2) (10, 2)\n"
    }
   ],
   "source": [
    "### SELECT TRAIN, TEST, VALIDATE ###\n",
    "\n",
    "df = df[['type_id', 'content']]\n",
    "\n",
    "# This can generate a dataset with random purmutation and a max size for each type(can be smaller if desired max is not possible)\n",
    "\n",
    "# max size for type\n",
    "max_size = 5\n",
    "# traning_set ratio - splits data into traning=ratio,  test and validate=(1-ratio)/2 ex. train=80%, test=10%, validate=10%\n",
    "ratio=0.7\n",
    "\n",
    "# Labels to include - ['fake', 'satire', 'bias', 'conspiracy', 'state', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable'] - all labels\n",
    "# use_types = ['fake', 'satire', 'bias', 'conspiracy', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable'] # <- string labes\n",
    "use_types = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # <- int labels\n",
    "\n",
    "# Random seed\n",
    "rnd = 1\n",
    "\n",
    "# initialize dataframes\n",
    "train    = pd.DataFrame(columns = df.columns)\n",
    "test     = pd.DataFrame(columns = df.columns)\n",
    "validate = pd.DataFrame(columns = df.columns)\n",
    "\n",
    "# add type to test splits\n",
    "for t in use_types:\n",
    "\n",
    "    # type size\n",
    "    type_size = df['type_id'].loc[df['type_id'] == t].value_counts().min()\n",
    "\n",
    "    # set size of type slice\n",
    "    if type_size < max_size:\n",
    "        tmp = df.loc[df['type_id'] == t].sample(n = type_size, random_state=rnd)\n",
    "    else:\n",
    "        tmp = df.loc[df['type_id'] == t].sample(n = max_size, random_state=rnd)\n",
    "\n",
    "    # split current type\n",
    "    train_tmp, test_tmp, validate_tmp = np.split(tmp, [int(ratio * len(tmp)), int(((1-ratio)/2 + ratio) * len(tmp))])\n",
    "\n",
    "    # add tmp to dataframes\n",
    "    train    = pd.concat([train, train_tmp])\n",
    "    test     = pd.concat([test, test_tmp])\n",
    "    validate = pd.concat([validate, validate_tmp])\n",
    "    \n",
    "    # print split shape\n",
    "    print(\"=>\", t, tmp.shape, train_tmp.shape, validate_tmp.shape, validate_tmp.shape)\n",
    "\n",
    "print(\"\\n[Final split]\\ntrain, test, validate ==>\", train.shape, test.shape, validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train    = train['content']\n",
    "y_train    = train['type']\n",
    "X_test     = test['content']\n",
    "y_test     = test['type']\n",
    "X_validate = validate['content']\n",
    "y_validate = validate['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laver modellen\n",
    "model = tf.keras.models.Sequential()\n",
    "# tilføjer et input på modellen\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# relu er default aktiverings funktion. Lav den om hvis resultatet ikke er godt nok\n",
    "model.add(tf.keras.layers.Dense(6, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(6, activation=tf.nn.relu))\n",
    "\n",
    "#antallet (10) er antal output. Det er 10 tal i datasettet derfor skal der være et 10 tal\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n",
    "\n",
    "# Dette er den mest komplexe del. adam er goto. Hvis der kun er 2 løsninger så brug binary_categorical_crossentropy eller binary_crossentropy i stedet for sparse_categorical_crossentropy\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-63ddeecf7cd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 5s 2ms/sample - loss: 0.0928 - accuracy: 0.9675\n",
      "0.09277646052340667 0.9675\n"
     ]
    }
   ],
   "source": [
    "# dette for for at trække om modellen er overfittet. val_loss og val_acc skal minde meget om det man får i fittet. \n",
    "#Det er ok at loss og acc er lidt højre på testen. De må ikke være fortæt eller for længt væk fra hinaden\n",
    "val_loss, val_acc = model.evaluate(X_test,y_test.values)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('new': conda)",
   "language": "python",
   "name": "python37764bitnewcondae2b41800e1a24f049ef50ffcb90345e7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}