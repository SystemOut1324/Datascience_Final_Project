{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            id               domain  type  \\\n1496  338715.0  theinternetpost.net  fake   \n8192   30948.0    beforeitsnews.com  fake   \n5154  819308.0    beforeitsnews.com  fake   \n2272  339605.0  theinternetpost.net  fake   \n2198   24651.0    beforeitsnews.com  fake   \n\n                                                    url  \\\n1496        https://theinternetpost.net/tag/global-war/   \n8192  http://beforeitsnews.com/spirit/2013/06/more-t...   \n5154  http://beforeitsnews.com/international/2013/01...   \n2272   https://theinternetpost.net/tag/free-trade-area/   \n2198  http://beforeitsnews.com/business/2015/11/ente...   \n\n                                                content  \\\n1496  we are witnessing a great turning inward in th...   \n8192  more than a parade and my new manifesto of rea...   \n5154  dallas gun buyback program countered with succ...   \n2272  by paul j balles many industries that started ...   \n2198  enterprise vsat market global research and ana...   \n\n                      scraped_at                 inserted_at  \\\n1496  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n8192  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n5154  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n2272  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n2198  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n\n                      updated_at  \\\n1496  2018-02-02 01:19:41.756664   \n8192  2018-02-02 01:19:41.756664   \n5154  2018-02-02 01:19:41.756664   \n2272  2018-02-02 01:19:41.756664   \n2198  2018-02-02 01:19:41.756664   \n\n                                                  title           authors  \\\n1496                                  THE INTERNET POST               NaN   \n8192           More than a parade, and my new manifesto  Lisa Beck Living   \n5154  Dallas Gun Buyback Program Countered With Succ...               NaN   \n2272                                  THE INTERNET POST               NaN   \n2198  Enterprise VSAT Market Global Research and Ana...               NaN   \n\n      keywords meta_keywords  \\\n1496       NaN           NaN   \n8192       NaN           NaN   \n5154       NaN           NaN   \n2272       NaN           NaN   \n2198       NaN           NaN   \n\n                                       meta_description  \\\n1496  Posts about Global War written by ajfloyd and ...   \n8192                                                NaN   \n5154                                                NaN   \n2272     Posts about Free trade area written by ajfloyd   \n2198                                                NaN   \n\n                                                   tags  summary  source  \\\n1496  global conflict, war on terror, United States,...      NaN     NaN   \n8192                                                NaN      NaN     NaN   \n5154                                                NaN      NaN     NaN   \n2272  employment, Free trade agreement, United State...      NaN     NaN   \n2198                                                NaN      NaN     NaN   \n\n      type_id                                   content_tokenize  \n1496        3  ['we', 'are', 'witnessing', 'a', 'great', 'tur...  \n8192        3  ['more', 'than', 'a', 'parade', 'and', 'my', '...  \n5154        3  ['dallas', 'gun', 'buyback', 'program', 'count...  \n2272        3  ['by', 'paul', 'j', 'balles', 'many', 'industr...  \n2198        3  ['enterprise', 'vsat', 'market', 'global', 're...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n      <th>content_tokenize</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1496</th>\n      <td>338715.0</td>\n      <td>theinternetpost.net</td>\n      <td>fake</td>\n      <td>https://theinternetpost.net/tag/global-war/</td>\n      <td>we are witnessing a great turning inward in th...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>THE INTERNET POST</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Posts about Global War written by ajfloyd and ...</td>\n      <td>global conflict, war on terror, United States,...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['we', 'are', 'witnessing', 'a', 'great', 'tur...</td>\n    </tr>\n    <tr>\n      <th>8192</th>\n      <td>30948.0</td>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>http://beforeitsnews.com/spirit/2013/06/more-t...</td>\n      <td>more than a parade and my new manifesto of rea...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>More than a parade, and my new manifesto</td>\n      <td>Lisa Beck Living</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['more', 'than', 'a', 'parade', 'and', 'my', '...</td>\n    </tr>\n    <tr>\n      <th>5154</th>\n      <td>819308.0</td>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>http://beforeitsnews.com/international/2013/01...</td>\n      <td>dallas gun buyback program countered with succ...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Dallas Gun Buyback Program Countered With Succ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['dallas', 'gun', 'buyback', 'program', 'count...</td>\n    </tr>\n    <tr>\n      <th>2272</th>\n      <td>339605.0</td>\n      <td>theinternetpost.net</td>\n      <td>fake</td>\n      <td>https://theinternetpost.net/tag/free-trade-area/</td>\n      <td>by paul j balles many industries that started ...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>THE INTERNET POST</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Posts about Free trade area written by ajfloyd</td>\n      <td>employment, Free trade agreement, United State...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['by', 'paul', 'j', 'balles', 'many', 'industr...</td>\n    </tr>\n    <tr>\n      <th>2198</th>\n      <td>24651.0</td>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>http://beforeitsnews.com/business/2015/11/ente...</td>\n      <td>enterprise vsat market global research and ana...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Enterprise VSAT Market Global Research and Ana...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['enterprise', 'vsat', 'market', 'global', 're...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "# importing and creating df (has to have type_id)\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random)\n",
    "#filepath = '../Data_sample/FakeNewsCorpus_250.csv' # 250 rows of FakeNewsCorpus\n",
    "filepath = '../Data_git_ignore/1mio-raw.csv'      # 1 mil rows raw\n",
    "filepath = '../Data_git_ignore/clean_corpus_25k.csv'\n",
    "#s = 1000000                                      # desired sample size\n",
    "#seed = 1                                         # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_csv(filepath, index_col = [0])\n",
    "df[\"content\"] = df[\"content\"].astype(str)\n",
    "# create type_id\n",
    "df['type_id'] = df.groupby(['type']).ngroup()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9924, 9925, 9926, 9927, 9928, 9929, 9930, 9931, 9932, 9933],\n      dtype='object', length=1000000)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df['id'] = pd.to_numeric(df['id'], errors = 'coerce', downcast = 'integer')\n",
    "df.dropna(subset=['id'], inplace = True)\n",
    "#df.drop_duplicates(subset = 'content', inplace = True)\n",
    "df.drop_duplicates(subset = 'id', inplace = True)\n",
    "df.id = df.id.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [id, domain, type, url, content, scraped_at, inserted_at, updated_at, title, authors, keywords, meta_keywords, meta_description, tags, summary, source, type_id]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "df.loc[df['id'] == np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [id, domain, type, url, content, scraped_at, inserted_at, updated_at, title, authors, keywords, meta_keywords, meta_description, tags, summary, source, type_id]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.loc[df['id'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2047      1\n983221    1\n995507    1\n997554    1\n991409    1\n         ..\n717812    1\n707571    1\n705522    1\n711665    1\n0         1\nLength: 999934, dtype: int64"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.reset_index(inplace = True, drop = True)\n",
    "df.index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()\n",
    "#df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupidx_lst = df.loc[df['content'].duplicated()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Uniq = df.loc[~df.index.isin(dupidx_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(660914, 17)"
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "df_Uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             id                domain type  \\\n397         463       www.cnsnews.com  NaN   \n663         751        freebeacon.com  NaN   \n664         752        freebeacon.com  NaN   \n3097       3569  www.greenmedinfo.com  NaN   \n3098       3570  www.greenmedinfo.com  NaN   \n...         ...                   ...  ...   \n993691  1163797    www.emptywheel.net  NaN   \n994996  1165136       www.newsmax.com  NaN   \n995014  1165155       www.newsmax.com  NaN   \n995132  1165273          tomwoods.com  NaN   \n997307  1167488       www.cnsnews.com  NaN   \n\n                                                      url  \\\n397     https://www.cnsnews.com/news/article/melanie-a...   \n663     http://freebeacon.com/politics/pelosi-tries-ex...   \n664     http://freebeacon.com/uncategorized/george-sor...   \n3097     http://www.greenmedinfo.com/gmi-blogs/docmalerba   \n3098    http://www.greenmedinfo.com/guide/health-guide...   \n...                                                   ...   \n993691  http://www.emptywheel.net/2013/01/25/kerry-res...   \n994996  https://www.newsmax.com/Newsmax-Tv/Ramones-Mar...   \n995014  https://www.newsmax.com/US/obituary-tommy-ramo...   \n995132       https://tomwoods.com/the-fast-food-protests/   \n997307  https://www.cnsnews.com/news/article/susan-jon...   \n\n                                                  content  \\\n397     Sen. Dick Durbin (D-Ill.) (Screenshot of C-SPA...   \n663     BY: Follow @susancrabtree\\n\\n\\n\\nHouse Minorit...   \n664     BY: Follow @JoeSchoffstall\\n\\n\\n\\nLiberal bill...   \n3097    All articles by Larry Malerba, DO\\n\\nThe Homeo...   \n3098    The Shocking Lack of Evidence Supporting Flu V...   \n...                                                   ...   \n993691  At his confirmation hearing yesterday before t...   \n994996  The Ramones famously sang \"I Wanna Be Sedated\"...   \n995014  Tommy Ramone, a co-founder of the seminal punk...   \n995132  Fast-food workers have begun protesting their ...   \n997307  The Obamacare website, HealthCare.gov. (AP)\\n\\...   \n\n                        scraped_at                 inserted_at  \\\n397     2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n663     2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n664     2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n3097    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n3098    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n...                            ...                         ...   \n993691  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n994996  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n995014  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n995132  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n997307  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n\n                        updated_at  \\\n397     2018-02-02 01:19:41.756664   \n663     2018-02-02 01:19:41.756664   \n664     2018-02-02 01:19:41.756664   \n3097    2018-02-02 01:19:41.756664   \n3098    2018-02-02 01:19:41.756664   \n...                            ...   \n993691  2018-02-02 01:19:41.756664   \n994996  2018-02-02 01:19:41.756664   \n995014  2018-02-02 01:19:41.756664   \n995132  2018-02-02 01:19:41.756664   \n997307  2018-02-02 01:19:41.756664   \n\n                                                    title  \\\n397     Sen. Dick Durbin Calls DACA ‘The Civil Rights ...   \n663     Pelosi Tries to Extend Tax Break for Two of He...   \n664     George Soros Spent Record Amount Lobbying Duri...   \n3097                    Recent Blogs by Larry Malerba, DO   \n3098                       Health Guide: Vaccine Research   \n...                                                   ...   \n993691  Kerry Resists Rand Paul on Pakistan Funding Qu...   \n994996  Marky Ramone: We Were Punk Brothers But Politi...   \n995014      Tommy Ramone, Last of the Ramones, Dies at 65   \n995132                             The Fast Food Protests   \n997307  White House Tells Obamacare Subscribers to Mak...   \n\n                                                  authors  keywords  \\\n397                                         Melanie Arter       NaN   \n663                                        Susan Crabtree       NaN   \n664                                       Joe Schoffstall       NaN   \n3097                                                  NaN       NaN   \n3098                                                  NaN       NaN   \n...                                                   ...       ...   \n993691  Jim White, John B., P J Evans, Bill Michtom, P...       NaN   \n994996                                      Bill Hoffmann       NaN   \n995014                                      Newsmax Wires       NaN   \n995132  Tom Woods, Anthony Hidalgo, Philip Rearden S G...       NaN   \n997307                                        Susan Jones       NaN   \n\n                                            meta_keywords  \\\n397                                        ['Washington']   \n663                                                  ['']   \n664                                                  ['']   \n3097    ['natural medicine', 'integrative health', 'al...   \n3098    ['natural medicine', 'integrative medicine', '...   \n...                                                   ...   \n993691                                               ['']   \n994996   ['Ramones', 'Marky Ramone', 'band', 'political']   \n995014          ['obituary', 'tommy', 'ramone', 'cancer']   \n995132                                               ['']   \n997307                                     ['Washington']   \n\n                                         meta_description  \\\n397     Speaking on the Senate floor Monday prior to a...   \n663     House Minority Leader Nancy Pelosi took pains ...   \n664     Liberal billionaire George Soros spent more on...   \n3097    Greenmedinfo.com - Natural Health Resource - T...   \n3098    Greenmedinfo.com - Natural Medical Resource - ...   \n...                                                   ...   \n993691                                                NaN   \n994996  The Ramones famously sang  I Wanna Be Sedated ...   \n995014  Tommy Ramone, a co-founder of the seminal punk...   \n995132  Fast-food workers have begun protesting their ...   \n997307                                                NaN   \n\n                                                     tags  summary  source  \\\n397                                                   NaN      NaN     NaN   \n663                                   Nancy Pelosi, Taxes      NaN     NaN   \n664               George Soros, Donald Trump, David Brock      NaN     NaN   \n3097                                                  NaN      NaN     NaN   \n3098                                                  NaN      NaN     NaN   \n...                                                   ...      ...     ...   \n993691  Pakistan, Rand Paul, John Kerry, Secretary of ...      NaN     NaN   \n994996  Trump Administration, Steve Malzberg Show, ISI...      NaN     NaN   \n995014  Donald Trump, Russia, Trump Administration, Gu...      NaN     NaN   \n995132                                                NaN      NaN     NaN   \n997307                                                NaN      NaN     NaN   \n\n        type_id  \n397          -1  \n663          -1  \n664          -1  \n3097         -1  \n3098         -1  \n...         ...  \n993691       -1  \n994996       -1  \n995014       -1  \n995132       -1  \n997307       -1  \n\n[32712 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>397</td>\n      <td>463</td>\n      <td>www.cnsnews.com</td>\n      <td>NaN</td>\n      <td>https://www.cnsnews.com/news/article/melanie-a...</td>\n      <td>Sen. Dick Durbin (D-Ill.) (Screenshot of C-SPA...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Sen. Dick Durbin Calls DACA ‘The Civil Rights ...</td>\n      <td>Melanie Arter</td>\n      <td>NaN</td>\n      <td>['Washington']</td>\n      <td>Speaking on the Senate floor Monday prior to a...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>663</td>\n      <td>751</td>\n      <td>freebeacon.com</td>\n      <td>NaN</td>\n      <td>http://freebeacon.com/politics/pelosi-tries-ex...</td>\n      <td>BY: Follow @susancrabtree\\n\\n\\n\\nHouse Minorit...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Pelosi Tries to Extend Tax Break for Two of He...</td>\n      <td>Susan Crabtree</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>House Minority Leader Nancy Pelosi took pains ...</td>\n      <td>Nancy Pelosi, Taxes</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>664</td>\n      <td>752</td>\n      <td>freebeacon.com</td>\n      <td>NaN</td>\n      <td>http://freebeacon.com/uncategorized/george-sor...</td>\n      <td>BY: Follow @JoeSchoffstall\\n\\n\\n\\nLiberal bill...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>George Soros Spent Record Amount Lobbying Duri...</td>\n      <td>Joe Schoffstall</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>Liberal billionaire George Soros spent more on...</td>\n      <td>George Soros, Donald Trump, David Brock</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>3097</td>\n      <td>3569</td>\n      <td>www.greenmedinfo.com</td>\n      <td>NaN</td>\n      <td>http://www.greenmedinfo.com/gmi-blogs/docmalerba</td>\n      <td>All articles by Larry Malerba, DO\\n\\nThe Homeo...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Recent Blogs by Larry Malerba, DO</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['natural medicine', 'integrative health', 'al...</td>\n      <td>Greenmedinfo.com - Natural Health Resource - T...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>3098</td>\n      <td>3570</td>\n      <td>www.greenmedinfo.com</td>\n      <td>NaN</td>\n      <td>http://www.greenmedinfo.com/guide/health-guide...</td>\n      <td>The Shocking Lack of Evidence Supporting Flu V...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Health Guide: Vaccine Research</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['natural medicine', 'integrative medicine', '...</td>\n      <td>Greenmedinfo.com - Natural Medical Resource - ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>993691</td>\n      <td>1163797</td>\n      <td>www.emptywheel.net</td>\n      <td>NaN</td>\n      <td>http://www.emptywheel.net/2013/01/25/kerry-res...</td>\n      <td>At his confirmation hearing yesterday before t...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Kerry Resists Rand Paul on Pakistan Funding Qu...</td>\n      <td>Jim White, John B., P J Evans, Bill Michtom, P...</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>Pakistan, Rand Paul, John Kerry, Secretary of ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>994996</td>\n      <td>1165136</td>\n      <td>www.newsmax.com</td>\n      <td>NaN</td>\n      <td>https://www.newsmax.com/Newsmax-Tv/Ramones-Mar...</td>\n      <td>The Ramones famously sang \"I Wanna Be Sedated\"...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Marky Ramone: We Were Punk Brothers But Politi...</td>\n      <td>Bill Hoffmann</td>\n      <td>NaN</td>\n      <td>['Ramones', 'Marky Ramone', 'band', 'political']</td>\n      <td>The Ramones famously sang  I Wanna Be Sedated ...</td>\n      <td>Trump Administration, Steve Malzberg Show, ISI...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>995014</td>\n      <td>1165155</td>\n      <td>www.newsmax.com</td>\n      <td>NaN</td>\n      <td>https://www.newsmax.com/US/obituary-tommy-ramo...</td>\n      <td>Tommy Ramone, a co-founder of the seminal punk...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Tommy Ramone, Last of the Ramones, Dies at 65</td>\n      <td>Newsmax Wires</td>\n      <td>NaN</td>\n      <td>['obituary', 'tommy', 'ramone', 'cancer']</td>\n      <td>Tommy Ramone, a co-founder of the seminal punk...</td>\n      <td>Donald Trump, Russia, Trump Administration, Gu...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>995132</td>\n      <td>1165273</td>\n      <td>tomwoods.com</td>\n      <td>NaN</td>\n      <td>https://tomwoods.com/the-fast-food-protests/</td>\n      <td>Fast-food workers have begun protesting their ...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>The Fast Food Protests</td>\n      <td>Tom Woods, Anthony Hidalgo, Philip Rearden S G...</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>Fast-food workers have begun protesting their ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>997307</td>\n      <td>1167488</td>\n      <td>www.cnsnews.com</td>\n      <td>NaN</td>\n      <td>https://www.cnsnews.com/news/article/susan-jon...</td>\n      <td>The Obamacare website, HealthCare.gov. (AP)\\n\\...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>White House Tells Obamacare Subscribers to Mak...</td>\n      <td>Susan Jones</td>\n      <td>NaN</td>\n      <td>['Washington']</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>32712 rows × 17 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "df_Uniq.loc[df_Uniq.type_id == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5222"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df_Uniq['type'].loc[df_Uniq['type'] == 'reliable'].value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Int64Index([     0,      1,      2,      3,      4,      5,      6,      7,\n                 8,      9,\n            ...\n            999868, 999880, 999881, 999882, 999883, 999895, 999906, 999913,\n            999923, 999927],\n           dtype='int64', length=660914)"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "df_Uniq.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "660914"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_Uniq.content.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> fake (7000, 17) (5600, 17) (1400, 17)\n=> satire (7000, 17) (5600, 17) (1400, 17)\n=> bias (7000, 17) (5600, 17) (1400, 17)\n=> conspiracy (7000, 17) (5600, 17) (1400, 17)\n=> junksci (7000, 17) (5600, 17) (1400, 17)\n=> hate (3112, 17) (2489, 17) (623, 17)\n=> clickbait (7000, 17) (5600, 17) (1400, 17)\n=> unreliable (7000, 17) (5600, 17) (1400, 17)\n=> political (7000, 17) (5600, 17) (1400, 17)\n=> reliable (5222, 17) (4177, 17) (1045, 17)\n\n[Final split]\ntrain, test==> (51466, 17) (12868, 17)\n"
    }
   ],
   "source": [
    "#USE THIS\n",
    "# This can generate a dataset with random purmutation and a max size for each type(can be smaller if desired max is not possible)\n",
    "\n",
    "# max size for type\n",
    "max_size = 7000\n",
    "# traning_set ratio - splits data into traning=ratio,  test and validate=(1-ratio)/2 ex. train=80%, test=10%, validate=10%\n",
    "ratio=0.8\n",
    "# Labels to include - ['fake', 'satire', 'bias', 'conspiracy', 'state', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable'] - all labels\n",
    "use_types = ['fake', 'satire', 'bias', 'conspiracy', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable', 'rumor']\n",
    "# Random seed\n",
    "rnd = 1\n",
    "\n",
    "# initialize dataframes\n",
    "train    = pd.DataFrame(columns = df_Uniq.columns)\n",
    "test     = pd.DataFrame(columns = df_Uniq.columns)\n",
    "\n",
    "# add type to test splits\n",
    "for t in use_types:\n",
    "\n",
    "    # type size\n",
    "    type_size = df_Uniq['type'].loc[df_Uniq['type'] == t].value_counts().min()\n",
    "\n",
    "    # set size of type slice\n",
    "    if type_size < max_size:\n",
    "        tmp = df_Uniq.loc[df_Uniq['type'] == t].sample(n = type_size, random_state=rnd)\n",
    "    else:\n",
    "        tmp = df_Uniq.loc[df_Uniq['type'] == t].sample(n = max_size, random_state=rnd)\n",
    "\n",
    "    # split current type\n",
    "    train_tmp, test_tmp = np.split(tmp, [int(ratio * len(tmp))])\n",
    "\n",
    "    # add tmp to dataframes\n",
    "    train    = pd.concat([train, train_tmp])\n",
    "    test     = pd.concat([test, test_tmp])\n",
    "    \n",
    "    # print split shape\n",
    "    print(\"=>\", t, tmp.shape, train_tmp.shape, test_tmp.shape)\n",
    "\n",
    "print(\"\\n[Final split]\\ntrain, test==>\", train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [id_x, domain_x, type_x, url_x, content, scraped_at_x, inserted_at_x, updated_at_x, title_x, authors_x, keywords_x, meta_keywords_x, meta_description_x, tags_x, summary_x, source_x, type_id_x, id_y, domain_y, type_y, url_y, scraped_at_y, inserted_at_y, updated_at_y, title_y, authors_y, keywords_y, meta_keywords_y, meta_description_y, tags_y, summary_y, source_y, type_id_y]\nIndex: []\n\n[0 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_x</th>\n      <th>domain_x</th>\n      <th>type_x</th>\n      <th>url_x</th>\n      <th>content</th>\n      <th>scraped_at_x</th>\n      <th>inserted_at_x</th>\n      <th>updated_at_x</th>\n      <th>title_x</th>\n      <th>authors_x</th>\n      <th>...</th>\n      <th>updated_at_y</th>\n      <th>title_y</th>\n      <th>authors_y</th>\n      <th>keywords_y</th>\n      <th>meta_keywords_y</th>\n      <th>meta_description_y</th>\n      <th>tags_y</th>\n      <th>summary_y</th>\n      <th>source_y</th>\n      <th>type_id_y</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 33 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "pd.merge(train,test, on =['content'], how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for faster calculation for details, dont run normally\n",
    "# importing and creating df (has to have type_id)\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "#filepath = '/Users/Master/Documents/KU/2.Semester/Datascience/news_sample.csv' #\n",
    "#s = 250                                            # desired sample size\n",
    "#seed = 1                                           # seed used by Pseudorandom number generator\n",
    "\n",
    "#df = pd.read_csv(filepath, index_col = [0])\n",
    "#content = df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec \n",
    "from gensim.utils import tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.set_index('id', inplace = True)\n",
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train, random_state = 1)\n",
    "test = shuffle(test, random_state = 1)\n",
    "train.reset_index(inplace = True, drop = True)\n",
    "test.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['this', 'is', 'to', 'token']"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "#this is how the tokenizer works\n",
    "list(tokenize(\"This , is 5to TOKEN\", lowercase = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 0 ns\n"
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "series_content = train.content\n",
    "#contTok = series_content.apply(nltk.word_tokenize)\n",
    "typeLst = train.type_id.tolist()\n",
    "#np.unique(typeLst, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = test.content\n",
    "test_typeLst = test.type_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_content\n",
    "#test_typeLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0        Mysterious metal spheres from space have lande...\n1        Email this article\\n\\nManitoba Physicians Coll...\n2        Why hospitals are a safe haven for hacking\\n\\n...\n3        Addison Wiggin\\n\\nEvery month, JPMorgan Chase ...\n4        LONDON - England - The lying fantasist and gol...\n                               ...                        \n12863    Put differently, they are nothing but snarky u...\n12864    On the back of reports that the White Helmets ...\n12865    Search by property\\n\\nA list of all pages that...\n12866    The views expressed by the author do not neces...\n12867    US Government Found GUILTY Of Murdering Martin...\nName: content, Length: 12868, dtype: object"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "test_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try gensim tokenizer\n",
    "testTok = test_content.apply(tokenize,lowercase =True)\n",
    "for i in testTok.index:\n",
    "    testTok[i] = list(testTok[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try gensim tokenizer\n",
    "contTok = series_content.apply(tokenize,lowercase =True)\n",
    "for i in contTok.index:\n",
    "    contTok[i] = list(contTok[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe fallback tokenizer\n",
    "#contTok1 = series_content.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'unreliable'"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "train.type[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(contTok, [i]) for i, contTok in enumerate(contTok)]\n",
    "#contTok[291496]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(enumerate(contTok))[0]\n",
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "020-06-10 14:06:06,299 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:06:06,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:06:06,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:06:06,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:06:06,301 : INFO : EPOCH - 9 : training on 33824106 raw words (26563430 effective words) took 14.6s, 1822045 effective words/s\n2020-06-10 14:06:07,316 : INFO : EPOCH 10 - PROGRESS: at 6.65% examples, 1776201 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:08,318 : INFO : EPOCH 10 - PROGRESS: at 13.51% examples, 1814294 words/s, in_qsize 29, out_qsize 1\n2020-06-10 14:06:09,324 : INFO : EPOCH 10 - PROGRESS: at 20.55% examples, 1807446 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:10,331 : INFO : EPOCH 10 - PROGRESS: at 27.66% examples, 1804961 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:06:11,338 : INFO : EPOCH 10 - PROGRESS: at 34.36% examples, 1812101 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:06:12,348 : INFO : EPOCH 10 - PROGRESS: at 41.11% examples, 1811573 words/s, in_qsize 30, out_qsize 2\n2020-06-10 14:06:13,349 : INFO : EPOCH 10 - PROGRESS: at 47.89% examples, 1816474 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:14,352 : INFO : EPOCH 10 - PROGRESS: at 54.62% examples, 1819276 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:15,355 : INFO : EPOCH 10 - PROGRESS: at 61.43% examples, 1814652 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:06:16,359 : INFO : EPOCH 10 - PROGRESS: at 68.38% examples, 1813640 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:17,358 : INFO : EPOCH 10 - PROGRESS: at 75.24% examples, 1814529 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:18,369 : INFO : EPOCH 10 - PROGRESS: at 82.22% examples, 1813128 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:19,376 : INFO : EPOCH 10 - PROGRESS: at 89.04% examples, 1810215 words/s, in_qsize 32, out_qsize 5\n2020-06-10 14:06:20,380 : INFO : EPOCH 10 - PROGRESS: at 96.05% examples, 1812245 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:20,899 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:06:20,910 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:06:20,918 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:06:20,926 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:06:20,929 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:06:20,929 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:06:20,930 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:06:20,931 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:06:20,931 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:06:20,932 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:06:20,933 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:06:20,933 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:06:20,933 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:06:20,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:06:20,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:06:20,934 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:06:20,934 : INFO : EPOCH - 10 : training on 33824106 raw words (26564647 effective words) took 14.6s, 1816193 effective words/s\n2020-06-10 14:06:21,949 : INFO : EPOCH 11 - PROGRESS: at 6.79% examples, 1807799 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:22,954 : INFO : EPOCH 11 - PROGRESS: at 13.59% examples, 1825088 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:23,956 : INFO : EPOCH 11 - PROGRESS: at 20.64% examples, 1816008 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:24,957 : INFO : EPOCH 11 - PROGRESS: at 27.89% examples, 1825355 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:25,977 : INFO : EPOCH 11 - PROGRESS: at 34.52% examples, 1819819 words/s, in_qsize 28, out_qsize 4\n2020-06-10 14:06:26,985 : INFO : EPOCH 11 - PROGRESS: at 41.25% examples, 1819372 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:06:27,987 : INFO : EPOCH 11 - PROGRESS: at 48.09% examples, 1822891 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:28,994 : INFO : EPOCH 11 - PROGRESS: at 54.79% examples, 1823096 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:06:30,003 : INFO : EPOCH 11 - PROGRESS: at 61.72% examples, 1821812 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:31,011 : INFO : EPOCH 11 - PROGRESS: at 68.75% examples, 1819950 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:32,012 : INFO : EPOCH 11 - PROGRESS: at 75.65% examples, 1820988 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:33,019 : INFO : EPOCH 11 - PROGRESS: at 82.60% examples, 1819202 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:06:34,024 : INFO : EPOCH 11 - PROGRESS: at 89.51% examples, 1818333 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:35,026 : INFO : EPOCH 11 - PROGRESS: at 96.37% examples, 1817331 words/s, in_qsize 32, out_qsize 4\n2020-06-10 14:06:35,486 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:06:35,506 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:06:35,511 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:06:35,515 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:06:35,516 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:06:35,518 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:06:35,518 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:06:35,519 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:06:35,519 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:06:35,520 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:06:35,520 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:06:35,521 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:06:35,521 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:06:35,521 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:06:35,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:06:35,522 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:06:35,522 : INFO : EPOCH - 11 : training on 33824106 raw words (26565615 effective words) took 14.6s, 1822027 effective words/s\n2020-06-10 14:06:36,538 : INFO : EPOCH 12 - PROGRESS: at 6.64% examples, 1766219 words/s, in_qsize 29, out_qsize 3\n2020-06-10 14:06:37,543 : INFO : EPOCH 12 - PROGRESS: at 13.60% examples, 1821287 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:38,547 : INFO : EPOCH 12 - PROGRESS: at 20.63% examples, 1811731 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:39,548 : INFO : EPOCH 12 - PROGRESS: at 27.80% examples, 1817948 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:40,552 : INFO : EPOCH 12 - PROGRESS: at 34.41% examples, 1819809 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:41,553 : INFO : EPOCH 12 - PROGRESS: at 41.12% examples, 1817799 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:42,557 : INFO : EPOCH 12 - PROGRESS: at 47.91% examples, 1820600 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:06:43,572 : INFO : EPOCH 12 - PROGRESS: at 54.58% examples, 1818510 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:44,574 : INFO : EPOCH 12 - PROGRESS: at 61.62% examples, 1821367 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:45,583 : INFO : EPOCH 12 - PROGRESS: at 68.46% examples, 1815085 words/s, in_qsize 32, out_qsize 3\n2020-06-10 14:06:46,589 : INFO : EPOCH 12 - PROGRESS: at 75.26% examples, 1813775 words/s, in_qsize 29, out_qsize 2\n2020-06-10 14:06:47,595 : INFO : EPOCH 12 - PROGRESS: at 82.26% examples, 1813137 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:48,596 : INFO : EPOCH 12 - PROGRESS: at 89.21% examples, 1814273 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:49,596 : INFO : EPOCH 12 - PROGRESS: at 96.14% examples, 1814548 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:06:50,103 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:06:50,114 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:06:50,125 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:06:50,131 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:06:50,132 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:06:50,133 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:06:50,134 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:06:50,136 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:06:50,136 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:06:50,136 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:06:50,137 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:06:50,137 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:06:50,138 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:06:50,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:06:50,139 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:06:50,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:06:50,140 : INFO : EPOCH - 12 : training on 33824106 raw words (26564811 effective words) took 14.6s, 1818102 effective words/s\n2020-06-10 14:06:51,153 : INFO : EPOCH 13 - PROGRESS: at 6.63% examples, 1769876 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:52,156 : INFO : EPOCH 13 - PROGRESS: at 13.51% examples, 1815241 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:53,166 : INFO : EPOCH 13 - PROGRESS: at 20.59% examples, 1808083 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:06:54,167 : INFO : EPOCH 13 - PROGRESS: at 27.71% examples, 1810139 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:06:55,178 : INFO : EPOCH 13 - PROGRESS: at 34.35% examples, 1811271 words/s, in_qsize 30, out_qsize 3\n2020-06-10 14:06:56,181 : INFO : EPOCH 13 - PROGRESS: at 41.18% examples, 1817031 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:57,188 : INFO : EPOCH 13 - PROGRESS: at 48.07% examples, 1822438 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:58,203 : INFO : EPOCH 13 - PROGRESS: at 54.74% examples, 1819923 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:06:59,217 : INFO : EPOCH 13 - PROGRESS: at 61.70% examples, 1819912 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:07:00,218 : INFO : EPOCH 13 - PROGRESS: at 68.77% examples, 1820019 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:01,218 : INFO : EPOCH 13 - PROGRESS: at 75.64% examples, 1820494 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:02,221 : INFO : EPOCH 13 - PROGRESS: at 82.61% examples, 1819818 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:03,235 : INFO : EPOCH 13 - PROGRESS: at 89.57% examples, 1818845 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:07:04,248 : INFO : EPOCH 13 - PROGRESS: at 96.57% examples, 1818951 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:07:04,680 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:07:04,697 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:07:04,703 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:07:04,705 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:07:04,708 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:07:04,711 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:07:04,712 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:07:04,713 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:07:04,713 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:07:04,714 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:07:04,714 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:07:04,715 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:07:04,715 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:07:04,715 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:07:04,715 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:07:04,717 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:07:04,717 : INFO : EPOCH - 13 : training on 33824106 raw words (26563231 effective words) took 14.6s, 1823117 effective words/s\n2020-06-10 14:07:05,725 : INFO : EPOCH 14 - PROGRESS: at 6.65% examples, 1778712 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:06,731 : INFO : EPOCH 14 - PROGRESS: at 13.52% examples, 1815649 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:07,736 : INFO : EPOCH 14 - PROGRESS: at 20.63% examples, 1814342 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:08,738 : INFO : EPOCH 14 - PROGRESS: at 27.84% examples, 1821778 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:09,738 : INFO : EPOCH 14 - PROGRESS: at 34.42% examples, 1822225 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:10,739 : INFO : EPOCH 14 - PROGRESS: at 41.16% examples, 1821374 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:11,744 : INFO : EPOCH 14 - PROGRESS: at 47.96% examples, 1823757 words/s, in_qsize 30, out_qsize 4\n2020-06-10 14:07:12,744 : INFO : EPOCH 14 - PROGRESS: at 54.70% examples, 1826199 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:07:13,751 : INFO : EPOCH 14 - PROGRESS: at 61.63% examples, 1825638 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:14,758 : INFO : EPOCH 14 - PROGRESS: at 68.73% examples, 1825143 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:15,759 : INFO : EPOCH 14 - PROGRESS: at 75.59% examples, 1825144 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:16,762 : INFO : EPOCH 14 - PROGRESS: at 82.54% examples, 1823825 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:17,768 : INFO : EPOCH 14 - PROGRESS: at 89.47% examples, 1822963 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:18,769 : INFO : EPOCH 14 - PROGRESS: at 96.29% examples, 1821262 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:19,245 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:07:19,250 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:07:19,264 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:07:19,270 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:07:19,275 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:07:19,276 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:07:19,278 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:07:19,279 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:07:19,279 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:07:19,279 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:07:19,280 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:07:19,280 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:07:19,281 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:07:19,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:07:19,281 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:07:19,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:07:19,282 : INFO : EPOCH - 14 : training on 33824106 raw words (26566299 effective words) took 14.6s, 1824762 effective words/s\n2020-06-10 14:07:20,292 : INFO : EPOCH 15 - PROGRESS: at 6.78% examples, 1812009 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:21,294 : INFO : EPOCH 15 - PROGRESS: at 13.57% examples, 1825861 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:22,295 : INFO : EPOCH 15 - PROGRESS: at 20.64% examples, 1820486 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:07:23,296 : INFO : EPOCH 15 - PROGRESS: at 27.81% examples, 1823130 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:24,301 : INFO : EPOCH 15 - PROGRESS: at 34.44% examples, 1825186 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:07:25,303 : INFO : EPOCH 15 - PROGRESS: at 41.27% examples, 1826611 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:26,303 : INFO : EPOCH 15 - PROGRESS: at 48.07% examples, 1829340 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:27,315 : INFO : EPOCH 15 - PROGRESS: at 54.70% examples, 1825664 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:07:28,322 : INFO : EPOCH 15 - PROGRESS: at 61.64% examples, 1824467 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:29,341 : INFO : EPOCH 15 - PROGRESS: at 68.73% examples, 1821795 words/s, in_qsize 32, out_qsize 5\n2020-06-10 14:07:30,343 : INFO : EPOCH 15 - PROGRESS: at 75.60% examples, 1822532 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:31,349 : INFO : EPOCH 15 - PROGRESS: at 82.59% examples, 1821412 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:07:32,351 : INFO : EPOCH 15 - PROGRESS: at 89.59% examples, 1822280 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:33,353 : INFO : EPOCH 15 - PROGRESS: at 96.55% examples, 1823249 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:33,811 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:07:33,827 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:07:33,829 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:07:33,835 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:07:33,838 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:07:33,839 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:07:33,841 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:07:33,842 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:07:33,843 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:07:33,843 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:07:33,844 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:07:33,844 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:07:33,845 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:07:33,845 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:07:33,845 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:07:33,846 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:07:33,846 : INFO : EPOCH - 15 : training on 33824106 raw words (26562783 effective words) took 14.6s, 1824592 effective words/s\n2020-06-10 14:07:33,847 : INFO : training on a 507361590 raw words (398470512 effective words) took 221.0s, 1803221 effective words/s\n"
    }
   ],
   "source": [
    "model2 = Doc2Vec(documents, vector_size = 300, dm = 0, epochs=15, workers = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3331\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-181-11141c9578dd>\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', 'model1 = Doc2Vec(documents, vector_size = , min_count = 3, window = 10, epochs=10, workers = 4)\\n')\n",
      "  File \u001b[0;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2362\u001b[0m, in \u001b[0;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[0;32m\"<decorator-gen-62>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35mtime\u001b[0m\n",
      "  File \u001b[0;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\magic.py\"\u001b[0m, line \u001b[0;32m187\u001b[0m, in \u001b[0;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[0;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\magics\\execution.py\"\u001b[0m, line \u001b[0;32m1268\u001b[0m, in \u001b[0;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\compilerop.py\"\u001b[1;36m, line \u001b[1;32m101\u001b[1;36m, in \u001b[1;35mast_parse\u001b[1;36m\u001b[0m\n\u001b[1;33m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<unknown>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    model1 = Doc2Vec(documents, vector_size = , min_count = 3, window = 10, epochs=10, workers = 4)\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model1 = Doc2Vec(documents, vector_size = , min_count = 3, window = 10, epochs=10, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eventuel træn andre modeller herefter:\n",
    "#%%time\n",
    "#model2 = Doc2Vec(documents, vector_size = 200, dm = 1, min_count = 3, window = 10, epochs= 10, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-10 14:07:58,329 : INFO : precomputing L2-norms of word weight vectors\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('inhabits', 0.23516283929347992),\n ('trio', 0.23322972655296326),\n ('су', 0.23255932331085205),\n ('rem', 0.2267533838748932),\n ('финансирование', 0.22597454488277435),\n ('seis', 0.22557631134986877),\n ('parigi', 0.2229282408952713),\n ('ciavarella', 0.21694324910640717),\n ('nita', 0.21591304242610931),\n ('obeying', 0.2151620239019394)]"
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "model2.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51466"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "model2.docvecs.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_list = model2.docvecs.most_similar(200)[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-be2f5dc91119>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfirst_tuple_elements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma_tuple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuple_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfirst_tuple_elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-183-be2f5dc91119>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfirst_tuple_elements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma_tuple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuple_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfirst_tuple_elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "first_tuple_elements = [a_tuple[0] for a_tuple in tuple_list]\n",
    "first_tuple_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "39974    fake\nName: type, dtype: object"
     },
     "metadata": {},
     "execution_count": 457
    }
   ],
   "source": [
    "train.type.loc[train.type.index == 39974]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.type.loc[train.index == 38424]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array(['bias', 'conspiracy', 'fake'], dtype=object),\n array([8, 1, 1], dtype=int64))"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "#See types of most similar documents for model2\n",
    "#for idx in train.type.loc[train.type == 'fake'].index:\n",
    "sim_labels1 = np.empty(10,dtype='object')\n",
    "for i in range(len(sim_labels1)):\n",
    "    sim_labels1[i] = train.type.loc[train.type.index == model2.docvecs.most_similar(16)[i][0]].values[0]\n",
    "\n",
    "# amount of articles with given type equal to testing article\n",
    "np.unique(sim_labels1,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "16       bias\n21       bias\n23       bias\n34       bias\n43       bias\n         ... \n51398    bias\n51399    bias\n51400    bias\n51416    bias\n51464    bias\nName: type, Length: 5600, dtype: object"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "#Lookup indeices for articles with certain types\n",
    "train.type.loc[train.type == 'bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-02105199d898>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Model 1 traindata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "#Model 1 traindata\n",
    "train_arrays = np.zeros((model1.docvecs.count,150),dtype=\"float32\")\n",
    "train_labels = np.zeros(model1.docvecs.count,dtype=\"float32\")\n",
    "for i in range(model1.docvecs.count):\n",
    "    train_arrays[i] = model1.docvecs[i]\n",
    "    train_labels[i] = train.type_id[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51466"
     },
     "metadata": {},
     "execution_count": 477
    }
   ],
   "source": [
    "model2.docvecs.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 traindata:\n",
    "train_arrays2 = np.zeros((model2.docvecs.count,300),dtype=\"float32\")\n",
    "train_labels2 = np.zeros(model2.docvecs.count,dtype=\"float32\")\n",
    "for i in range(model2.docvecs.count):\n",
    "    train_arrays2[i] = model2.docvecs[i]\n",
    "    train_labels2[i] = train.type_id[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(train_labels, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_arrays[0]\n",
    "#train_labels[0]\n",
    "#model1.docvecs.most_similar([train_arrays[0]])\n",
    "#train.loc[train.index == 37400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-75a43f0ef60d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#How infer_vector works:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"this is new sentence\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnew_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_vector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#model2.docvecs.most_similar([new_vec])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "#How infer_vector works:\n",
    "token = \"this is new sentence\".split()\n",
    "new_vector = model1.infer_vector(token)\n",
    "sims = model1.docvecs.most_similar([new_vector])\n",
    "#model2.docvecs.most_similar([new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_vec = model2.infer_vector(contTok[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.docvecs.most_similar([new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-1640ee33e506>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtest_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestTok\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_typeLst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "#Inferring vectors for testdata with model1\n",
    "test_arrays = np.zeros((len(test),150),dtype = \"float32\")\n",
    "for i in range(len(test)):\n",
    "    test_arrays[i] = model1.infer_vector(testTok[i],steps = 15)   \n",
    "test_labels = np.array(test_typeLst,dtype=\"float32\")\n",
    "\n",
    "#test_arrays = np.zeros((model1.docvecs.count,150),dtype=\"float32\")\n",
    "#test_labels = np.zeros(model1.docvecs.count,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 3min 5s\n"
    }
   ],
   "source": [
    "%%time\n",
    "#Inferring vectors for testdata with model2\n",
    "test_arrays2 = np.zeros((len(test),300),dtype = \"float32\")\n",
    "for i in range(len(test)):\n",
    "    test_arrays2[i] = model2.infer_vector(testTok[i], epochs = 20, alpha = 0.025)   \n",
    "test_labels = np.array(test_typeLst,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testTok[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_labels[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(test_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-c0c2c1a6f2f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "model1.docvecs.most_similar([test_arrays[200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3642    junksci\nName: type, dtype: object"
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "test.type.loc[test.type.index == 3642]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-7e4e95c284f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msim_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msim_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "#Find originalarticle types most similar to inferred vector by model1:\n",
    "sim_labels = np.empty(10,dtype='object')\n",
    "for i in range(len(sim_labels)):\n",
    "    sim_labels[i] = train.type.loc[train.type.index == model1.docvecs.most_similar([test_arrays[10]])[i][0]].values[0]\n",
    "\n",
    "np.unique(sim_labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-10 14:11:51,523 : INFO : precomputing L2-norms of doc weight vectors\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(10615, 0.5106222629547119),\n (35760, 0.500029444694519),\n (22072, 0.4754215180873871),\n (10282, 0.4753485321998596),\n (12222, 0.47503337264060974),\n (24140, 0.47311046719551086),\n (4678, 0.47090116143226624),\n (19009, 0.46703222393989563),\n (5028, 0.4660884439945221),\n (1725, 0.46571171283721924)]"
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "model2.docvecs.most_similar([test_arrays2[200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'satire'"
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "source": [
    "train.type[10004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(31820, 0.5368525981903076),\n (14004, 0.5289143323898315),\n (10004, 0.5062718391418457),\n (4258, 0.5002168416976929),\n (17200, 0.4998486638069153),\n (8084, 0.4979134798049927),\n (45557, 0.49462491273880005),\n (22523, 0.49202004075050354),\n (17669, 0.4897216856479645),\n (48858, 0.4863385558128357)]"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "model2.docvecs.most_similar([test_arrays2[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array(['clickbait', 'hate', 'political', 'reliable'], dtype=object),\n array([2, 2, 1, 5], dtype=int64))"
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "#Find originalarticle types most similar to inferred vector by model2:\n",
    "sim_labels = np.empty(10,dtype='object')\n",
    "for i in range(len(sim_labels)):\n",
    "    sim_labels[i] = train.type.loc[train.type.index == model2.docvecs.most_similar([test_arrays2[90]])[i][0]].values[0]\n",
    "\n",
    "np.unique(sim_labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Int64Index([   11,    17,    25,    29,    54,    63,    75,    88,    90,\n              124,\n            ...\n            12742, 12748, 12762, 12784, 12803, 12816, 12818, 12826, 12851,\n            12866],\n           dtype='int64', length=1045)"
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "#Get indices for testarticles of a certain type:\n",
    "test.type.loc[test.type == 'reliable'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.docvecs.most_similar([test_arrays[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.type.loc[train.type == 'bias'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_arrays2)\n",
    "X_train = scaler.transform(train_arrays2)\n",
    "X_test = scaler.transform(test_arrays2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2489"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "len(train.loc[train.type == 'hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.6070711128967456"
     },
     "metadata": {},
     "execution_count": 405
    }
   ],
   "source": [
    "4000/2489\n",
    "2489/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=0.01, multi_class='multinomial', solver='saga',\n                   warm_start=True)"
     },
     "metadata": {},
     "execution_count": 196
    }
   ],
   "source": [
    "#Now train a logistic classifier:\n",
    "#can try gridsearch for parameters:\n",
    "#Logistic = LogisticRegression()\n",
    "#grid = {\"C\" :np.logspace(0,4,10), \"penalty\": [\"l1\", \"l2\"]}\n",
    "#logClf = GridSearchCV(Logistic, grid, cv = 5)\n",
    "#best_model = logClf.fit(X_train, train_labels)\n",
    "clfLog = LogisticRegression(C = 0.01,multi_class = 'multinomial', solver = 'saga', penalty = 'l2', warm_start = True)\n",
    "clfLog.fit(X_train, train_labels2)\n",
    "\n",
    "#classifier.fit(X_train,train_labels)\n",
    "#classifier.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6555020205160087"
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "source": [
    "clfLog.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_labels\n",
    "#y_true\n",
    "y_pred = clfLog.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 674,   82,  147,   29,   87,   65,  193,   51,   54,   18],\n       [ 104,  705,   62,   12,   26,   57,  175,   94,  152,   13],\n       [ 133,   67,  810,   38,   51,   88,  101,   32,   68,   12],\n       [  16,   18,   45, 1224,    6,   38,   32,    5,   14,    2],\n       [  77,   48,   39,   18,  310,   19,   59,   25,   22,    6],\n       [  27,   38,  107,   19,    4, 1107,   28,   24,   45,    1],\n       [ 140,  159,  117,   20,   66,   36,  647,   93,  105,   17],\n       [  70,   70,   21,   13,   17,   29,   56,  719,   43,    7],\n       [  77,  108,   73,   16,   19,   53,   74,   23,  949,    8],\n       [  28,   57,   26,    4,    9,   13,   26,   14,   29, 1194]],\n      dtype=int64)"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6406526031383144"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GaussianNB()"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "#Try GaussianNB:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clfNB = GaussianNB()\n",
    "clfNB.fit(X_train, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5508237488343177"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "clfNB.score(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n          verbose=0)"
     },
     "metadata": {},
     "execution_count": 252
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "LinSvm = LinearSVC()\n",
    "LinSvm.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'LinSvm' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-bc7c7d703180>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLinSvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'LinSvm' is not defined"
     ]
    }
   ],
   "source": [
    "LinSvm.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC()"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "#rbf kernel = 0 .72, poly kernel = 0.7\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#parameters = {'C':[1,10], 'kernel': ('linear','rbf')}\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train,train_labels2)\n",
    "#clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7239664283493938"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "clf.score(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "X.shape[1] = 300 should be equal to 150, the number of features at training time",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-ba22c0ef8c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    484\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0;32m    485\u001b[0m                              \u001b[1;34m\"the number of features at training time\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m                              (X.shape[1], self.shape_fit_[1]))\n\u001b[0m\u001b[0;32m    487\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X.shape[1] = 300 should be equal to 150, the number of features at training time"
     ]
    }
   ],
   "source": [
    "y_pred_svm = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[603,  51,  73,  23,  50,  27, 106,  33,  34,   0],\n       [101, 614,  37,  15,  13,  24,  97,  38,  61,   0],\n       [117,  50, 578,  40,  34,  42,  98,   5,  34,   2],\n       [ 17,  10,  25, 890,   5,  22,  19,   5,   7,   0],\n       [ 81,  24,  31,  15, 382,   8,  54,  16,  11,   1],\n       [ 31,  34,  45,  17,   5, 824,  12,   7,  24,   1],\n       [173,  92,  72,  28,  27,  18, 501,  31,  57,   1],\n       [ 42,  37,  13,   4,   8,   7,  33, 829,  27,   0],\n       [ 58,  46,  25,  25,  11,  24,  41,  12, 755,   3],\n       [ 21,  47,   8,   3,   6,   8,  21,   7,   9, 870]])"
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "confusion_matrix(y_true,y_pred_svm)\n",
    "#np.unique(train_labels2 == typeLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')"
     },
     "metadata": {},
     "execution_count": 534
    }
   ],
   "source": [
    "#Decicion tree\n",
    "from sklearn import tree\n",
    "\n",
    "treeClf = tree.DecisionTreeClassifier()\n",
    "treeClf.fit(X_train, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.2661641280696301"
     },
     "metadata": {},
     "execution_count": 543
    }
   ],
   "source": [
    "treeClf.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Number of labels=38489 does not match number of samples=51466",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-566-a8d6210465ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mforestClf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mforestClf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 250\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels=38489 does not match number of samples=51466"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forestClf = RandomForestClassifier()\n",
    "forestClf.fit(X_train, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.35708734846129936"
     },
     "metadata": {},
     "execution_count": 542
    }
   ],
   "source": [
    "forestClf.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n                     weights='uniform')"
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN_clf.fit(X_train, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5564792684194119"
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "KNN_clf.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  9., 11.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 605
    }
   ],
   "source": [
    "np.unique(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[252, 131, 121,  46, 244,  63, 101,  28,  12,   2],\n       [ 27, 504,  69,  19,  99,  87, 103,  67,  23,   2],\n       [ 54, 102, 408,  54, 186,  81,  79,  15,  21,   0],\n       [ 30,  89, 150, 266, 181, 124,  92,  19,  44,   5],\n       [ 11,  50,  63,  13, 422,  21,  25,  13,   5,   0],\n       [  8,  55,  68,  20, 130, 691,  18,   8,   2,   0],\n       [ 40, 186,  94,  38, 182,  46, 363,  32,  18,   1],\n       [ 25,  79,  13,  10, 178,  54,  35, 591,  15,   0],\n       [ 16, 119,  50,  67,  83,  63,  78,  43, 474,   7],\n       [ 11,  46,  14,   9, 167,  15,  22,   6,   3, 707]])"
     },
     "metadata": {},
     "execution_count": 593
    }
   ],
   "source": [
    "confusion_matrix(y_true,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tf-idf svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(train[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scikit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "clf_NB = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('NB', MultinomialNB()),])\n",
    "                   \n",
    "# Support vector machine\n",
    "clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('svm', SGDClassifier()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm.fit(series_content,typeLst )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([ True]), array([9623]))"
     },
     "metadata": {},
     "execution_count": 631
    }
   ],
   "source": [
    "np.unique(test_labels == test_typeLst, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7225397485191728"
     },
     "metadata": {},
     "execution_count": 646
    }
   ],
   "source": [
    "clf_svm.score(test_content, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-645-bc1c4179375a>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-645-bc1c4179375a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    clf_svm.score(test_content, test_labels)clf_svm.score(test_content, test_labels)\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "clf_svm.score(test_content, test_labels)clf_svm.score(test_content, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51466"
     },
     "metadata": {},
     "execution_count": 585
    }
   ],
   "source": [
    "#X_train\n",
    "#len(train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51466"
     },
     "metadata": {},
     "execution_count": 589
    }
   ],
   "source": [
    "train_labels2 = np.array(train_labels2)\n",
    "len(train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train)\n",
    "#len(train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/15\n1609/1609 [==============================] - 1s 670us/step - loss: 1.3161 - accuracy: 0.5580\nEpoch 2/15\n1609/1609 [==============================] - 1s 661us/step - loss: 0.9816 - accuracy: 0.6781\nEpoch 3/15\n1609/1609 [==============================] - 1s 651us/step - loss: 0.9090 - accuracy: 0.7023\nEpoch 4/15\n1609/1609 [==============================] - 1s 660us/step - loss: 0.8664 - accuracy: 0.7163\nEpoch 5/15\n1609/1609 [==============================] - 1s 670us/step - loss: 0.8406 - accuracy: 0.7250\nEpoch 6/15\n1609/1609 [==============================] - 1s 663us/step - loss: 0.8210 - accuracy: 0.7312\nEpoch 7/15\n1609/1609 [==============================] - 1s 659us/step - loss: 0.8061 - accuracy: 0.7354\nEpoch 8/15\n1609/1609 [==============================] - 1s 660us/step - loss: 0.7921 - accuracy: 0.7381\nEpoch 9/15\n1609/1609 [==============================] - 1s 661us/step - loss: 0.7795 - accuracy: 0.7436\nEpoch 10/15\n1609/1609 [==============================] - 1s 667us/step - loss: 0.7703 - accuracy: 0.7458\nEpoch 11/15\n1609/1609 [==============================] - 1s 666us/step - loss: 0.7616 - accuracy: 0.7481\nEpoch 12/15\n1609/1609 [==============================] - 1s 686us/step - loss: 0.7522 - accuracy: 0.7513\nEpoch 13/15\n1609/1609 [==============================] - 1s 686us/step - loss: 0.7448 - accuracy: 0.7556\nEpoch 14/15\n1609/1609 [==============================] - 1s 688us/step - loss: 0.7399 - accuracy: 0.7542\nEpoch 15/15\n1609/1609 [==============================] - 1s 672us/step - loss: 0.7322 - accuracy: 0.7578\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1d7eff35e88>"
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "### Create Model size ###\n",
    "\n",
    "# laver modellen\n",
    "model = tf.keras.models.Sequential()\n",
    "# tilføjer et input på modellen\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# relu er default aktiverings funktion. Lav den om hvis resultatet ikke er godt nok\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "\n",
    "#antallet (10) er antal output. Det er 10 tal i datasettet derfor skal der være et 10 tal\n",
    "model.add(tf.keras.layers.Dense(20, activation=tf.nn.softmax))\n",
    "\n",
    "# Dette er den mest komplexe del. adam er goto. Hvis der kun er 2 løsninger så brug binary_categorical_crossentropy eller binary_crossentropy i stedet for sparse_categorical_crossentropy\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, train_labels2, epochs = 15)\n",
    "# model.fit(X_train, train_labels2, batch_size=64, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "403/403 [==============================] - 0s 484us/step - loss: 1.0131 - accuracy: 0.6864\n1.013122797012329 0.6863537430763245\n"
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(X_test,test_labels)\n",
    "print(val_loss, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/3\n51466/51466 [==============================] - 26s 496us/step - loss: 1.3845 - accuracy: 0.5289\nEpoch 2/3\n51466/51466 [==============================] - 25s 493us/step - loss: 1.2402 - accuracy: 0.5869\nEpoch 3/3\n51466/51466 [==============================] - 25s 495us/step - loss: 1.2241 - accuracy: 0.5912\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1d7d11bb8c8>"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "model.fit(X_train, train_labels2, batch_size=1, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-81379e84d0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmySentTok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmySentTok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "mySent = \"These are my sentences in the books\"\n",
    "mySentTok = word_tokenize(mySent)\n",
    "ps = PorterStemmer()\n",
    "for w in mySentTok:\n",
    "    mySentTok[w] = ps.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitnewcondae2b41800e1a24f049ef50ffcb90345e7",
   "display_name": "Python 3.7.7 64-bit ('new': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}