{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            id               domain  type  \\\n1496  338715.0  theinternetpost.net  fake   \n8192   30948.0    beforeitsnews.com  fake   \n5154  819308.0    beforeitsnews.com  fake   \n2272  339605.0  theinternetpost.net  fake   \n2198   24651.0    beforeitsnews.com  fake   \n\n                                                    url  \\\n1496        https://theinternetpost.net/tag/global-war/   \n8192  http://beforeitsnews.com/spirit/2013/06/more-t...   \n5154  http://beforeitsnews.com/international/2013/01...   \n2272   https://theinternetpost.net/tag/free-trade-area/   \n2198  http://beforeitsnews.com/business/2015/11/ente...   \n\n                                                content  \\\n1496  we are witnessing a great turning inward in th...   \n8192  more than a parade and my new manifesto of rea...   \n5154  dallas gun buyback program countered with succ...   \n2272  by paul j balles many industries that started ...   \n2198  enterprise vsat market global research and ana...   \n\n                      scraped_at                 inserted_at  \\\n1496  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n8192  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n5154  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n2272  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n2198  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n\n                      updated_at  \\\n1496  2018-02-02 01:19:41.756664   \n8192  2018-02-02 01:19:41.756664   \n5154  2018-02-02 01:19:41.756664   \n2272  2018-02-02 01:19:41.756664   \n2198  2018-02-02 01:19:41.756664   \n\n                                                  title           authors  \\\n1496                                  THE INTERNET POST               NaN   \n8192           More than a parade, and my new manifesto  Lisa Beck Living   \n5154  Dallas Gun Buyback Program Countered With Succ...               NaN   \n2272                                  THE INTERNET POST               NaN   \n2198  Enterprise VSAT Market Global Research and Ana...               NaN   \n\n      keywords meta_keywords  \\\n1496       NaN           NaN   \n8192       NaN           NaN   \n5154       NaN           NaN   \n2272       NaN           NaN   \n2198       NaN           NaN   \n\n                                       meta_description  \\\n1496  Posts about Global War written by ajfloyd and ...   \n8192                                                NaN   \n5154                                                NaN   \n2272     Posts about Free trade area written by ajfloyd   \n2198                                                NaN   \n\n                                                   tags  summary  source  \\\n1496  global conflict, war on terror, United States,...      NaN     NaN   \n8192                                                NaN      NaN     NaN   \n5154                                                NaN      NaN     NaN   \n2272  employment, Free trade agreement, United State...      NaN     NaN   \n2198                                                NaN      NaN     NaN   \n\n      type_id                                   content_tokenize  \n1496        3  ['we', 'are', 'witnessing', 'a', 'great', 'tur...  \n8192        3  ['more', 'than', 'a', 'parade', 'and', 'my', '...  \n5154        3  ['dallas', 'gun', 'buyback', 'program', 'count...  \n2272        3  ['by', 'paul', 'j', 'balles', 'many', 'industr...  \n2198        3  ['enterprise', 'vsat', 'market', 'global', 're...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n      <th>content_tokenize</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1496</th>\n      <td>338715.0</td>\n      <td>theinternetpost.net</td>\n      <td>fake</td>\n      <td>https://theinternetpost.net/tag/global-war/</td>\n      <td>we are witnessing a great turning inward in th...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>THE INTERNET POST</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Posts about Global War written by ajfloyd and ...</td>\n      <td>global conflict, war on terror, United States,...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['we', 'are', 'witnessing', 'a', 'great', 'tur...</td>\n    </tr>\n    <tr>\n      <th>8192</th>\n      <td>30948.0</td>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>http://beforeitsnews.com/spirit/2013/06/more-t...</td>\n      <td>more than a parade and my new manifesto of rea...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>More than a parade, and my new manifesto</td>\n      <td>Lisa Beck Living</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['more', 'than', 'a', 'parade', 'and', 'my', '...</td>\n    </tr>\n    <tr>\n      <th>5154</th>\n      <td>819308.0</td>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>http://beforeitsnews.com/international/2013/01...</td>\n      <td>dallas gun buyback program countered with succ...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Dallas Gun Buyback Program Countered With Succ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['dallas', 'gun', 'buyback', 'program', 'count...</td>\n    </tr>\n    <tr>\n      <th>2272</th>\n      <td>339605.0</td>\n      <td>theinternetpost.net</td>\n      <td>fake</td>\n      <td>https://theinternetpost.net/tag/free-trade-area/</td>\n      <td>by paul j balles many industries that started ...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>THE INTERNET POST</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Posts about Free trade area written by ajfloyd</td>\n      <td>employment, Free trade agreement, United State...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['by', 'paul', 'j', 'balles', 'many', 'industr...</td>\n    </tr>\n    <tr>\n      <th>2198</th>\n      <td>24651.0</td>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>http://beforeitsnews.com/business/2015/11/ente...</td>\n      <td>enterprise vsat market global research and ana...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Enterprise VSAT Market Global Research and Ana...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>['enterprise', 'vsat', 'market', 'global', 're...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "# importing and creating df (has to have type_id)\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random)\n",
    "#filepath = '../Data_sample/FakeNewsCorpus_250.csv' # 250 rows of FakeNewsCorpus\n",
    "filepath = '../Data_git_ignore/1mio-raw.csv'      # 1 mil rows raw\n",
    "filepath = '../Data_git_ignore/clean_corpus_25k.csv'\n",
    "#s = 1000000                                      # desired sample size\n",
    "#seed = 1                                         # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_csv(filepath, index_col = [0])\n",
    "df[\"content\"] = df[\"content\"].astype(str)\n",
    "# create type_id\n",
    "df['type_id'] = df.groupby(['type']).ngroup()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9924, 9925, 9926, 9927, 9928, 9929, 9930, 9931, 9932, 9933],\n      dtype='object', length=1000000)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df['id'] = pd.to_numeric(df['id'], errors = 'coerce', downcast = 'integer')\n",
    "df.dropna(subset=['id'], inplace = True)\n",
    "#df.drop_duplicates(subset = 'content', inplace = True)\n",
    "df.drop_duplicates(subset = 'id', inplace = True)\n",
    "df.id = df.id.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [id, domain, type, url, content, scraped_at, inserted_at, updated_at, title, authors, keywords, meta_keywords, meta_description, tags, summary, source, type_id]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.loc[df['id'] == np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [id, domain, type, url, content, scraped_at, inserted_at, updated_at, title, authors, keywords, meta_keywords, meta_description, tags, summary, source, type_id]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.loc[df['id'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2047      1\n983221    1\n995507    1\n997554    1\n991409    1\n         ..\n717812    1\n707571    1\n705522    1\n711665    1\n0         1\nLength: 999934, dtype: int64"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.reset_index(inplace = True, drop = True)\n",
    "df.index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()\n",
    "#df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupidx_lst = df.loc[df['content'].duplicated()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Uniq = df.loc[~df.index.isin(dupidx_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             id              domain        type  \\\n0             2       express.co.uk       rumor   \n1             6  barenakedislam.com        hate   \n2             7  barenakedislam.com        hate   \n3             8  barenakedislam.com        hate   \n4             9  barenakedislam.com        hate   \n...         ...                 ...         ...   \n999895  1170082       wikileaks.org  unreliable   \n999906  1170093       wikileaks.org  unreliable   \n999913  1170100       wikileaks.org  unreliable   \n999923  1170110       wikileaks.org  unreliable   \n999927  1170114       express.co.uk       rumor   \n\n                                                      url  \\\n0       https://www.express.co.uk/news/science/738402/...   \n1       http://barenakedislam.com/category/donald-trum...   \n2       http://barenakedislam.com/category/donald-trum...   \n3       http://barenakedislam.com/2017/12/24/more-winn...   \n4       http://barenakedislam.com/2017/12/25/oh-trump-...   \n...                                                   ...   \n999895  https://www.wikileaks.org/plusd/cables/1974ATO...   \n999906  https://www.wikileaks.org/plusd/cables/1976ABU...   \n999913  https://www.wikileaks.org/plusd/cables/1976ANK...   \n999923  https://www.wikileaks.org/plusd/cables/1976ROM...   \n999927  https://www.express.co.uk/celebrity-news/56649...   \n\n                                                  content  \\\n0       Life is an illusion, at least on a quantum lev...   \n1       Unfortunately, he hasn’t yet attacked her for ...   \n2       The Los Angeles Police Department has been den...   \n3       The White House has decided to quietly withdra...   \n4       “The time has come to cut off the tongues of t...   \n...                                                   ...   \n999895  Raw content\\n\\nPAGE 01 NATO 05116 01 OF 02 201...   \n999906  Raw content\\n\\nCONFIDENTIAL PAGE 01 ABU DH 000...   \n999913  Raw content\\n\\nCONFIDENTIAL PAGE 01 ANKARA 019...   \n999923  Raw content\\n\\nLIMITED OFFICIAL USE PAGE 01 RO...   \n999927  FLYNET Eddie looked unrecognisable as Einar We...   \n\n                        scraped_at                 inserted_at  \\\n0       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n1       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n2       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n3       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n4       2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n...                            ...                         ...   \n999895  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n999906  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n999913  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n999923  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n999927  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n\n                        updated_at  \\\n0       2018-02-02 01:19:41.756664   \n1       2018-02-02 01:19:41.756664   \n2       2018-02-02 01:19:41.756664   \n3       2018-02-02 01:19:41.756664   \n4       2018-02-02 01:19:41.756664   \n...                            ...   \n999895  2018-02-02 01:19:41.756664   \n999906  2018-02-02 01:19:41.756664   \n999913  2018-02-02 01:19:41.756664   \n999923  2018-02-02 01:19:41.756664   \n999927  2018-02-02 01:19:41.756664   \n\n                                                    title  \\\n0       Is life an ILLUSION? Researchers prove 'realit...   \n1                                            Donald Trump   \n2                                            Donald Trump   \n3       MORE WINNING! Israeli intelligence source, DEB...   \n4       “Oh, Trump, you coward, you just wait, we will...   \n...                                                   ...   \n999895                                Cable: 1974ATO05116   \n999906                              Cable: 1976ABUDH00021   \n999913                             Cable: 1976ANKARA01989   \n999923                               Cable: 1976ROME16372   \n999927  Eddie Redmayne looks ultra feminine as transge...   \n\n                                                  authors  keywords  \\\n0                                             Sean Martin       NaN   \n1       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n2       Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN   \n3       Cleavis Nowell, Cleavisnowell, Clarence J. Fei...       NaN   \n4       F.N. Lehner, Don Spilman, Clarence J. Feinour,...       NaN   \n...                                                   ...       ...   \n999895                                                NaN       NaN   \n999906                                                NaN       NaN   \n999913                                                NaN       NaN   \n999923                                                NaN       NaN   \n999927                                        Annie Price       NaN   \n\n       meta_keywords                                   meta_description  \\\n0               ['']  THE UNIVERSE ceases to exist when we are not l...   \n1               ['']                                                NaN   \n2               ['']                                                NaN   \n3               ['']                                                NaN   \n4               ['']                                                NaN   \n...              ...                                                ...   \n999895          ['']                                                NaN   \n999906          ['']                                                NaN   \n999913          ['']                                                NaN   \n999923          ['']                                                NaN   \n999927          ['']  FOR his last role Eddie Redmayne transformed h...   \n\n             tags  summary  source  type_id  \n0             NaN      NaN     NaN        8  \n1             NaN      NaN     NaN        4  \n2             NaN      NaN     NaN        4  \n3             NaN      NaN     NaN        4  \n4             NaN      NaN     NaN        4  \n...           ...      ...     ...      ...  \n999895  View Tags      NaN     NaN       11  \n999906  View Tags      NaN     NaN       11  \n999913  View Tags      NaN     NaN       11  \n999923  View Tags      NaN     NaN       11  \n999927        NaN      NaN     NaN        8  \n\n[660914 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>express.co.uk</td>\n      <td>rumor</td>\n      <td>https://www.express.co.uk/news/science/738402/...</td>\n      <td>Life is an illusion, at least on a quantum lev...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Is life an ILLUSION? Researchers prove 'realit...</td>\n      <td>Sean Martin</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>THE UNIVERSE ceases to exist when we are not l...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>barenakedislam.com</td>\n      <td>hate</td>\n      <td>http://barenakedislam.com/category/donald-trum...</td>\n      <td>Unfortunately, he hasn’t yet attacked her for ...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Donald Trump</td>\n      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>barenakedislam.com</td>\n      <td>hate</td>\n      <td>http://barenakedislam.com/category/donald-trum...</td>\n      <td>The Los Angeles Police Department has been den...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Donald Trump</td>\n      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>barenakedislam.com</td>\n      <td>hate</td>\n      <td>http://barenakedislam.com/2017/12/24/more-winn...</td>\n      <td>The White House has decided to quietly withdra...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>MORE WINNING! Israeli intelligence source, DEB...</td>\n      <td>Cleavis Nowell, Cleavisnowell, Clarence J. Fei...</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>barenakedislam.com</td>\n      <td>hate</td>\n      <td>http://barenakedislam.com/2017/12/25/oh-trump-...</td>\n      <td>“The time has come to cut off the tongues of t...</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>“Oh, Trump, you coward, you just wait, we will...</td>\n      <td>F.N. Lehner, Don Spilman, Clarence J. Feinour,...</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>999895</th>\n      <td>1170082</td>\n      <td>wikileaks.org</td>\n      <td>unreliable</td>\n      <td>https://www.wikileaks.org/plusd/cables/1974ATO...</td>\n      <td>Raw content\\n\\nPAGE 01 NATO 05116 01 OF 02 201...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Cable: 1974ATO05116</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>View Tags</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>999906</th>\n      <td>1170093</td>\n      <td>wikileaks.org</td>\n      <td>unreliable</td>\n      <td>https://www.wikileaks.org/plusd/cables/1976ABU...</td>\n      <td>Raw content\\n\\nCONFIDENTIAL PAGE 01 ABU DH 000...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Cable: 1976ABUDH00021</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>View Tags</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>999913</th>\n      <td>1170100</td>\n      <td>wikileaks.org</td>\n      <td>unreliable</td>\n      <td>https://www.wikileaks.org/plusd/cables/1976ANK...</td>\n      <td>Raw content\\n\\nCONFIDENTIAL PAGE 01 ANKARA 019...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Cable: 1976ANKARA01989</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>View Tags</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>999923</th>\n      <td>1170110</td>\n      <td>wikileaks.org</td>\n      <td>unreliable</td>\n      <td>https://www.wikileaks.org/plusd/cables/1976ROM...</td>\n      <td>Raw content\\n\\nLIMITED OFFICIAL USE PAGE 01 RO...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Cable: 1976ROME16372</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>View Tags</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>999927</th>\n      <td>1170114</td>\n      <td>express.co.uk</td>\n      <td>rumor</td>\n      <td>https://www.express.co.uk/celebrity-news/56649...</td>\n      <td>FLYNET Eddie looked unrecognisable as Einar We...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Eddie Redmayne looks ultra feminine as transge...</td>\n      <td>Annie Price</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>FOR his last role Eddie Redmayne transformed h...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>660914 rows × 17 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_Uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5222"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df_Uniq['type'].loc[df_Uniq['type'] == 'reliable'].value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "660914"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_Uniq.content.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> fake (7000, 17) (5600, 17) (1400, 17)\n=> satire (7000, 17) (5600, 17) (1400, 17)\n=> bias (7000, 17) (5600, 17) (1400, 17)\n=> conspiracy (7000, 17) (5600, 17) (1400, 17)\n=> junksci (7000, 17) (5600, 17) (1400, 17)\n=> hate (3112, 17) (2489, 17) (623, 17)\n=> clickbait (7000, 17) (5600, 17) (1400, 17)\n=> unreliable (7000, 17) (5600, 17) (1400, 17)\n=> political (7000, 17) (5600, 17) (1400, 17)\n=> reliable (5222, 17) (4177, 17) (1045, 17)\n\n[Final split]\ntrain, test==> (51466, 17) (12868, 17)\n"
    }
   ],
   "source": [
    "#USE THIS\n",
    "# This can generate a dataset with random purmutation and a max size for each type(can be smaller if desired max is not possible)\n",
    "\n",
    "# max size for type\n",
    "max_size = 7000\n",
    "# traning_set ratio - splits data into traning=ratio,  test and validate=(1-ratio)/2 ex. train=80%, test=10%, validate=10%\n",
    "ratio=0.8\n",
    "# Labels to include - ['fake', 'satire', 'bias', 'conspiracy', 'state', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable'] - all labels\n",
    "use_types = ['fake', 'satire', 'bias', 'conspiracy', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable']\n",
    "# Random seed\n",
    "rnd = 1\n",
    "\n",
    "# initialize dataframes\n",
    "train    = pd.DataFrame(columns = df_Uniq.columns)\n",
    "test     = pd.DataFrame(columns = df_Uniq.columns)\n",
    "\n",
    "# add type to test splits\n",
    "for t in use_types:\n",
    "\n",
    "    # type size\n",
    "    type_size = df_Uniq['type'].loc[df_Uniq['type'] == t].value_counts().min()\n",
    "\n",
    "    # set size of type slice\n",
    "    if type_size < max_size:\n",
    "        tmp = df_Uniq.loc[df_Uniq['type'] == t].sample(n = type_size, random_state=rnd)\n",
    "    else:\n",
    "        tmp = df_Uniq.loc[df_Uniq['type'] == t].sample(n = max_size, random_state=rnd)\n",
    "\n",
    "    # split current type\n",
    "    train_tmp, test_tmp = np.split(tmp, [int(ratio * len(tmp))])\n",
    "\n",
    "    # add tmp to dataframes\n",
    "    train    = pd.concat([train, train_tmp])\n",
    "    test     = pd.concat([test, test_tmp])\n",
    "    \n",
    "    # print split shape\n",
    "    print(\"=>\", t, tmp.shape, train_tmp.shape, test_tmp.shape)\n",
    "\n",
    "print(\"\\n[Final split]\\ntrain, test==>\", train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [id_x, domain_x, type_x, url_x, content, scraped_at_x, inserted_at_x, updated_at_x, title_x, authors_x, keywords_x, meta_keywords_x, meta_description_x, tags_x, summary_x, source_x, type_id_x, id_y, domain_y, type_y, url_y, scraped_at_y, inserted_at_y, updated_at_y, title_y, authors_y, keywords_y, meta_keywords_y, meta_description_y, tags_y, summary_y, source_y, type_id_y]\nIndex: []\n\n[0 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_x</th>\n      <th>domain_x</th>\n      <th>type_x</th>\n      <th>url_x</th>\n      <th>content</th>\n      <th>scraped_at_x</th>\n      <th>inserted_at_x</th>\n      <th>updated_at_x</th>\n      <th>title_x</th>\n      <th>authors_x</th>\n      <th>...</th>\n      <th>updated_at_y</th>\n      <th>title_y</th>\n      <th>authors_y</th>\n      <th>keywords_y</th>\n      <th>meta_keywords_y</th>\n      <th>meta_description_y</th>\n      <th>tags_y</th>\n      <th>summary_y</th>\n      <th>source_y</th>\n      <th>type_id_y</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 33 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "pd.merge(train,test, on =['content'], how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for faster calculation for details, dont run normally\n",
    "# importing and creating df (has to have type_id)\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "#filepath = '/Users/Master/Documents/KU/2.Semester/Datascience/news_sample.csv' #\n",
    "#s = 250                                            # desired sample size\n",
    "#seed = 1                                           # seed used by Pseudorandom number generator\n",
    "\n",
    "#df = pd.read_csv(filepath, index_col = [0])\n",
    "#content = df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec \n",
    "from gensim.utils import tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.set_index('id', inplace = True)\n",
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train, random_state = 1)\n",
    "test = shuffle(test, random_state = 1)\n",
    "train.reset_index(inplace = True, drop = True)\n",
    "test.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['this', 'is', 'to', 'token']"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "#this is how the tokenizer works\n",
    "list(tokenize(\"This , is 5to TOKEN\", lowercase = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 0 ns\n"
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "series_content = train.content\n",
    "#contTok = series_content.apply(nltk.word_tokenize)\n",
    "typeLst = train.type_id.tolist()\n",
    "#np.unique(typeLst, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = test.content\n",
    "test_typeLst = test.type_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_content\n",
    "#test_typeLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try gensim tokenizer\n",
    "testTok = test_content.apply(tokenize,lowercase =True)\n",
    "for i in testTok.index:\n",
    "    testTok[i] = list(testTok[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try gensim tokenizer\n",
    "contTok = series_content.apply(tokenize,lowercase =True)\n",
    "for i in contTok.index:\n",
    "    contTok[i] = list(contTok[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe fallback tokenizer\n",
    "#contTok1 = series_content.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'unreliable'"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "train.type[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(contTok, [i]) for i, contTok in enumerate(contTok)]\n",
    "#contTok[291496]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(enumerate(contTok))[0]\n",
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "020-06-10 14:06:06,299 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:06:06,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:06:06,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:06:06,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:06:06,301 : INFO : EPOCH - 9 : training on 33824106 raw words (26563430 effective words) took 14.6s, 1822045 effective words/s\n2020-06-10 14:06:07,316 : INFO : EPOCH 10 - PROGRESS: at 6.65% examples, 1776201 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:08,318 : INFO : EPOCH 10 - PROGRESS: at 13.51% examples, 1814294 words/s, in_qsize 29, out_qsize 1\n2020-06-10 14:06:09,324 : INFO : EPOCH 10 - PROGRESS: at 20.55% examples, 1807446 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:10,331 : INFO : EPOCH 10 - PROGRESS: at 27.66% examples, 1804961 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:06:11,338 : INFO : EPOCH 10 - PROGRESS: at 34.36% examples, 1812101 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:06:12,348 : INFO : EPOCH 10 - PROGRESS: at 41.11% examples, 1811573 words/s, in_qsize 30, out_qsize 2\n2020-06-10 14:06:13,349 : INFO : EPOCH 10 - PROGRESS: at 47.89% examples, 1816474 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:14,352 : INFO : EPOCH 10 - PROGRESS: at 54.62% examples, 1819276 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:15,355 : INFO : EPOCH 10 - PROGRESS: at 61.43% examples, 1814652 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:06:16,359 : INFO : EPOCH 10 - PROGRESS: at 68.38% examples, 1813640 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:17,358 : INFO : EPOCH 10 - PROGRESS: at 75.24% examples, 1814529 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:18,369 : INFO : EPOCH 10 - PROGRESS: at 82.22% examples, 1813128 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:19,376 : INFO : EPOCH 10 - PROGRESS: at 89.04% examples, 1810215 words/s, in_qsize 32, out_qsize 5\n2020-06-10 14:06:20,380 : INFO : EPOCH 10 - PROGRESS: at 96.05% examples, 1812245 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:20,899 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:06:20,910 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:06:20,918 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:06:20,926 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:06:20,929 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:06:20,929 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:06:20,930 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:06:20,931 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:06:20,931 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:06:20,932 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:06:20,933 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:06:20,933 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:06:20,933 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:06:20,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:06:20,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:06:20,934 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:06:20,934 : INFO : EPOCH - 10 : training on 33824106 raw words (26564647 effective words) took 14.6s, 1816193 effective words/s\n2020-06-10 14:06:21,949 : INFO : EPOCH 11 - PROGRESS: at 6.79% examples, 1807799 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:22,954 : INFO : EPOCH 11 - PROGRESS: at 13.59% examples, 1825088 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:23,956 : INFO : EPOCH 11 - PROGRESS: at 20.64% examples, 1816008 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:24,957 : INFO : EPOCH 11 - PROGRESS: at 27.89% examples, 1825355 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:25,977 : INFO : EPOCH 11 - PROGRESS: at 34.52% examples, 1819819 words/s, in_qsize 28, out_qsize 4\n2020-06-10 14:06:26,985 : INFO : EPOCH 11 - PROGRESS: at 41.25% examples, 1819372 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:06:27,987 : INFO : EPOCH 11 - PROGRESS: at 48.09% examples, 1822891 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:28,994 : INFO : EPOCH 11 - PROGRESS: at 54.79% examples, 1823096 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:06:30,003 : INFO : EPOCH 11 - PROGRESS: at 61.72% examples, 1821812 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:31,011 : INFO : EPOCH 11 - PROGRESS: at 68.75% examples, 1819950 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:32,012 : INFO : EPOCH 11 - PROGRESS: at 75.65% examples, 1820988 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:33,019 : INFO : EPOCH 11 - PROGRESS: at 82.60% examples, 1819202 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:06:34,024 : INFO : EPOCH 11 - PROGRESS: at 89.51% examples, 1818333 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:35,026 : INFO : EPOCH 11 - PROGRESS: at 96.37% examples, 1817331 words/s, in_qsize 32, out_qsize 4\n2020-06-10 14:06:35,486 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:06:35,506 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:06:35,511 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:06:35,515 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:06:35,516 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:06:35,518 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:06:35,518 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:06:35,519 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:06:35,519 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:06:35,520 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:06:35,520 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:06:35,521 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:06:35,521 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:06:35,521 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:06:35,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:06:35,522 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:06:35,522 : INFO : EPOCH - 11 : training on 33824106 raw words (26565615 effective words) took 14.6s, 1822027 effective words/s\n2020-06-10 14:06:36,538 : INFO : EPOCH 12 - PROGRESS: at 6.64% examples, 1766219 words/s, in_qsize 29, out_qsize 3\n2020-06-10 14:06:37,543 : INFO : EPOCH 12 - PROGRESS: at 13.60% examples, 1821287 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:38,547 : INFO : EPOCH 12 - PROGRESS: at 20.63% examples, 1811731 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:39,548 : INFO : EPOCH 12 - PROGRESS: at 27.80% examples, 1817948 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:40,552 : INFO : EPOCH 12 - PROGRESS: at 34.41% examples, 1819809 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:41,553 : INFO : EPOCH 12 - PROGRESS: at 41.12% examples, 1817799 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:42,557 : INFO : EPOCH 12 - PROGRESS: at 47.91% examples, 1820600 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:06:43,572 : INFO : EPOCH 12 - PROGRESS: at 54.58% examples, 1818510 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:44,574 : INFO : EPOCH 12 - PROGRESS: at 61.62% examples, 1821367 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:45,583 : INFO : EPOCH 12 - PROGRESS: at 68.46% examples, 1815085 words/s, in_qsize 32, out_qsize 3\n2020-06-10 14:06:46,589 : INFO : EPOCH 12 - PROGRESS: at 75.26% examples, 1813775 words/s, in_qsize 29, out_qsize 2\n2020-06-10 14:06:47,595 : INFO : EPOCH 12 - PROGRESS: at 82.26% examples, 1813137 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:48,596 : INFO : EPOCH 12 - PROGRESS: at 89.21% examples, 1814273 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:49,596 : INFO : EPOCH 12 - PROGRESS: at 96.14% examples, 1814548 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:06:50,103 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:06:50,114 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:06:50,125 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:06:50,131 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:06:50,132 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:06:50,133 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:06:50,134 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:06:50,136 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:06:50,136 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:06:50,136 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:06:50,137 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:06:50,137 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:06:50,138 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:06:50,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:06:50,139 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:06:50,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:06:50,140 : INFO : EPOCH - 12 : training on 33824106 raw words (26564811 effective words) took 14.6s, 1818102 effective words/s\n2020-06-10 14:06:51,153 : INFO : EPOCH 13 - PROGRESS: at 6.63% examples, 1769876 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:06:52,156 : INFO : EPOCH 13 - PROGRESS: at 13.51% examples, 1815241 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:53,166 : INFO : EPOCH 13 - PROGRESS: at 20.59% examples, 1808083 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:06:54,167 : INFO : EPOCH 13 - PROGRESS: at 27.71% examples, 1810139 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:06:55,178 : INFO : EPOCH 13 - PROGRESS: at 34.35% examples, 1811271 words/s, in_qsize 30, out_qsize 3\n2020-06-10 14:06:56,181 : INFO : EPOCH 13 - PROGRESS: at 41.18% examples, 1817031 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:06:57,188 : INFO : EPOCH 13 - PROGRESS: at 48.07% examples, 1822438 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:06:58,203 : INFO : EPOCH 13 - PROGRESS: at 54.74% examples, 1819923 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:06:59,217 : INFO : EPOCH 13 - PROGRESS: at 61.70% examples, 1819912 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:07:00,218 : INFO : EPOCH 13 - PROGRESS: at 68.77% examples, 1820019 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:01,218 : INFO : EPOCH 13 - PROGRESS: at 75.64% examples, 1820494 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:02,221 : INFO : EPOCH 13 - PROGRESS: at 82.61% examples, 1819818 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:03,235 : INFO : EPOCH 13 - PROGRESS: at 89.57% examples, 1818845 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:07:04,248 : INFO : EPOCH 13 - PROGRESS: at 96.57% examples, 1818951 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:07:04,680 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:07:04,697 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:07:04,703 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:07:04,705 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:07:04,708 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:07:04,711 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:07:04,712 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:07:04,713 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:07:04,713 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:07:04,714 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:07:04,714 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:07:04,715 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:07:04,715 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:07:04,715 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:07:04,715 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:07:04,717 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:07:04,717 : INFO : EPOCH - 13 : training on 33824106 raw words (26563231 effective words) took 14.6s, 1823117 effective words/s\n2020-06-10 14:07:05,725 : INFO : EPOCH 14 - PROGRESS: at 6.65% examples, 1778712 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:06,731 : INFO : EPOCH 14 - PROGRESS: at 13.52% examples, 1815649 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:07,736 : INFO : EPOCH 14 - PROGRESS: at 20.63% examples, 1814342 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:08,738 : INFO : EPOCH 14 - PROGRESS: at 27.84% examples, 1821778 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:09,738 : INFO : EPOCH 14 - PROGRESS: at 34.42% examples, 1822225 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:10,739 : INFO : EPOCH 14 - PROGRESS: at 41.16% examples, 1821374 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:11,744 : INFO : EPOCH 14 - PROGRESS: at 47.96% examples, 1823757 words/s, in_qsize 30, out_qsize 4\n2020-06-10 14:07:12,744 : INFO : EPOCH 14 - PROGRESS: at 54.70% examples, 1826199 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:07:13,751 : INFO : EPOCH 14 - PROGRESS: at 61.63% examples, 1825638 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:14,758 : INFO : EPOCH 14 - PROGRESS: at 68.73% examples, 1825143 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:15,759 : INFO : EPOCH 14 - PROGRESS: at 75.59% examples, 1825144 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:16,762 : INFO : EPOCH 14 - PROGRESS: at 82.54% examples, 1823825 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:17,768 : INFO : EPOCH 14 - PROGRESS: at 89.47% examples, 1822963 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:18,769 : INFO : EPOCH 14 - PROGRESS: at 96.29% examples, 1821262 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:19,245 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:07:19,250 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:07:19,264 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:07:19,270 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:07:19,275 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:07:19,276 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:07:19,278 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:07:19,279 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:07:19,279 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:07:19,279 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:07:19,280 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:07:19,280 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:07:19,281 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:07:19,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:07:19,281 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:07:19,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:07:19,282 : INFO : EPOCH - 14 : training on 33824106 raw words (26566299 effective words) took 14.6s, 1824762 effective words/s\n2020-06-10 14:07:20,292 : INFO : EPOCH 15 - PROGRESS: at 6.78% examples, 1812009 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:21,294 : INFO : EPOCH 15 - PROGRESS: at 13.57% examples, 1825861 words/s, in_qsize 31, out_qsize 0\n2020-06-10 14:07:22,295 : INFO : EPOCH 15 - PROGRESS: at 20.64% examples, 1820486 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:07:23,296 : INFO : EPOCH 15 - PROGRESS: at 27.81% examples, 1823130 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:24,301 : INFO : EPOCH 15 - PROGRESS: at 34.44% examples, 1825186 words/s, in_qsize 31, out_qsize 1\n2020-06-10 14:07:25,303 : INFO : EPOCH 15 - PROGRESS: at 41.27% examples, 1826611 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:26,303 : INFO : EPOCH 15 - PROGRESS: at 48.07% examples, 1829340 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:27,315 : INFO : EPOCH 15 - PROGRESS: at 54.70% examples, 1825664 words/s, in_qsize 32, out_qsize 2\n2020-06-10 14:07:28,322 : INFO : EPOCH 15 - PROGRESS: at 61.64% examples, 1824467 words/s, in_qsize 30, out_qsize 1\n2020-06-10 14:07:29,341 : INFO : EPOCH 15 - PROGRESS: at 68.73% examples, 1821795 words/s, in_qsize 32, out_qsize 5\n2020-06-10 14:07:30,343 : INFO : EPOCH 15 - PROGRESS: at 75.60% examples, 1822532 words/s, in_qsize 32, out_qsize 1\n2020-06-10 14:07:31,349 : INFO : EPOCH 15 - PROGRESS: at 82.59% examples, 1821412 words/s, in_qsize 31, out_qsize 2\n2020-06-10 14:07:32,351 : INFO : EPOCH 15 - PROGRESS: at 89.59% examples, 1822280 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:33,353 : INFO : EPOCH 15 - PROGRESS: at 96.55% examples, 1823249 words/s, in_qsize 32, out_qsize 0\n2020-06-10 14:07:33,811 : INFO : worker thread finished; awaiting finish of 15 more threads\n2020-06-10 14:07:33,827 : INFO : worker thread finished; awaiting finish of 14 more threads\n2020-06-10 14:07:33,829 : INFO : worker thread finished; awaiting finish of 13 more threads\n2020-06-10 14:07:33,835 : INFO : worker thread finished; awaiting finish of 12 more threads\n2020-06-10 14:07:33,838 : INFO : worker thread finished; awaiting finish of 11 more threads\n2020-06-10 14:07:33,839 : INFO : worker thread finished; awaiting finish of 10 more threads\n2020-06-10 14:07:33,841 : INFO : worker thread finished; awaiting finish of 9 more threads\n2020-06-10 14:07:33,842 : INFO : worker thread finished; awaiting finish of 8 more threads\n2020-06-10 14:07:33,843 : INFO : worker thread finished; awaiting finish of 7 more threads\n2020-06-10 14:07:33,843 : INFO : worker thread finished; awaiting finish of 6 more threads\n2020-06-10 14:07:33,844 : INFO : worker thread finished; awaiting finish of 5 more threads\n2020-06-10 14:07:33,844 : INFO : worker thread finished; awaiting finish of 4 more threads\n2020-06-10 14:07:33,845 : INFO : worker thread finished; awaiting finish of 3 more threads\n2020-06-10 14:07:33,845 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-06-10 14:07:33,845 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-06-10 14:07:33,846 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-06-10 14:07:33,846 : INFO : EPOCH - 15 : training on 33824106 raw words (26562783 effective words) took 14.6s, 1824592 effective words/s\n2020-06-10 14:07:33,847 : INFO : training on a 507361590 raw words (398470512 effective words) took 221.0s, 1803221 effective words/s\n"
    }
   ],
   "source": [
    "model2 = Doc2Vec(documents, vector_size = 300, dm = 0, epochs=15, workers = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3331\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-181-11141c9578dd>\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', 'model1 = Doc2Vec(documents, vector_size = , min_count = 3, window = 10, epochs=10, workers = 4)\\n')\n",
      "  File \u001b[0;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2362\u001b[0m, in \u001b[0;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[0;32m\"<decorator-gen-62>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35mtime\u001b[0m\n",
      "  File \u001b[0;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\magic.py\"\u001b[0m, line \u001b[0;32m187\u001b[0m, in \u001b[0;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[0;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\magics\\execution.py\"\u001b[0m, line \u001b[0;32m1268\u001b[0m, in \u001b[0;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\danie\\anaconda3\\envs\\new\\lib\\site-packages\\IPython\\core\\compilerop.py\"\u001b[1;36m, line \u001b[1;32m101\u001b[1;36m, in \u001b[1;35mast_parse\u001b[1;36m\u001b[0m\n\u001b[1;33m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<unknown>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    model1 = Doc2Vec(documents, vector_size = , min_count = 3, window = 10, epochs=10, workers = 4)\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model1 = Doc2Vec(documents, vector_size = , min_count = 3, window = 10, epochs=10, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eventuel træn andre modeller herefter:\n",
    "#%%time\n",
    "#model2 = Doc2Vec(documents, vector_size = 200, dm = 1, min_count = 3, window = 10, epochs= 10, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-10 14:07:58,329 : INFO : precomputing L2-norms of word weight vectors\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('inhabits', 0.23516283929347992),\n ('trio', 0.23322972655296326),\n ('су', 0.23255932331085205),\n ('rem', 0.2267533838748932),\n ('финансирование', 0.22597454488277435),\n ('seis', 0.22557631134986877),\n ('parigi', 0.2229282408952713),\n ('ciavarella', 0.21694324910640717),\n ('nita', 0.21591304242610931),\n ('obeying', 0.2151620239019394)]"
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "model2.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51466"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "model2.docvecs.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_list = model2.docvecs.most_similar(200)[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-be2f5dc91119>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfirst_tuple_elements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma_tuple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuple_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfirst_tuple_elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-183-be2f5dc91119>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfirst_tuple_elements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma_tuple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuple_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfirst_tuple_elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "first_tuple_elements = [a_tuple[0] for a_tuple in tuple_list]\n",
    "first_tuple_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "39974    fake\nName: type, dtype: object"
     },
     "metadata": {},
     "execution_count": 457
    }
   ],
   "source": [
    "train.type.loc[train.type.index == 39974]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.type.loc[train.index == 38424]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array(['bias', 'conspiracy', 'fake'], dtype=object),\n array([8, 1, 1], dtype=int64))"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "#See types of most similar documents for model2\n",
    "#for idx in train.type.loc[train.type == 'fake'].index:\n",
    "sim_labels1 = np.empty(10,dtype='object')\n",
    "for i in range(len(sim_labels1)):\n",
    "    sim_labels1[i] = train.type.loc[train.type.index == model2.docvecs.most_similar(16)[i][0]].values[0]\n",
    "\n",
    "# amount of articles with given type equal to testing article\n",
    "np.unique(sim_labels1,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "16       bias\n21       bias\n23       bias\n34       bias\n43       bias\n         ... \n51398    bias\n51399    bias\n51400    bias\n51416    bias\n51464    bias\nName: type, Length: 5600, dtype: object"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "#Lookup indeices for articles with certain types\n",
    "train.type.loc[train.type == 'bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-02105199d898>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Model 1 traindata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "#Model 1 traindata\n",
    "train_arrays = np.zeros((model1.docvecs.count,150),dtype=\"float32\")\n",
    "train_labels = np.zeros(model1.docvecs.count,dtype=\"float32\")\n",
    "for i in range(model1.docvecs.count):\n",
    "    train_arrays[i] = model1.docvecs[i]\n",
    "    train_labels[i] = train.type_id[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51466"
     },
     "metadata": {},
     "execution_count": 477
    }
   ],
   "source": [
    "model2.docvecs.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 traindata:\n",
    "train_arrays2 = np.zeros((model2.docvecs.count,300),dtype=\"float32\")\n",
    "train_labels2 = np.zeros(model2.docvecs.count,dtype=\"float32\")\n",
    "for i in range(model2.docvecs.count):\n",
    "    train_arrays2[i] = model2.docvecs[i]\n",
    "    train_labels2[i] = train.type_id[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(train_labels, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_arrays[0]\n",
    "#train_labels[0]\n",
    "#model1.docvecs.most_similar([train_arrays[0]])\n",
    "#train.loc[train.index == 37400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-75a43f0ef60d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#How infer_vector works:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"this is new sentence\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnew_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_vector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#model2.docvecs.most_similar([new_vec])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "#How infer_vector works:\n",
    "token = \"this is new sentence\".split()\n",
    "new_vector = model1.infer_vector(token)\n",
    "sims = model1.docvecs.most_similar([new_vector])\n",
    "#model2.docvecs.most_similar([new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_vec = model2.infer_vector(contTok[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.docvecs.most_similar([new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-1640ee33e506>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtest_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestTok\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_typeLst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "#Inferring vectors for testdata with model1\n",
    "test_arrays = np.zeros((len(test),150),dtype = \"float32\")\n",
    "for i in range(len(test)):\n",
    "    test_arrays[i] = model1.infer_vector(testTok[i],steps = 15)   \n",
    "test_labels = np.array(test_typeLst,dtype=\"float32\")\n",
    "\n",
    "#test_arrays = np.zeros((model1.docvecs.count,150),dtype=\"float32\")\n",
    "#test_labels = np.zeros(model1.docvecs.count,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 3min 5s\n"
    }
   ],
   "source": [
    "%%time\n",
    "#Inferring vectors for testdata with model2\n",
    "test_arrays2 = np.zeros((len(test),300),dtype = \"float32\")\n",
    "for i in range(len(test)):\n",
    "    test_arrays2[i] = model2.infer_vector(testTok[i], epochs = 20, alpha = 0.025)   \n",
    "test_labels = np.array(test_typeLst,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testTok[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_labels[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(test_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-c0c2c1a6f2f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "model1.docvecs.most_similar([test_arrays[200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3642    junksci\nName: type, dtype: object"
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "test.type.loc[test.type.index == 3642]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-7e4e95c284f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msim_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msim_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "#Find originalarticle types most similar to inferred vector by model1:\n",
    "sim_labels = np.empty(10,dtype='object')\n",
    "for i in range(len(sim_labels)):\n",
    "    sim_labels[i] = train.type.loc[train.type.index == model1.docvecs.most_similar([test_arrays[10]])[i][0]].values[0]\n",
    "\n",
    "np.unique(sim_labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-10 14:11:51,523 : INFO : precomputing L2-norms of doc weight vectors\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(10615, 0.5106222629547119),\n (35760, 0.500029444694519),\n (22072, 0.4754215180873871),\n (10282, 0.4753485321998596),\n (12222, 0.47503337264060974),\n (24140, 0.47311046719551086),\n (4678, 0.47090116143226624),\n (19009, 0.46703222393989563),\n (5028, 0.4660884439945221),\n (1725, 0.46571171283721924)]"
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "model2.docvecs.most_similar([test_arrays2[200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'satire'"
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "source": [
    "train.type[10004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(31820, 0.5368525981903076),\n (14004, 0.5289143323898315),\n (10004, 0.5062718391418457),\n (4258, 0.5002168416976929),\n (17200, 0.4998486638069153),\n (8084, 0.4979134798049927),\n (45557, 0.49462491273880005),\n (22523, 0.49202004075050354),\n (17669, 0.4897216856479645),\n (48858, 0.4863385558128357)]"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "model2.docvecs.most_similar([test_arrays2[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array(['clickbait', 'hate', 'political', 'reliable'], dtype=object),\n array([2, 2, 1, 5], dtype=int64))"
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "#Find originalarticle types most similar to inferred vector by model2:\n",
    "sim_labels = np.empty(10,dtype='object')\n",
    "for i in range(len(sim_labels)):\n",
    "    sim_labels[i] = train.type.loc[train.type.index == model2.docvecs.most_similar([test_arrays2[90]])[i][0]].values[0]\n",
    "\n",
    "np.unique(sim_labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Int64Index([   11,    17,    25,    29,    54,    63,    75,    88,    90,\n              124,\n            ...\n            12742, 12748, 12762, 12784, 12803, 12816, 12818, 12826, 12851,\n            12866],\n           dtype='int64', length=1045)"
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "#Get indices for testarticles of a certain type:\n",
    "test.type.loc[test.type == 'reliable'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.docvecs.most_similar([test_arrays[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.type.loc[train.type == 'bias'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_arrays2)\n",
    "X_train = scaler.transform(train_arrays2)\n",
    "X_test = scaler.transform(test_arrays2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2489"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "len(train.loc[train.type == 'hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.6070711128967456"
     },
     "metadata": {},
     "execution_count": 405
    }
   ],
   "source": [
    "4000/2489\n",
    "2489/400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=0.01, multi_class='multinomial', solver='saga',\n                   warm_start=True)"
     },
     "metadata": {},
     "execution_count": 196
    }
   ],
   "source": [
    "#Now train a logistic classifier:\n",
    "#can try gridsearch for parameters:\n",
    "#Logistic = LogisticRegression()\n",
    "#grid = {\"C\" :np.logspace(0,4,10), \"penalty\": [\"l1\", \"l2\"]}\n",
    "#logClf = GridSearchCV(Logistic, grid, cv = 5)\n",
    "#best_model = logClf.fit(X_train, train_labels)\n",
    "clfLog = LogisticRegression(C = 0.01,multi_class = 'multinomial', solver = 'saga', penalty = 'l2', warm_start = True)\n",
    "clfLog.fit(X_train, train_labels2)\n",
    "\n",
    "#classifier.fit(X_train,train_labels)\n",
    "#classifier.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6555020205160087"
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "source": [
    "clfLog.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_labels\n",
    "#y_true\n",
    "y_pred = clfLog.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 674,   82,  147,   29,   87,   65,  193,   51,   54,   18],\n       [ 104,  705,   62,   12,   26,   57,  175,   94,  152,   13],\n       [ 133,   67,  810,   38,   51,   88,  101,   32,   68,   12],\n       [  16,   18,   45, 1224,    6,   38,   32,    5,   14,    2],\n       [  77,   48,   39,   18,  310,   19,   59,   25,   22,    6],\n       [  27,   38,  107,   19,    4, 1107,   28,   24,   45,    1],\n       [ 140,  159,  117,   20,   66,   36,  647,   93,  105,   17],\n       [  70,   70,   21,   13,   17,   29,   56,  719,   43,    7],\n       [  77,  108,   73,   16,   19,   53,   74,   23,  949,    8],\n       [  28,   57,   26,    4,    9,   13,   26,   14,   29, 1194]],\n      dtype=int64)"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6406526031383144"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GaussianNB()"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "#Try GaussianNB:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clfNB = GaussianNB()\n",
    "clfNB.fit(X_train, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5508237488343177"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "clfNB.score(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n          verbose=0)"
     },
     "metadata": {},
     "execution_count": 252
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "LinSvm = LinearSVC()\n",
    "LinSvm.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'LinSvm' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-bc7c7d703180>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLinSvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'LinSvm' is not defined"
     ]
    }
   ],
   "source": [
    "LinSvm.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC()"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "#rbf kernel = 0 .72, poly kernel = 0.7\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#parameters = {'C':[1,10], 'kernel': ('linear','rbf')}\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train,train_labels2)\n",
    "#clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7239664283493938"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "clf.score(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "X.shape[1] = 300 should be equal to 150, the number of features at training time",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-ba22c0ef8c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    484\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0;32m    485\u001b[0m                              \u001b[1;34m\"the number of features at training time\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m                              (X.shape[1], self.shape_fit_[1]))\n\u001b[0m\u001b[0;32m    487\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X.shape[1] = 300 should be equal to 150, the number of features at training time"
     ]
    }
   ],
   "source": [
    "y_pred_svm = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[603,  51,  73,  23,  50,  27, 106,  33,  34,   0],\n       [101, 614,  37,  15,  13,  24,  97,  38,  61,   0],\n       [117,  50, 578,  40,  34,  42,  98,   5,  34,   2],\n       [ 17,  10,  25, 890,   5,  22,  19,   5,   7,   0],\n       [ 81,  24,  31,  15, 382,   8,  54,  16,  11,   1],\n       [ 31,  34,  45,  17,   5, 824,  12,   7,  24,   1],\n       [173,  92,  72,  28,  27,  18, 501,  31,  57,   1],\n       [ 42,  37,  13,   4,   8,   7,  33, 829,  27,   0],\n       [ 58,  46,  25,  25,  11,  24,  41,  12, 755,   3],\n       [ 21,  47,   8,   3,   6,   8,  21,   7,   9, 870]])"
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "confusion_matrix(y_true,y_pred_svm)\n",
    "#np.unique(train_labels2 == typeLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')"
     },
     "metadata": {},
     "execution_count": 534
    }
   ],
   "source": [
    "#Decicion tree\n",
    "from sklearn import tree\n",
    "\n",
    "treeClf = tree.DecisionTreeClassifier()\n",
    "treeClf.fit(X_train, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.2661641280696301"
     },
     "metadata": {},
     "execution_count": 543
    }
   ],
   "source": [
    "treeClf.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Number of labels=38489 does not match number of samples=51466",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-566-a8d6210465ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mforestClf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mforestClf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 250\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels=38489 does not match number of samples=51466"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forestClf = RandomForestClassifier()\n",
    "forestClf.fit(X_train, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.35708734846129936"
     },
     "metadata": {},
     "execution_count": 542
    }
   ],
   "source": [
    "forestClf.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n                     weights='uniform')"
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN_clf.fit(X_train, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5564792684194119"
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "KNN_clf.score(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  9., 11.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 605
    }
   ],
   "source": [
    "np.unique(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[252, 131, 121,  46, 244,  63, 101,  28,  12,   2],\n       [ 27, 504,  69,  19,  99,  87, 103,  67,  23,   2],\n       [ 54, 102, 408,  54, 186,  81,  79,  15,  21,   0],\n       [ 30,  89, 150, 266, 181, 124,  92,  19,  44,   5],\n       [ 11,  50,  63,  13, 422,  21,  25,  13,   5,   0],\n       [  8,  55,  68,  20, 130, 691,  18,   8,   2,   0],\n       [ 40, 186,  94,  38, 182,  46, 363,  32,  18,   1],\n       [ 25,  79,  13,  10, 178,  54,  35, 591,  15,   0],\n       [ 16, 119,  50,  67,  83,  63,  78,  43, 474,   7],\n       [ 11,  46,  14,   9, 167,  15,  22,   6,   3, 707]])"
     },
     "metadata": {},
     "execution_count": 593
    }
   ],
   "source": [
    "confusion_matrix(y_true,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tf-idf svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(train[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scikit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "clf_NB = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('NB', MultinomialNB()),])\n",
    "                   \n",
    "# Support vector machine\n",
    "clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('svm', SGDClassifier()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm.fit(series_content,typeLst )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([ True]), array([9623]))"
     },
     "metadata": {},
     "execution_count": 631
    }
   ],
   "source": [
    "np.unique(test_labels == test_typeLst, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7225397485191728"
     },
     "metadata": {},
     "execution_count": 646
    }
   ],
   "source": [
    "clf_svm.score(test_content, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-645-bc1c4179375a>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-645-bc1c4179375a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    clf_svm.score(test_content, test_labels)clf_svm.score(test_content, test_labels)\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "clf_svm.score(test_content, test_labels)clf_svm.score(test_content, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51466"
     },
     "metadata": {},
     "execution_count": 585
    }
   ],
   "source": [
    "#X_train\n",
    "#len(train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51466"
     },
     "metadata": {},
     "execution_count": 589
    }
   ],
   "source": [
    "train_labels2 = np.array(train_labels2)\n",
    "len(train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train)\n",
    "#len(train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/15\n1609/1609 [==============================] - 1s 670us/step - loss: 1.3161 - accuracy: 0.5580\nEpoch 2/15\n1609/1609 [==============================] - 1s 661us/step - loss: 0.9816 - accuracy: 0.6781\nEpoch 3/15\n1609/1609 [==============================] - 1s 651us/step - loss: 0.9090 - accuracy: 0.7023\nEpoch 4/15\n1609/1609 [==============================] - 1s 660us/step - loss: 0.8664 - accuracy: 0.7163\nEpoch 5/15\n1609/1609 [==============================] - 1s 670us/step - loss: 0.8406 - accuracy: 0.7250\nEpoch 6/15\n1609/1609 [==============================] - 1s 663us/step - loss: 0.8210 - accuracy: 0.7312\nEpoch 7/15\n1609/1609 [==============================] - 1s 659us/step - loss: 0.8061 - accuracy: 0.7354\nEpoch 8/15\n1609/1609 [==============================] - 1s 660us/step - loss: 0.7921 - accuracy: 0.7381\nEpoch 9/15\n1609/1609 [==============================] - 1s 661us/step - loss: 0.7795 - accuracy: 0.7436\nEpoch 10/15\n1609/1609 [==============================] - 1s 667us/step - loss: 0.7703 - accuracy: 0.7458\nEpoch 11/15\n1609/1609 [==============================] - 1s 666us/step - loss: 0.7616 - accuracy: 0.7481\nEpoch 12/15\n1609/1609 [==============================] - 1s 686us/step - loss: 0.7522 - accuracy: 0.7513\nEpoch 13/15\n1609/1609 [==============================] - 1s 686us/step - loss: 0.7448 - accuracy: 0.7556\nEpoch 14/15\n1609/1609 [==============================] - 1s 688us/step - loss: 0.7399 - accuracy: 0.7542\nEpoch 15/15\n1609/1609 [==============================] - 1s 672us/step - loss: 0.7322 - accuracy: 0.7578\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1d7eff35e88>"
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "### Create Model size ###\n",
    "\n",
    "# laver modellen\n",
    "model = tf.keras.models.Sequential()\n",
    "# tilføjer et input på modellen\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# relu er default aktiverings funktion. Lav den om hvis resultatet ikke er godt nok\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "# jeg tilføjer 2 lag til netwærket. Dette er fordi det er en simpel opgave\n",
    "model.add(tf.keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "\n",
    "#antallet (10) er antal output. Det er 10 tal i datasettet derfor skal der være et 10 tal\n",
    "model.add(tf.keras.layers.Dense(20, activation=tf.nn.softmax))\n",
    "\n",
    "# Dette er den mest komplexe del. adam er goto. Hvis der kun er 2 løsninger så brug binary_categorical_crossentropy eller binary_crossentropy i stedet for sparse_categorical_crossentropy\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, train_labels2, epochs = 15)\n",
    "# model.fit(X_train, train_labels2, batch_size=64, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "403/403 [==============================] - 0s 484us/step - loss: 1.0131 - accuracy: 0.6864\n1.013122797012329 0.6863537430763245\n"
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(X_test,test_labels)\n",
    "print(val_loss, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/3\n51466/51466 [==============================] - 26s 496us/step - loss: 1.3845 - accuracy: 0.5289\nEpoch 2/3\n51466/51466 [==============================] - 25s 493us/step - loss: 1.2402 - accuracy: 0.5869\nEpoch 3/3\n51466/51466 [==============================] - 25s 495us/step - loss: 1.2241 - accuracy: 0.5912\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1d7d11bb8c8>"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "model.fit(X_train, train_labels2, batch_size=1, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-81379e84d0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmySentTok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmySentTok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "mySent = \"These are my sentences in the books\"\n",
    "mySentTok = word_tokenize(mySent)\n",
    "ps = PorterStemmer()\n",
    "for w in mySentTok:\n",
    "    mySentTok[w] = ps.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitnewcondae2b41800e1a24f049ef50ffcb90345e7",
   "display_name": "Python 3.7.7 64-bit ('new': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}