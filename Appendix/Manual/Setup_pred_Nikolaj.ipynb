{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           id               domain        type  \\\n6803   322758        zerohedge.com  conspiracy   \n9359  1017129      shadowproof.com     unknown   \n4476   577340  us.blastingnews.com      satire   \n988    695601        breitbart.com   political   \n155   1150037        express.co.uk       rumor   \n\n                                                    url  \\\n6803  https://www.zerohedge.com/news/2014-12-17/new-...   \n9359  https://shadowproof.com/2012/09/18/oregons-mar...   \n4476  http://us.blastingnews.com/news/2017/05/photo/...   \n988   http://www.breitbart.com/author/joseph-c-phill...   \n155   https://www.express.co.uk/sport/football/61716...   \n\n                                                content  \\\n6803  Submitted by Mike Krieger via Liberty Blitzkri...   \n9359  Measure 80, which would legalize marijuana for...   \n4476  This website uses profiling (non technical) co...   \n988   It is said that when Alexander the Great visit...   \n155   Barca, along with Arsenal and Liverpool, have ...   \n\n                      scraped_at                 inserted_at  \\\n6803  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n9359  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n4476  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n988   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n155   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n\n                      updated_at  \\\n6803  2018-02-02 01:19:41.756664   \n9359  2018-02-02 01:19:41.756664   \n4476  2018-02-02 01:19:41.756664   \n988   2018-02-02 01:19:41.756664   \n155   2018-02-02 01:19:41.756664   \n\n                                                  title  \\\n6803  59% Of Americans Support Post-9/11 Torture – P...   \n9359  Oregon’s Marijuana Legalization Measure Traili...   \n4476  Photogallery - Donald Trump 'gaining weight' o...   \n988             Joseph C. Phillips, Author at Breitbart   \n155   Arsenal and Liverpool target to snub bargain £...   \n\n                                                authors  keywords  \\\n6803                                                NaN       NaN   \n9359  Jon Walker, Jonathan Walker Grew Up In New Jer...       NaN   \n4476  Blasting News, P. Ghose, M. Singh, W. Camille,...       NaN   \n988                                  Joseph C. Phillips       NaN   \n155                                           Joe Short       NaN   \n\n                                          meta_keywords  \\\n6803                                               ['']   \n9359                                               ['']   \n4476  ['Donald Trump', 'Russian Scandal', 'Gaining W...   \n988                                                ['']   \n155                                                ['']   \n\n                                       meta_description  \\\n6803  \"By an almost 2-1 margin, or 59-to-31 percent,...   \n9359                                                NaN   \n4476  Article's photos Donald Trump 'gaining weight'...   \n988   Joseph C. Phillips, Author at Breitbart - Page...   \n155   EZEQUIEL LAVEZZI will not be joining Barcelona...   \n\n                                                   tags  summary  source  \\\n6803                                                NaN      NaN     NaN   \n9359                                                NaN      NaN     NaN   \n4476  4062 followers Duggar Family, 3756 followers S...      NaN     NaN   \n988                                                 NaN      NaN     NaN   \n155   Wenger would likely be tempted by a cheap deal...      NaN     NaN   \n\n      type_id  \n6803        2  \n9359       10  \n4476        9  \n988         6  \n155         8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>6803</td>\n      <td>322758</td>\n      <td>zerohedge.com</td>\n      <td>conspiracy</td>\n      <td>https://www.zerohedge.com/news/2014-12-17/new-...</td>\n      <td>Submitted by Mike Krieger via Liberty Blitzkri...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>59% Of Americans Support Post-9/11 Torture – P...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>\"By an almost 2-1 margin, or 59-to-31 percent,...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>9359</td>\n      <td>1017129</td>\n      <td>shadowproof.com</td>\n      <td>unknown</td>\n      <td>https://shadowproof.com/2012/09/18/oregons-mar...</td>\n      <td>Measure 80, which would legalize marijuana for...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Oregon’s Marijuana Legalization Measure Traili...</td>\n      <td>Jon Walker, Jonathan Walker Grew Up In New Jer...</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <td>4476</td>\n      <td>577340</td>\n      <td>us.blastingnews.com</td>\n      <td>satire</td>\n      <td>http://us.blastingnews.com/news/2017/05/photo/...</td>\n      <td>This website uses profiling (non technical) co...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Photogallery - Donald Trump 'gaining weight' o...</td>\n      <td>Blasting News, P. Ghose, M. Singh, W. Camille,...</td>\n      <td>NaN</td>\n      <td>['Donald Trump', 'Russian Scandal', 'Gaining W...</td>\n      <td>Article's photos Donald Trump 'gaining weight'...</td>\n      <td>4062 followers Duggar Family, 3756 followers S...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <td>988</td>\n      <td>695601</td>\n      <td>breitbart.com</td>\n      <td>political</td>\n      <td>http://www.breitbart.com/author/joseph-c-phill...</td>\n      <td>It is said that when Alexander the Great visit...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Joseph C. Phillips, Author at Breitbart</td>\n      <td>Joseph C. Phillips</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>Joseph C. Phillips, Author at Breitbart - Page...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1150037</td>\n      <td>express.co.uk</td>\n      <td>rumor</td>\n      <td>https://www.express.co.uk/sport/football/61716...</td>\n      <td>Barca, along with Arsenal and Liverpool, have ...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Arsenal and Liverpool target to snub bargain £...</td>\n      <td>Joe Short</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>EZEQUIEL LAVEZZI will not be joining Barcelona...</td>\n      <td>Wenger would likely be tempted by a cheap deal...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# importing and creating df (has to have type_id)\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random)\n",
    "#filepath = '../Data_sample/FakeNewsCorpus_250.csv' # 250 rows of FakeNewsCorpus\n",
    "filepath = '/Users/Master/Documents/KU/2.Semester/Datascience/1mioraw.csv'       # 1 mil rows raw\n",
    "s = 1000000                                            # desired sample size\n",
    "seed = 1                                           # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_csv(filepath, index_col = [0]).sample(n=s, random_state=seed)\n",
    "df[\"content\"] = df[\"content\"].astype(str)\n",
    "# create type_id\n",
    "df['type_id'] = df.groupby(['type']).ngroup()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = pd.to_numeric(df['id'], errors = 'coerce', downcast = 'integer')\n",
    "df.drop_duplicates(subset = 'id', inplace = True)\n",
    "df = df.dropna(subset=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6803     322758\n9359    1017129\n4476     577340\n988      695601\n155     1150037\n         ...   \n1240     561926\n1558     952987\n901      534794\n1732     562502\n8026     146392\nName: id, Length: 999934, dtype: int64"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           id               domain        type  \\\n6803   322758        zerohedge.com  conspiracy   \n9359  1017129      shadowproof.com     unknown   \n4476   577340  us.blastingnews.com      satire   \n988    695601        breitbart.com   political   \n155   1150037        express.co.uk       rumor   \n\n                                                    url  \\\n6803  https://www.zerohedge.com/news/2014-12-17/new-...   \n9359  https://shadowproof.com/2012/09/18/oregons-mar...   \n4476  http://us.blastingnews.com/news/2017/05/photo/...   \n988   http://www.breitbart.com/author/joseph-c-phill...   \n155   https://www.express.co.uk/sport/football/61716...   \n\n                                                content  \\\n6803  Submitted by Mike Krieger via Liberty Blitzkri...   \n9359  Measure 80, which would legalize marijuana for...   \n4476  This website uses profiling (non technical) co...   \n988   It is said that when Alexander the Great visit...   \n155   Barca, along with Arsenal and Liverpool, have ...   \n\n                      scraped_at                 inserted_at  \\\n6803  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n9359  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n4476  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n988   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n155   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n\n                      updated_at  \\\n6803  2018-02-02 01:19:41.756664   \n9359  2018-02-02 01:19:41.756664   \n4476  2018-02-02 01:19:41.756664   \n988   2018-02-02 01:19:41.756664   \n155   2018-02-02 01:19:41.756664   \n\n                                                  title  \\\n6803  59% Of Americans Support Post-9/11 Torture – P...   \n9359  Oregon’s Marijuana Legalization Measure Traili...   \n4476  Photogallery - Donald Trump 'gaining weight' o...   \n988             Joseph C. Phillips, Author at Breitbart   \n155   Arsenal and Liverpool target to snub bargain £...   \n\n                                                authors  keywords  \\\n6803                                                NaN       NaN   \n9359  Jon Walker, Jonathan Walker Grew Up In New Jer...       NaN   \n4476  Blasting News, P. Ghose, M. Singh, W. Camille,...       NaN   \n988                                  Joseph C. Phillips       NaN   \n155                                           Joe Short       NaN   \n\n                                          meta_keywords  \\\n6803                                               ['']   \n9359                                               ['']   \n4476  ['Donald Trump', 'Russian Scandal', 'Gaining W...   \n988                                                ['']   \n155                                                ['']   \n\n                                       meta_description  \\\n6803  \"By an almost 2-1 margin, or 59-to-31 percent,...   \n9359                                                NaN   \n4476  Article's photos Donald Trump 'gaining weight'...   \n988   Joseph C. Phillips, Author at Breitbart - Page...   \n155   EZEQUIEL LAVEZZI will not be joining Barcelona...   \n\n                                                   tags  summary  source  \\\n6803                                                NaN      NaN     NaN   \n9359                                                NaN      NaN     NaN   \n4476  4062 followers Duggar Family, 3756 followers S...      NaN     NaN   \n988                                                 NaN      NaN     NaN   \n155   Wenger would likely be tempted by a cheap deal...      NaN     NaN   \n\n      type_id  \n6803        2  \n9359       10  \n4476        9  \n988         6  \n155         8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>domain</th>\n      <th>type</th>\n      <th>url</th>\n      <th>content</th>\n      <th>scraped_at</th>\n      <th>inserted_at</th>\n      <th>updated_at</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>keywords</th>\n      <th>meta_keywords</th>\n      <th>meta_description</th>\n      <th>tags</th>\n      <th>summary</th>\n      <th>source</th>\n      <th>type_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>6803</td>\n      <td>322758</td>\n      <td>zerohedge.com</td>\n      <td>conspiracy</td>\n      <td>https://www.zerohedge.com/news/2014-12-17/new-...</td>\n      <td>Submitted by Mike Krieger via Liberty Blitzkri...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>59% Of Americans Support Post-9/11 Torture – P...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>\"By an almost 2-1 margin, or 59-to-31 percent,...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>9359</td>\n      <td>1017129</td>\n      <td>shadowproof.com</td>\n      <td>unknown</td>\n      <td>https://shadowproof.com/2012/09/18/oregons-mar...</td>\n      <td>Measure 80, which would legalize marijuana for...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Oregon’s Marijuana Legalization Measure Traili...</td>\n      <td>Jon Walker, Jonathan Walker Grew Up In New Jer...</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <td>4476</td>\n      <td>577340</td>\n      <td>us.blastingnews.com</td>\n      <td>satire</td>\n      <td>http://us.blastingnews.com/news/2017/05/photo/...</td>\n      <td>This website uses profiling (non technical) co...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Photogallery - Donald Trump 'gaining weight' o...</td>\n      <td>Blasting News, P. Ghose, M. Singh, W. Camille,...</td>\n      <td>NaN</td>\n      <td>['Donald Trump', 'Russian Scandal', 'Gaining W...</td>\n      <td>Article's photos Donald Trump 'gaining weight'...</td>\n      <td>4062 followers Duggar Family, 3756 followers S...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <td>988</td>\n      <td>695601</td>\n      <td>breitbart.com</td>\n      <td>political</td>\n      <td>http://www.breitbart.com/author/joseph-c-phill...</td>\n      <td>It is said that when Alexander the Great visit...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Joseph C. Phillips, Author at Breitbart</td>\n      <td>Joseph C. Phillips</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>Joseph C. Phillips, Author at Breitbart - Page...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1150037</td>\n      <td>express.co.uk</td>\n      <td>rumor</td>\n      <td>https://www.express.co.uk/sport/football/61716...</td>\n      <td>Barca, along with Arsenal and Liverpool, have ...</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>Arsenal and Liverpool target to snub bargain £...</td>\n      <td>Joe Short</td>\n      <td>NaN</td>\n      <td>['']</td>\n      <td>EZEQUIEL LAVEZZI will not be joining Barcelona...</td>\n      <td>Wenger would likely be tempted by a cheap deal...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> fake (5000, 17) (3500, 17) (750, 17) (750, 17)\n=> satire (5000, 17) (3500, 17) (750, 17) (750, 17)\n=> bias (5000, 17) (3500, 17) (750, 17) (750, 17)\n=> conspiracy (5000, 17) (3500, 17) (750, 17) (750, 17)\n=> junksci (5000, 17) (3500, 17) (750, 17) (750, 17)\n=> hate (3619, 17) (2533, 17) (543, 17) (543, 17)\n=> clickbait (5000, 17) (3500, 17) (750, 17) (750, 17)\n=> unreliable (5000, 17) (3500, 17) (750, 17) (750, 17)\n=> political (5000, 17) (3500, 17) (750, 17) (750, 17)\n=> reliable (5000, 17) (3500, 17) (750, 17) (750, 17)\n\n[Final split]\ntrain, test, validate ==> (34033, 17) (7293, 17) (7293, 17)\n"
    }
   ],
   "source": [
    "# This can generate a dataset with random purmutation and a max size for each type(can be smaller if desired max is not possible)\n",
    "\n",
    "# max size for type\n",
    "max_size = 5000\n",
    "# traning_set ratio - splits data into traning=ratio,  test and validate=(1-ratio)/2 ex. train=80%, test=10%, validate=10%\n",
    "ratio=0.7\n",
    "# Labels to include - ['fake', 'satire', 'bias', 'conspiracy', 'state', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable'] - all labels\n",
    "use_types = ['fake', 'satire', 'bias', 'conspiracy', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable']\n",
    "# Random seed\n",
    "rnd = 1\n",
    "\n",
    "# initialize dataframes\n",
    "train    = pd.DataFrame(columns = df.columns)\n",
    "test     = pd.DataFrame(columns = df.columns)\n",
    "validate = pd.DataFrame(columns = df.columns)\n",
    "\n",
    "# add type to test splits\n",
    "for t in use_types:\n",
    "\n",
    "    # type size\n",
    "    type_size = df['type'].loc[df['type'] == t].value_counts().min()\n",
    "\n",
    "    # set size of type slice\n",
    "    if type_size < max_size:\n",
    "        tmp = df.loc[df['type'] == t].sample(n = type_size, random_state=rnd)\n",
    "    else:\n",
    "        tmp = df.loc[df['type'] == t].sample(n = max_size, random_state=rnd)\n",
    "\n",
    "    # split current type\n",
    "    train_tmp, test_tmp, validate_tmp = np.split(tmp, [int(ratio * len(tmp)), int(((1-ratio)/2 + ratio) * len(tmp))])\n",
    "\n",
    "    # add tmp to dataframes\n",
    "    train    = pd.concat([train, train_tmp])\n",
    "    test     = pd.concat([test, test_tmp])\n",
    "    validate = pd.concat([validate, validate_tmp])\n",
    "    \n",
    "    # print split shape\n",
    "    print(\"=>\", t, tmp.shape, train_tmp.shape, validate_tmp.shape, validate_tmp.shape)\n",
    "\n",
    "print(\"\\n[Final split]\\ntrain, test, validate ==>\", train.shape, test.shape, validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and creating df (has to have type_id)\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "filepath = '/Users/Master/Documents/KU/2.Semester/Datascience/news_sample.csv' #\n",
    "#s = 250                                            # desired sample size\n",
    "#seed = 1                                           # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_csv(filepath, index_col = [0])\n",
    "content = df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"these word are features\")\n",
    "with nlp.disable_pipes():\n",
    "    vectors = np.array([token.vector for token in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "these\nword\n,\nare\nfeatures\n"
    }
   ],
   "source": [
    "doc = nlp(\"these word, are features\")\n",
    "for token in doc:\n",
    "    print(token)\n",
    "#token.lemma_\n",
    "#token.is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nlp.disable_pipes():\n",
    "    doc_vectors = np.array([nlp(sentence).vector for sentence in df['content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeidx = df.index[df.type == 'fake'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakevecs = np.array([doc_vectors[i] for i in fakeidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2.442973"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "\n",
    "len(fakevecs)\n",
    "np.linalg.norm(fakevecs[0])\n",
    "#calc avg vector of the type-vectors:\n",
    "#np.linalg.norm(fakevecs[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.type.fillna('no type', inplace = True)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(doc_vectors, df.type, test_size = 0.1, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nlp('REPLY NOW FOR FREE TEA').vector\n",
    "b = nlp('REPLY NOW FOR FREE TEA AND Potatoes').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 64.000%\n"
    }
   ],
   "source": [
    "svc = LinearSVC(random_state = 1, dual = False, max_iter = 10000)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\",)\n",
    "#svc.score(X_test,y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn.metrics.pairwise.cosine_similarity(a,b)\n",
    "def cosine_similarity(a,b):\n",
    "    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.95132565"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "cosine_similarity(fakevecs[152],doc_vectors[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tf-idf svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, df.type, test_size = 0.1, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.52"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.40884613016820903\n1.0\n"
    }
   ],
   "source": [
    "x = nlp(\"man\")\n",
    "y = nlp(\"king\")\n",
    "print(x.similarity(y))\n",
    "print(x.similarity(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['bag', 'is', 'up']"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "import gensim \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec \n",
    "from collections import namedtuple\n",
    "q = word_tokenize('bag is up')\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.type.fillna('no type', inplace = True)\n",
    "train['type_id'] = train.groupby(['type']).ngroup()\n",
    "train.set_index('id', inplace = True)\n",
    "train.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 2min 57s, sys: 6.51 s, total: 3min 3s\nWall time: 3min 28s\n"
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "series_content = train.content\n",
    "\n",
    "contTok = series_content.apply(nltk.word_tokenize)\n",
    "typeLst =train.type_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "id\n449029    [How, To, Easily, Understand, The, Difference,...\n603573    [Butter, Chocolate, Cheese, Cake, %, of, reade...\n34728     [Chipotle, E., coli, O26, Outbreak, Update, –,...\n337055    [U.S., Repeals, Propaganda, Ban, ,, Spreads, G...\n771816    [Darwin, and, the, Voyage, :, 11, ~, Elephants...\n                                ...                        \n933115    [(, Photo, :, Screengrab/ERLC, ), Russell, Moo...\n788062    [(, Photo, :, Screencap/First, Baptist, Dallas...\n740245    [On, Monday, ,, North, Carolina, sophomore, po...\n930566    [Photo, :, (, YouTube, Screenshot/Timothy, W.,...\n832948    [In, 1968, ,, the, public, anger, over, the, V...\nName: content, Length: 1400, dtype: object"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "contTok\n",
    "#model = Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.doc2vec.Doc2Vec(vector_size=250, min_count=2, epochs=40)\n",
    "#documents = [TaggedDocument(contTok,df.index) for sentence in df]\n",
    "documents = [TaggedDocument(contTok, [train.index[i]]) for i, contTok in enumerate(contTok)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "id\n449029    [How, To, Easily, Understand, The, Difference,...\n603573    [Butter, Chocolate, Cheese, Cake, %, of, reade...\n34728     [Chipotle, E., coli, O26, Outbreak, Update, –,...\n337055    [U.S., Repeals, Propaganda, Ban, ,, Spreads, G...\n771816    [Darwin, and, the, Voyage, :, 11, ~, Elephants...\n                                ...                        \n933115    [(, Photo, :, Screengrab/ERLC, ), Russell, Moo...\n788062    [(, Photo, :, Screencap/First, Baptist, Dallas...\n740245    [On, Monday, ,, North, Carolina, sophomore, po...\n930566    [Photo, :, (, YouTube, Screenshot/Timothy, W.,...\n832948    [In, 1968, ,, the, public, anger, over, the, V...\nName: content, Length: 1400, dtype: object"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "contTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 9min 55s, sys: 13.8 s, total: 10min 9s\nWall time: 5min 10s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model = Doc2Vec(documents, vector_size = 150, min_count = 2, window = 10, epochs= 5, workers = cores, alpha = 0.025, min_alpha = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('salad', 0.825251579284668),\n ('cheese', 0.8202081322669983),\n ('lettuce', 0.8176130652427673),\n ('sauce', 0.8084850907325745),\n ('flour', 0.8066356778144836),\n ('sausage', 0.804688572883606),\n ('potatoes', 0.79893958568573),\n ('sweet', 0.794294536113739),\n ('soup', 0.7887793183326721),\n ('wine', 0.7831469178199768)]"
     },
     "metadata": {},
     "execution_count": 257
    }
   ],
   "source": [
    "model.most_similar('potato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "34033\n"
    }
   ],
   "source": [
    "print((model.docvecs.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "#doc2 = nlp(df.content[20])v\n",
    "#tokdoc2 = [token for token in nlp(df.content[20])] \n",
    "#for token in doc2:\n",
    "#    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.docvecs.most_similar([2]) unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.type[(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(33237, 0.8410108089447021),\n (30508, 0.8348163366317749),\n (32362, 0.8327586650848389),\n (32968, 0.8319017887115479),\n (18620, 0.8270764350891113),\n (31936, 0.8258225917816162),\n (32952, 0.8226457834243774),\n (8898, 0.8212412595748901),\n (32392, 0.820825457572937),\n (19785, 0.8205291032791138)]"
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "sim_doc = model.docvecs.most_similar([30536])\n",
    "sim_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "32362    reliable\nName: type, dtype: object"
     },
     "metadata": {},
     "execution_count": 230
    }
   ],
   "source": [
    "train.loc[train.index == 32362 ].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.loc[train['type'] == 'reliable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all docvecs and labels into arrays for classifier:\n",
    "train_arrays = np.zeros((model.docvecs.count,150))\n",
    "train_labels = np.zeros(model.docvecs.count)\n",
    "for i in range(model.docvecs.count):\n",
    "    train_arrays[i] = model[i]\n",
    "    train_labels[i] = train.type_id[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "34033"
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "source": [
    "train_arrays[0]\n",
    "len(train_labels)\n",
    "#train.loc[train.index == 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_arrays, train_labels = shuffle(train_arrays, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_arrays_shuff, train_labels_shuff, test_size = 0.2, random_state= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([3., 3., 3., ..., 7., 7., 7.])"
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "source": [
    "#len(train_arrays)\n",
    "#train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([5., 5., 5., ..., 7., 7., 7.])"
     },
     "metadata": {},
     "execution_count": 250
    }
   ],
   "source": [
    "len(test_arrays)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "metadata": {},
     "execution_count": 326
    }
   ],
   "source": [
    "#Now train a logistic classifier:\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.2908770383428823"
     },
     "metadata": {},
     "execution_count": 327
    }
   ],
   "source": [
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([4., 8., 9., ..., 7., 7., 5.])"
     },
     "metadata": {},
     "execution_count": 328
    }
   ],
   "source": [
    "classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)"
     },
     "metadata": {},
     "execution_count": 332
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X_train, y_train)\n",
    "#clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5332745702952842"
     },
     "metadata": {},
     "execution_count": 333
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tf-idf svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('salad', 0.825251579284668),\n ('cheese', 0.8202081322669983),\n ('lettuce', 0.8176130652427673),\n ('sauce', 0.8084850907325745),\n ('flour', 0.8066356778144836),\n ('sausage', 0.804688572883606),\n ('potatoes', 0.79893958568573),\n ('sweet', 0.794294536113739),\n ('soup', 0.7887793183326721),\n ('wine', 0.7831469178199768)]"
     },
     "metadata": {},
     "execution_count": 331
    }
   ],
   "source": [
    "model.most_similar('potato')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}