{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and creating df (has to have type_id)\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "filepath = '/Users/Master/Documents/KU/2.Semester/Datascience/news_sample.csv' #\n",
    "#s = 250                                            # desired sample size\n",
    "#seed = 1                                           # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_csv(filepath, index_col = [0])\n",
    "content = df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Sometimes the power of Christmas will make you...\n",
       "1      AWAKENING OF 12 STRANDS of DNA – “Reconnecting...\n",
       "2      Never Hike Alone: A Friday the 13th Fan Film U...\n",
       "3      When a rare shark was caught, scientists were ...\n",
       "4      Donald Trump has the unnerving ability to abil...\n",
       "                             ...                        \n",
       "245    Prison for Rahm, God’s Work And Many Others\\n\\...\n",
       "246    4 Useful Items for Your Tiny Home\\n\\nHeadline:...\n",
       "247    Former CIA Director Michael Hayden said Thursd...\n",
       "248    Antonio Sabato Jr. says Hollywood's liberal el...\n",
       "249    Former U.S. President Bill Clinton on Monday c...\n",
       "Name: content, Length: 250, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"these word are features\")\n",
    "with nlp.disable_pipes():\n",
    "    vectors = np.array([token.vector for token in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these\n",
      "word\n",
      ",\n",
      "are\n",
      "features\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"these word, are features\")\n",
    "for token in doc:\n",
    "    print(token)\n",
    "#token.lemma_\n",
    "#token.is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nlp.disable_pipes():\n",
    "    doc_vectors = np.array([nlp(sentence).vector for sentence in df['content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeidx = df.index[df.type == 'fake'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakevecs = np.array([doc_vectors[i] for i in fakeidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.442973"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(fakevecs)\n",
    "np.linalg.norm(fakevecs[0])\n",
    "#calc avg vector of the type-vectors:\n",
    "#np.linalg.norm(fakevecs[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.fillna('no type', inplace = True)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(doc_vectors, df.type, test_size = 0.1, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nlp('REPLY NOW FOR FREE TEA').vector\n",
    "b = nlp('REPLY NOW FOR FREE TEA AND Potatoes').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.000%\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(random_state = 1, dual = False, max_iter = 10000)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\",)\n",
    "#svc.score(X_test,y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn.metrics.pairwise.cosine_similarity(a,b)\n",
    "def cosine_similarity(a,b):\n",
    "    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95132565"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(fakevecs[152],doc_vectors[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tf-idf svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, df.type, test_size = 0.1, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40884613016820903\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "x = nlp(\"man\")\n",
    "y = nlp(\"king\")\n",
    "print(x.similarity(y))\n",
    "print(x.similarity(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec \n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type_id'] = df.groupby(['type']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sometimes',\n",
       " 'the',\n",
       " 'power',\n",
       " 'of',\n",
       " 'Christmas',\n",
       " 'will',\n",
       " 'make',\n",
       " 'you',\n",
       " 'do',\n",
       " 'wild',\n",
       " 'and',\n",
       " 'wonderful',\n",
       " 'things.',\n",
       " 'You',\n",
       " 'do',\n",
       " 'not',\n",
       " 'need',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Holy',\n",
       " 'Trinity',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'in',\n",
       " 'the',\n",
       " 'positive',\n",
       " 'power',\n",
       " 'of',\n",
       " 'doing',\n",
       " 'good',\n",
       " 'for',\n",
       " 'others.',\n",
       " 'The',\n",
       " 'simple',\n",
       " 'act',\n",
       " 'of',\n",
       " 'giving',\n",
       " 'without',\n",
       " 'receiving',\n",
       " 'is',\n",
       " 'lost',\n",
       " 'on',\n",
       " 'many',\n",
       " 'of',\n",
       " 'us',\n",
       " 'these',\n",
       " 'days,',\n",
       " 'as',\n",
       " 'worries',\n",
       " 'about',\n",
       " 'money',\n",
       " 'and',\n",
       " 'success',\n",
       " 'hold',\n",
       " 'us',\n",
       " 'back',\n",
       " 'from',\n",
       " 'giving',\n",
       " 'to',\n",
       " 'others',\n",
       " 'who',\n",
       " 'are',\n",
       " 'in',\n",
       " 'need.',\n",
       " 'One',\n",
       " 'congregation',\n",
       " 'in',\n",
       " 'Ohio',\n",
       " 'was',\n",
       " 'moved',\n",
       " 'to',\n",
       " 'action',\n",
       " 'by',\n",
       " 'the',\n",
       " 'power',\n",
       " 'of',\n",
       " 'a',\n",
       " 'sermon',\n",
       " 'given',\n",
       " 'at',\n",
       " 'their',\n",
       " 'church',\n",
       " 'on',\n",
       " 'Christmas',\n",
       " 'Eve.',\n",
       " 'The',\n",
       " 'pastor',\n",
       " 'at',\n",
       " 'Grand',\n",
       " 'Lake',\n",
       " 'United',\n",
       " 'Methodist',\n",
       " 'Church',\n",
       " 'in',\n",
       " 'Celina,',\n",
       " 'Ohio',\n",
       " 'gave',\n",
       " 'an',\n",
       " 'emotional',\n",
       " 'sermon',\n",
       " 'about',\n",
       " 'the',\n",
       " 'importance',\n",
       " 'of',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'message',\n",
       " 'of',\n",
       " 'Jesus.',\n",
       " 'For',\n",
       " 'many',\n",
       " 'religious',\n",
       " 'people',\n",
       " 'the',\n",
       " 'message',\n",
       " 'of',\n",
       " 'Jesus',\n",
       " 'is',\n",
       " 'to',\n",
       " 'help',\n",
       " 'others',\n",
       " 'before',\n",
       " 'yourself,',\n",
       " 'to',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'the',\n",
       " 'people',\n",
       " 'who',\n",
       " 'are',\n",
       " 'suffering',\n",
       " 'get',\n",
       " 'the',\n",
       " 'help',\n",
       " 'they',\n",
       " 'need',\n",
       " 'to',\n",
       " 'enjoy',\n",
       " 'life',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit.',\n",
       " 'The',\n",
       " 'sermon',\n",
       " 'was',\n",
       " 'really',\n",
       " 'about',\n",
       " 'generosity',\n",
       " 'and',\n",
       " 'what',\n",
       " 'that',\n",
       " 'can',\n",
       " 'look',\n",
       " 'like',\n",
       " 'in',\n",
       " 'our',\n",
       " 'lives.',\n",
       " 'Jesus',\n",
       " 'lived',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'he',\n",
       " 'acted',\n",
       " 'generously',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fashion',\n",
       " 'of',\n",
       " 'his',\n",
       " 'time',\n",
       " '–',\n",
       " 'but',\n",
       " 'what',\n",
       " 'would',\n",
       " 'a',\n",
       " 'generous',\n",
       " 'act',\n",
       " 'look',\n",
       " 'like',\n",
       " 'in',\n",
       " 'our',\n",
       " 'times?',\n",
       " 'That',\n",
       " 'was',\n",
       " 'the',\n",
       " 'focus',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sermon.',\n",
       " 'The',\n",
       " 'potency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sermon',\n",
       " 'was',\n",
       " 'not',\n",
       " 'lost',\n",
       " 'on',\n",
       " 'the',\n",
       " 'congregation,',\n",
       " 'who',\n",
       " 'were',\n",
       " 'so',\n",
       " 'moved',\n",
       " 'they',\n",
       " 'had',\n",
       " 'to',\n",
       " 'take',\n",
       " 'action!',\n",
       " 'After',\n",
       " 'the',\n",
       " 'sermon',\n",
       " 'ended,',\n",
       " 'the',\n",
       " 'congregation',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'take',\n",
       " 'an',\n",
       " 'offering.',\n",
       " 'A',\n",
       " 'bowl',\n",
       " 'was',\n",
       " 'passed',\n",
       " 'around',\n",
       " 'the',\n",
       " 'room',\n",
       " 'and',\n",
       " 'everyone',\n",
       " 'pitched',\n",
       " 'in',\n",
       " 'what',\n",
       " 'they',\n",
       " 'could',\n",
       " 'on',\n",
       " 'this',\n",
       " 'Christmas',\n",
       " 'Eve',\n",
       " 'with',\n",
       " 'the',\n",
       " 'words',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sermon',\n",
       " 'still',\n",
       " 'ringing',\n",
       " 'in',\n",
       " 'their',\n",
       " 'ears.',\n",
       " 'What',\n",
       " 'did',\n",
       " 'they',\n",
       " 'do',\n",
       " 'with',\n",
       " 'this',\n",
       " 'offering?',\n",
       " 'Members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'congregation',\n",
       " 'drove',\n",
       " 'down',\n",
       " 'to',\n",
       " 'the',\n",
       " 'local',\n",
       " 'Waffle',\n",
       " 'House',\n",
       " 'to',\n",
       " 'visit',\n",
       " 'the',\n",
       " 'ladies',\n",
       " 'working',\n",
       " 'the',\n",
       " 'night',\n",
       " 'shift.',\n",
       " 'What',\n",
       " 'a',\n",
       " 'great',\n",
       " 'choice',\n",
       " 'on',\n",
       " 'this',\n",
       " 'most',\n",
       " 'holy',\n",
       " 'of',\n",
       " 'days',\n",
       " 'when',\n",
       " 'everyone',\n",
       " 'should',\n",
       " 'be',\n",
       " 'with',\n",
       " 'their',\n",
       " 'families!',\n",
       " 'The',\n",
       " 'ladies',\n",
       " 'working',\n",
       " 'at',\n",
       " 'Waffle',\n",
       " 'House',\n",
       " 'clearly',\n",
       " 'were',\n",
       " 'not',\n",
       " 'with',\n",
       " 'their',\n",
       " 'families.',\n",
       " 'They',\n",
       " 'had',\n",
       " 'no',\n",
       " 'choice',\n",
       " 'but',\n",
       " 'to',\n",
       " 'work',\n",
       " 'on',\n",
       " 'this',\n",
       " 'holy',\n",
       " 'day',\n",
       " 'because',\n",
       " 'it',\n",
       " 'paid',\n",
       " 'the',\n",
       " 'bills.',\n",
       " 'The',\n",
       " 'congregation',\n",
       " 'understood',\n",
       " 'the',\n",
       " 'sacrifice',\n",
       " 'being',\n",
       " 'made',\n",
       " 'by',\n",
       " 'these',\n",
       " 'ladies,',\n",
       " 'and',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'help',\n",
       " 'them',\n",
       " 'out.',\n",
       " 'They',\n",
       " 'donated',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'offering',\n",
       " 'to',\n",
       " 'be',\n",
       " 'split',\n",
       " 'amongst',\n",
       " 'the',\n",
       " 'ladies',\n",
       " 'at',\n",
       " 'Waffle',\n",
       " 'House.',\n",
       " 'In',\n",
       " 'total',\n",
       " 'that',\n",
       " 'amounted',\n",
       " 'to',\n",
       " '$3,500',\n",
       " 'being',\n",
       " 'split',\n",
       " 'amongst',\n",
       " 'the',\n",
       " 'staff.',\n",
       " 'What',\n",
       " 'a',\n",
       " 'beautiful',\n",
       " 'moment!',\n",
       " 'What',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'example',\n",
       " 'of',\n",
       " 'what',\n",
       " 'the',\n",
       " 'preacher',\n",
       " 'was',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'in',\n",
       " 'his',\n",
       " 'sermon!',\n",
       " 'Doing',\n",
       " 'a',\n",
       " 'good',\n",
       " 'deed',\n",
       " 'like',\n",
       " 'this',\n",
       " 'on',\n",
       " 'Christmas',\n",
       " 'really',\n",
       " 'helped',\n",
       " 'ease',\n",
       " 'the',\n",
       " 'burden',\n",
       " 'felt',\n",
       " 'by',\n",
       " 'the',\n",
       " 'ladies',\n",
       " 'working',\n",
       " 'at',\n",
       " 'Waffle',\n",
       " 'House.',\n",
       " 'Sure,',\n",
       " 'they',\n",
       " 'could',\n",
       " 'not',\n",
       " 'see',\n",
       " 'their',\n",
       " 'families,',\n",
       " 'but',\n",
       " 'at',\n",
       " 'least',\n",
       " 'they',\n",
       " 'got',\n",
       " 'a',\n",
       " 'little',\n",
       " 'gift',\n",
       " 'from',\n",
       " 'the',\n",
       " 'good',\n",
       " 'people',\n",
       " 'of',\n",
       " 'their',\n",
       " 'community.',\n",
       " 'Perhaps',\n",
       " 'the',\n",
       " 'best',\n",
       " 'part',\n",
       " 'about',\n",
       " 'this',\n",
       " 'whole',\n",
       " 'event',\n",
       " 'was',\n",
       " 'that',\n",
       " 'the',\n",
       " 'congregation',\n",
       " 'did',\n",
       " 'not',\n",
       " 'ask',\n",
       " 'anything',\n",
       " 'in',\n",
       " 'return.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'act',\n",
       " 'of',\n",
       " 'generosity',\n",
       " 'from',\n",
       " 'people',\n",
       " 'who',\n",
       " 'understood',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'being',\n",
       " 'felt',\n",
       " 'by',\n",
       " 'another',\n",
       " 'group',\n",
       " 'and',\n",
       " 'sought',\n",
       " 'to',\n",
       " 'alleviate',\n",
       " 'some',\n",
       " 'of',\n",
       " 'that',\n",
       " 'pain.',\n",
       " 'It',\n",
       " 'speaks',\n",
       " 'volumes',\n",
       " 'about',\n",
       " 'the',\n",
       " 'merits',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Church',\n",
       " 'in',\n",
       " 'our',\n",
       " 'daily',\n",
       " 'lives.',\n",
       " 'This',\n",
       " 'simple',\n",
       " 'act',\n",
       " 'brought',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'community',\n",
       " 'together',\n",
       " 'because',\n",
       " 'it',\n",
       " 'showed',\n",
       " 'empathy',\n",
       " 'and',\n",
       " 'compassion',\n",
       " 'on',\n",
       " 'the',\n",
       " 'most',\n",
       " 'special',\n",
       " 'day',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year.']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "series_content = df.content\n",
    "\n",
    "contTok =series_content.str.split().tolist()\n",
    "typeLst =df.type_id.tolist()\n",
    "\n",
    "doc1 = [\"This is a sentence\", \"This is another sentence\"]\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(doc1):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.doc2vec.Doc2Vec(vector_size=250, min_count=2, epochs=40)\n",
    "#documents = [TaggedDocument(contTok,df.index) for sentence in df]\n",
    "documents = [TaggedDocument(contTok, [i]) for i, contTok in enumerate(contTok)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was', 'utterly', 'nothing', 'objectionable', 'in', 'any', 'way,', 'shape', 'or', 'form', 'in', 'that', 'video,', 'but', 'since', 'she', 'was', 'out', 'of', 'town', 'and', 'I', 'noted', 'the', 'lack', 'of', 'ads', 'I', 'filed', 'the', 'protest', 'for', 'her.', 'They', 'turned', 'it', 'back', 'on.', 'Instead', 'of', 'looking', 'before', 'killing', 'it', 'in', 'the', 'first', 'place,', 'of', 'course,', 'they', 'just', 'used', 'a', 'computer', 'to', 'do', 'it,', 'you', 'see,', 'because', '30', 'seconds', 'of', 'actual', 'looking', 'would', 'have', 'disclosed…..', 'a', 'cat', 'and', 'a', 'hike.', 'It', 'also', 'would', 'have', 'meant', 'Google', 'would', 'have', 'made', 'a', 'bit', 'less', 'money', 'to', 'actually', 'employ', 'people,', 'and,', 'well,', 'we', 'can’t', 'have', 'that.', 'Jobs?', 'Forget', 'about', 'it.', 'Then', 'there’s', 'Chicago', 'and', 'Goldman’s', 'latest', 'fraud,', 'and', 'what', 'a', 'doozey', 'that', 'one', 'is.', 'The', 'City,', 'which', 'incidentally', 'is', 'basically', 'bankrupt', 'and', 'has', 'insane', 'pension', 'liabilities,', 'created', 'a', '“new”', 'corporation,', 'assigned', 'it', 'alleged', '“preference”', 'for', 'sales', 'tax', 'revenues,', 'and', 'then', 'had', 'it', 'issue', 'bonds', 'in', 'order', 'to', 'game', 'the', 'ratings.', 'The', 'result?', 'A', '“AAA”', 'rated', 'bond', 'issue', 'for', 'a', 'bankrupt', 'municipality.', 'May', 'I', 'remind', 'you', 'that', 'if', 'you', 'tried', 'this', 'you’d', 'be', 'tossed', 'in', 'prison', 'for', '30', 'years', 'for', 'fraud', 'and', 'the', 'entire', 'set', 'of', 'transactions', 'would', 'be', 'unwound', 'because', 'you', 'cannot', 'structure', 'your', 'finances', 'to', 'avoid', 'paying', 'creditors', 'after', 'the', 'event', 'that', 'gives', 'rise', 'to', 'the', 'liability', 'has', 'happened.', 'For', 'example,', 'you', 'cannot', 'get', 'into', 'a', 'car', 'accident', 'where', 'you’re', 'at', 'fault', 'and', 'then', 'put', 'your', 'assets', 'in', 'a', 'trust', 'to', 'avoid', 'losing', 'them', 'in', 'the', 'ensuing', 'lawsuit.', 'Yet', 'that’s', 'exactly', 'what', 'Chicago', 'did', 'with', 'Goldman’s', 'help', 'to', 'both', 'set', 'it', 'up', 'and', 'run', 'the', 'bond', 'offering.', 'This', 'is', 'blatant', 'and', 'outrageous', 'fraud', 'upon', 'every', 'single', 'previous', 'creditor,', 'and', 'if', 'you', 'or', 'I', 'tried', 'it', 'not', 'only', 'would', 'the', 'transaction', 'be', 'unwound', 'we’d', 'be', 'indicted', 'on', 'top', 'of', 'it.', 'Well?', 'Where', 'are', 'the', 'handcuffs', 'on', 'Rahm', 'and', 'why', 'isn’t', 'Goldman', 'shut', 'down', 'right', 'here', 'and', 'now', 'as', 'a', 'criminal', 'enterprise', 'with', 'Mr.', 'God’s', 'Work', 'being', 'led', 'off', 'in', 'cuffs?', 'Then', 'you', 'have', 'the', 'grand-daddy', 'of', 'all,', 'which', 'is', 'quite-clearly', 'outlined', 'here', '—', 'not', 'that', 'if', 'you’ve', 'read', 'my', 'column', 'you', 'need', 'another', 'example.', 'It', 'deals', 'with', 'the', 'medical', 'scam.', 'Just', 'a', 'few', 'miles', 'south', 'having', 'your', 'appendix', 'removed', 'is', '1/10th', 'of', 'the', 'cost', 'of', 'having', 'it', 'done', 'here.', 'The', 'difference?', 'Blatant', 'and', 'lawless', 'behavior', 'that', 'violates', '100+', 'year', 'old', 'anti-trust', 'law.', 'This', 'law', 'is', 'not', 'just', 'civil', 'in', 'nature', 'either;', 'it', 'is', 'a', 'felony', 'to', 'even', 'attempt', 'to', 'monopolize', 'trade', 'or', 'commerce.', 'May', 'I', 'remind', 'you', 'that', 'this', 'body', 'of', 'law', '(15', 'USC', 'Chapter', '1,', 'Sections', '1', 'and', '2)', 'do', 'not', 'require', 'that', 'prices', 'go', 'up,', 'although', 'in', 'the', 'case', 'of', 'medical', 'care', 'of', 'all', 'sorts', 'they', 'sure', 'as', 'hell', 'have', '–', 'by', 'a', 'factor', 'of', '10', 'or', 'more.', 'The', 'reason', 'the', 'law', 'was', 'written', 'to', 'not', 'require', 'pricing', 'to', 'increase', 'is', 'that', 'it', 'is', 'extremely', 'common', 'for', 'monopolists', 'to', 'cross-subsidize', '—', 'that', 'is,', 'screw', 'someone', 'else', 'so', 'you', 'think', 'you’re', 'getting', 'a', '“deal”.', 'It’s', 'illegal', 'irrespective', 'of', 'price', 'rises', '(or', 'not)', 'for', 'the', 'precise', 'reason', 'that', 'the', 'people', 'who', 'wrote', 'the', 'law', 'at', 'the', 'time', 'were', 'well-aware', 'that', 'it', 'is', 'trivial', 'for', 'large,', 'powerful', 'corporations', 'to', 'cost-shift', 'and', 'thus', 'hide', 'what', 'they’re', 'doing,', 'making', 'it', 'appear', 'that', 'you’re', 'getting', 'a', 'reasonable', 'deal', 'when', 'in', 'fact', 'they’re', 'jacking', 'people', 'up', 'the', 'cornhole', 'left,', 'right', 'and', 'center.', 'Amazon', 'anyone?', 'Steve', 'Forbes', 'recently', 'wrote', 'on', 'this', 'but', 'he', 'has', 'no', 'sack', 'either', 'for', 'he', 'has', 'refused', 'to', 'use', 'the', '“F”', 'word', '—', 'consistently', 'and', 'over', 'decades.', 'See,', 'people', 'don’t', 'want', 'to', 'use', 'the', '“F”', 'word', '(no,', 'not', '****,', 'felony)', 'because', 'if', 'the', 'American', 'public', 'was', 'to', 'actually', 'start', 'reading', 'said', 'law,', 'and', 'realized', 'that', 'the', 'first', 'two', 'sections', 'consisted', 'of', 'all', 'of', 'a', 'couple', 'of', 'paragraphs', 'and', 'nowhere', 'is', 'there', 'a', 'requirement', 'that', 'prices', 'go', 'up', '(never', 'mind', 'that', 'they', 'sure', 'as', 'hell', 'have', 'in', 'the', 'medical', 'field)', 'they', 'might', 'pick', 'up', 'pitchforks', 'and', 'torches', 'and', 'demand', 'that', 'people', 'start', 'going', 'to', 'prison', 'right', 'now', '“or', 'else.”', 'I', 'mean', 'it’s', 'not', 'like', 'hospital', 'administrators', '(who', 'provide', 'zero', 'care', 'to', 'patients)', 'have', 'risen', 'in', 'count', 'by', 'over', '3,000%', 'while', 'doctors', 'have', 'risen', 'by', 'something', 'like', '100%', 'from', '1970', 'to', 'today,', 'or', 'medical', 'costs', 'have', 'gone', 'up', '800%', 'while', 'wages', 'have', 'risen', '16%.', 'Oh', 'wait…..', 'People', 'have', 'told', 'me', 'that', 'I’m', 'being', 'completely', 'unrealistic', 'when', 'I', 'point', 'out', 'that', 'fixing', 'this', 'would', 'cause', 'medical', 'costs', 'to', 'drop', 'by', '80%', 'or', 'more,', 'making', '“insurance”', 'completely', 'unnecessary', 'for', '95%', 'of', 'all', 'things', 'medical', 'and', 'the', 'cost', 'of', 'insurance', 'for', 'the', 'remaining', '5%', 'about', 'as', 'expensive', 'on', 'an', 'annual', 'basis', 'as', 'one', 'nice', 'night', 'out', 'on', 'the', 'town.', 'In', 'other', 'words', 'with', 'the', 'exception', 'of', 'the', 'truly', 'destitute', 'nobody', 'would', 'need', 'any', 'help', 'at', 'all', 'from', 'government', 'or', 'anywhere', 'else.', 'Don’t', 'tell', 'me', 'that', 'this', 'outcome', 'is', 'impossible', 'either;', 'if', 'you’re', 'older', 'than', '45', 'or', 'so', 'your', 'parents', 'did', 'exactly', 'that', 'when', 'you', 'were', 'a', 'child.', 'Were', 'you', 'stoned', 'when', 'you', 'were', 'six', 'or', 'are', 'you', 'intentionally', 'refusing', 'to', 'face', 'facts?', 'I', 'remember', 'the', 'doctor’s', 'office', '—', 'exactly', 'where', 'the', 'building', 'was,', 'the', 'waiting', 'room', '(divided', 'into', 'two', 'sections;', 'one', 'for', 'well', 'kids', 'there', 'for', 'routine', 'things,', 'the', 'other', 'for', 'sick', 'kids', '—', 'an', 'attempt', 'to', 'avoid', 'getting', 'the', 'well', 'kids', 'sick!),', 'the', 'front', 'desk', '(where', 'you', 'paid,', 'natch)', 'and', 'the', 'little', 'exam', 'rooms.', 'I', 'also', 'remember', 'my', 'very-much', 'middle', 'class', 'mother', 'writing', 'a', 'check', 'for', 'the', 'modest', 'cost', 'incurred.', 'There', 'were', 'no', 'insurance', 'cards', 'and', 'no', 'angst', 'about', 'a', 'visit', 'to', 'the', 'doctor,', 'if', 'you', 'really', 'needed', 'one.', 'You', 'called,', 'you', 'showed', 'up,', 'you', 'had', 'whatever', 'you', 'needed', 'attended', 'to', 'taken', 'care', 'of,', 'you', 'wrote', 'a', 'check', 'and', 'left.', 'That’s', 'because', 'it', 'didn’t', 'bankrupt', 'you.', 'Read', 'More', '@', 'Market-Ticker.org', 'Source:', 'https://www.sgtreport.com/articles/2017/12/6/prison-for-rahm-gods-work-and-many-others'], tags=[245]),\n",
       " TaggedDocument(words=['4', 'Useful', 'Items', 'for', 'Your', 'Tiny', 'Home', 'Headline:', 'Bitcoin', '&', 'Blockchain', 'Searches', 'Exceed', 'Trump!', 'Blockchain', 'Stocks', 'Are', 'Next!', 'While', 'a', 'tiny', 'home', 'is', 'great', 'because', 'of', 'its', 'low', 'price', 'compared', 'to', 'a', 'bigger', 'property,', 'the', 'limitations', 'on', 'space', 'mean', 'you', 'must', 'make', 'other', 'adjustments', 'with', 'how', 'you', 'furnish', 'it.', 'A', 'certain', 'degree', 'of', 'creative', 'thinking', 'is', 'needed', 'to', 'fit', 'in', 'the', 'kind', 'of', 'useful', 'things', 'that', 'will', 'help', 'make', 'the', 'place', 'feel', 'homely', 'but', 'also', 'not', 'cramped', 'at', 'the', 'same', 'time.', 'Royalty', 'Free', 'Photo', 'There', 'are', 'also', 'a', 'few', 'essentials', 'that', 'you’ll', 'find', 'you', 'won’t', 'want', 'to', 'go', 'without', 'regardless', 'of', 'space', 'considerations.', 'Think', 'about', 'how', 'much', 'each', 'item', 'weighs', 'because', 'one', 'of', 'the', 'points', 'of', 'a', 'tiny', 'home', 'is', 'going', 'minimalist', 'in', 'the', 'number', 'and', 'the', 'weight', 'of', 'all', 'your', 'possessions.', 'Let’s', 'take', 'a', 'look', 'at', 'four', 'useful', 'items', 'that', 'your', 'tiny', 'home', 'cannot', 'easily', 'do', 'without.', 'Washer', '&', 'Dryer', 'Machine', 'While', 'it’s', 'possible', 'to', 'do', 'without', 'the', 'washer', 'and', 'dryer', 'equipment,', 'having', 'to', 'trek', 'your', 'dirty', 'laundry', 'down', 'to', 'the', 'laundromat', 'every', 'week', 'gets', 'old', 'real', 'fast.', 'If', 'you', 'don’t', 'have', 'a', 'car,', 'you’ll', 'have', 'to', 'carry', 'it', 'over', 'your', 'shoulder', 'or', 'in', 'a', 'backpack', 'which', 'won’t', 'be', 'too', 'comfortable', 'either.', 'It’s', 'true', 'that', 'making', 'the', 'space', 'for', 'the', 'washing', 'in', 'your', 'tiny', 'home', 'isn’t', 'easy,', 'but', 'it’s', 'well', 'worth', 'it.', 'To', 'cut', 'down', 'on', 'the', 'floor', 'space', 'used,', 'there', 'are', 'combination', 'units', 'you', 'can', 'buy', 'to', 'avoid', 'having', 'separate', 'ones.', 'Pair', 'the', 'washer', 'and', 'dryer', 'machine', 'with', 'a', 'drying', 'rack', 'that', 'works', 'both', 'inside', 'or', 'outside', 'your', 'home', '(weather', 'dependent)', 'and', 'you’re', 'all', 'set.', 'Space', 'Heater', 'It', 'might', 'be', 'warm', 'enough', 'now,', 'but', 'when', 'the', 'temperature', 'drops', 'you', 'will', 'be', 'glad', 'you', 'bought', 'a', 'space', 'heater', 'to', 'warm', 'up', 'the', 'place.', 'These', 'products', 'are', 'now', 'more', 'energy', 'efficient', 'than', 'in', 'the', 'past', 'with', 'thermostats', 'to', 'turn', 'them', 'off', 'when', 'the', 'home', 'reaches', 'an', 'agreeable', 'temperature.', 'They', 'are', 'also', 'not', 'as', 'expensive', 'to', 'run', 'as', 'you’d', 'think', 'because', 'you’re', 'not', 'having', 'to', 'heat', 'multiple', 'rooms;', 'the', 'warmth', 'spreads', 'around', 'the', 'home', 'evenly', 'as', 'it', 'radiates', 'out.', 'Small', 'Refrigerator', 'When', 'it', 'comes', 'to', 'the', 'refrigerator,', 'there', 'are', 'a', 'good', 'number', 'of', 'choices', 'available.', 'You', 'can,', 'of', 'course,', 'look', 'for', 'a', 'tall', 'refrigerator', 'to', 'take', 'maximum', 'advantage', 'of', 'the', 'height', 'of', 'your', 'home.', 'However,', 'these', 'units', 'do', 'use', 'considerable', 'amounts', 'of', 'energy.', 'A', 'compact', 'refrigerator', 'that', 'might', 'even', 'fit', 'under', 'a', 'well-designed', 'kitchen', 'counter', 'top', 'is', 'ideal', 'because', 'you', 'can', 'reach', 'it', 'without', 'having', 'to', 'stretch', 'up.', 'Alternatively,', 'installing', 'a', 'refrigerator', 'under', 'the', 'stairs', 'or', 'as', 'part', 'of', 'a', 'wall', 'are', 'other', 'options', 'if', 'you’re', 'really', 'trying', 'to', 'save', 'space', 'and', 'don’t', 'care', 'about', 'the', 'unit', 'being', 'in', 'the', 'kitchen', 'area.', 'Composting', 'Toilet', 'Facility', 'Having', 'a', 'toilet', 'is', 'a', 'must', 'for', 'most', 'tiny', 'home', 'owners.', 'You', 'don’t', 'want', 'to', 'have', 'to', 'rely', 'on', 'local', 'facilities', 'in', 'the', 'dead', 'of', 'night', 'when', 'you', 'get', 'caught', 'short.', 'The', 'composting', 'version', 'doesn’t', 'use', 'water', 'and', 'is', 'environmentally', 'friendly.', 'It’s', 'easy', 'to', 'empty', 'on', 'a', 'regular', 'basis', 'and', 'doesn’t', 'take', 'up', 'too', 'much', 'space', 'either.', 'There’s', 'also', 'no', 'black', 'water', 'tank,', 'so', 'it’s', 'more', 'convenient', 'that', 'way', 'too.', 'Your', 'tiny', 'home', 'doesn’t', 'need', 'too', 'many', 'items', 'to', 'feel', 'comfy,', 'but', 'there', 'are', 'some', 'that', 'we’d', 'think', 'are', 'essential', 'to', 'your', 'overall', 'enjoyment.', 'You', 'want', 'to', 'avoid', 'adding', 'too', 'many', 'extras', 'in', 'your', 'tiny', 'home', 'as', 'the', 'point', 'is', 'to', 'live', 'small,', 'but', 'there', 'are', 'some', 'things', 'you', 'just', 'cannot', 'do', 'without.'], tags=[246]),\n",
       " TaggedDocument(words=['Former', 'CIA', 'Director', 'Michael', 'Hayden', 'said', 'Thursday', '\"we', 'all', 'collectively', 'should', 'be', 'a', 'little', 'bit', 'frightened\"', 'by', 'Donald', 'Trump', 'reported', 'remarks', 'about', 'immigrants', 'from', 'certain', '\"s**thole', 'countries.\"', '\"We', 'all', 'collectively', 'should', 'be', 'a', 'bit', 'embarrassed,\"', 'Hayden,', 'who', 'also', 'directed', 'the', 'NSA,', 'told', 'Erin', 'Burnett', 'in', 'an', 'interview', 'on', 'CNN.', '\"We', 'all', 'collectively', 'should', 'be', 'a', 'bit', 'ashamed', '—', 'and,', 'frankly,', 'Erin,', 'I', 'think', 'we', 'all', 'collectively', 'should', 'be', 'a', 'little', 'bit', 'frightened.', '\"That\\'s', 'the', 'president', 'of', 'the', 'United', 'States,\"', 'Hayden', 'said.', '\"He', 'is', 'not', 'appealing', 'to', 'the', 'better', 'angels', 'of', 'our', 'nature', 'or', 'our', \"nation's\", 'values.\"', 'The', 'Washington', 'Post', 'reported', 'Thursday', 'that', 'President', 'Trump', 'grew', 'frustrated', 'in', 'a', 'bipartisan', 'meeting', 'on', 'immigration', 'as', 'lawmakers', 'floated', 'the', 'idea', 'of', 'restoring', 'protections', 'for', 'immigrants', 'from', 'Africa,', 'El', 'Salvador,', 'and', 'Haiti.', '\"Why', 'are', 'we', 'having', 'all', 'these', 'people', 'from', 's**thole', 'countries', 'come', 'here?\"', 'Trump', 'asked,', 'according', 'to', 'the', 'Post,', 'referring', 'to', 'the', 'African', 'countries', 'and', 'Haiti.', 'The', 'newspaper', 'cited', 'two', 'unnamed', 'sources', 'who', 'were', 'briefed', 'on', 'the', 'meeting', 'in', 'its', 'report.', 'Hayden', 'declined', 'to', '\"make', 'a', 'personal', 'judgment', 'on', 'the', 'president\"', 'but', 'called', 'his', 'comments', '\"totally', 'inappropriate.\"', '\"I', \"don't\", 'know', 'of', 'any', 'other', 'example', 'in', 'recent', 'American', 'history', '—', 'we', 'have', 'a', 'muddled', 'past', '—', 'but', 'in', 'recent', 'history', 'where', 'a', 'president', 'says', 'something', 'in', 'environment', 'where', '.', '.', '.', 'he', 'knew', 'it', 'would', 'become', 'public.\"'], tags=[247]),\n",
       " TaggedDocument(words=['Antonio', 'Sabato', 'Jr.', 'says', \"Hollywood's\", 'liberal', 'elite', 'desperately', 'wants', 'Oprah', 'Winfrey', 'to', 'run', 'for', 'president', 'in', '2020', 'because', 'she', 'is', 'the', 'Democratic', \"Party's\", 'one', 'and', 'only', 'chance', 'to', 'regain', 'power', 'in', 'Washington.', '\"I', 'understand', 'why', 'Hollywood', 'wants', 'Oprah', 'to', 'run', 'because', 'I', 'think', \"she's\", 'the', 'last', 'hope', 'for', 'their', 'party,\"', 'the', 'actor-turned-congressional', 'candidate', 'said', 'Monday', 'to', 'Miranda', 'Khan', 'on', 'Newsmax', \"TV's\", '\"America', 'Talks', 'Live.\"', '\"Oprah', 'has', 'been', 'asked', 'numerous', 'times', 'to', 'run', 'and', \"she's\", 'said', 'no,', 'not', 'now.', \"We'll\", 'see', 'in', 'a', 'couple', 'of', 'years', 'or', 'a', 'year', 'and', 'a', 'half', 'or', 'whatever.\"', 'Important:', 'Newsmax', 'TV', 'is', 'available', 'on', 'DirecTV', 'Ch.', '349,', 'U-Verse', '1220,', 'Dish', '216', 'and', 'FiOS', '615.', 'If', 'your', 'cable', 'operator', \"doesn't\", 'have', 'Newsmax', 'TV', 'just', 'call', 'and', 'ask', 'them', 'to', 'put', 'us', 'on', '–', 'Call', 'toll-free', '1-844-500-6397', 'and', \"we'll\", 'connect', 'you', 'right', 'away', 'to', 'your', 'cable', 'operator!', 'For', 'more', 'places', 'to', 'find', 'Newsmax', 'TV', '–', 'Click', 'Here', 'Now', 'Sabato', 'was', 'commenting', 'on', 'the', 'empowering', 'speech', 'the', 'TV', 'talk', 'queen', 'and', 'movie', 'actress', 'gave', 'in', 'Beverly', 'Hills', 'as', 'she', 'received', 'the', 'Cecil', 'B.', 'DeMille', 'Award', 'for', 'lifetime', 'achievement', 'at', \"Sunday's\", '75th', 'Golden', 'Globe', 'Awards', 'ceremony.', 'Winfrey', 'called', 'it', 'a', '\"new', 'day\"', 'for', 'women', 'in', 'light', 'of', 'the', 'dozens', 'of', 'sexual', 'harassment', 'assault', 'complaints', 'lodged', 'against', 'male', 'actors', 'in', 'recent', 'months.', '\"When', 'that', 'new', 'day', 'finally', 'dawns,', 'it', 'will', 'be', 'because', 'of', 'a', 'lot', 'of', 'magnificent', 'women,', 'many', 'of', 'whom', 'are', 'right', 'here', 'in', 'this', 'room', 'tonight,', 'and', 'some', 'pretty', 'phenomenal', 'men,', 'fighting', 'hard', 'to', 'make', 'sure', 'that', 'they', 'become', 'the', 'leaders', 'who', 'take', 'us', 'to', 'the', 'time', 'when', 'nobody', 'ever', 'has', 'to', 'say', \"'Me\", \"too'\", 'again,\"', 'she', 'told', 'the', 'star-studded', 'crowd.', 'After', 'her', 'address,', 'which', 'brought', 'thunderous', 'applause,', 'Winfrey', 'said', 'she', 'has', 'no', 'plans', 'to', 'throw', 'her', 'hat', 'in', 'the', 'ring.', 'But', 'her', 'longtime', 'partner', 'Stedman', 'Graham', 'told', 'The', 'Los', 'Angeles', 'Times:', '\"It\\'s', 'up', 'to', 'the', 'people.', 'She', 'would', 'absolutely', 'do', 'it.\"', 'Sabato', '—', 'a', 'former', 'Calvin', 'Klein', 'model', 'who', 'has', 'appeared', 'in', 'TV', 'shows', 'like', '\"General', 'Hospital\"', 'and', '\"Melrose', 'Place,\"', 'and', 'has', 'now', 'entered', 'politics', 'by', 'facing', 'off', 'Democratic', 'Rep.', 'Julia', 'Brownley', 'for', \"California's\", '26th', 'District', '–', 'said', 'he', 'liked', \"Winfrey's\", 'address,', 'but', 'it', 'could', 'have', 'gone', 'further.', '\"I', 'think', 'the', 'speech', 'was', 'OK.', 'It', 'was', 'good', 'for', 'what', 'it', 'was', 'going', 'after,\"', 'he', 'told', 'Khan.', '\"But', 'I', 'think', 'it', \"would've\", 'been', 'great', 'if', 'she', \"would've\", 'mentioned', 'the', 'fact', 'that', 'President', 'Trump', 'is', 'doing', 'a', 'lot', 'for', 'the', 'Israeli', 'government', 'and', 'a', 'lot', 'of', 'stuff', 'has', 'been', 'done', 'in', 'a', 'year', 'that', 'is', 'positive', 'and', 'I', 'think', 'we', 'need', 'to', 'support', 'our', 'president', 'but', 'I', 'understand', 'it.\"'], tags=[248]),\n",
       " TaggedDocument(words=['Former', 'U.S.', 'President', 'Bill', 'Clinton', 'on', 'Monday', 'called', 'for', 'the', 'immediate', 'release', 'of', 'two', 'Reuters', 'journalists', 'being', 'held', 'in', 'Myanmar.', '\"A', 'free', 'press', 'is', 'critical', 'to', 'a', 'free', 'society', '-', 'the', 'detention', 'of', 'journalists', 'anywhere', 'is', 'unacceptable.', 'The', 'Reuters', 'journalists', 'being', 'held', 'in', 'Myanmar', 'should', 'be', 'released', 'immediately,\"', 'Clinton', 'said', 'in', 'a', 'Twitter', 'post.', 'Myanmar', 'has', 'accused', 'Reuters', 'reporters', 'Wa', 'Lone,', '31,', 'and', 'Kyaw', 'Soe', 'Oo,', '27,', 'of', 'breaching', 'the', \"country's\", 'Official', 'Secrets', 'Act,', 'a', 'little-used', 'law', 'from', 'colonial', 'rule.', 'They', 'are', 'due', 'to', 'appear', 'in', 'court', 'in', 'the', 'main', 'city', 'of', 'Yangon', 'on', 'Wednesday.', 'It', 'will', 'be', 'their', 'second', 'appearance', 'in', 'court', 'and', 'the', 'prosecutor', 'could', 'request', 'that', 'charges', 'are', 'filed', 'against', 'them.'], tags=[249])]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size = 250, min_count = 2, epochs= 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "#doc2 = nlp(df.content[20])\n",
    "#tokdoc2 = [token for token in nlp(df.content[20])] \n",
    "#for token in doc2:\n",
    "#    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model.infer_vector(contTok[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(196, 0.8644919395446777),\n",
       " (83, 0.817737340927124),\n",
       " (239, 0.7764697074890137),\n",
       " (4, 0.7751911878585815),\n",
       " (128, 0.7693681716918945),\n",
       " (192, 0.7382450699806213),\n",
       " (15, 0.7334905862808228),\n",
       " (110, 0.7207229137420654),\n",
       " (74, 0.7105898857116699),\n",
       " (66, 0.6957017183303833)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_doc = model.docvecs.most_similar([12])\n",
    "sim_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fake'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.type[128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1685</td>\n",
       "      <td>charismanews.com</td>\n",
       "      <td>bias</td>\n",
       "      <td>https://charismanews.com/politics/69268-in-cal...</td>\n",
       "      <td>President Trump, right, with Rep. Robert Light...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>In Calling Out 'Fake News,' Donald Trump Engag...</td>\n",
       "      <td>Hamilton Strategies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['fake news', 'donald trump', 'president trump...</td>\n",
       "      <td>Stephen E. Strang, author of 'God and Donald T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>19639</td>\n",
       "      <td>unz.com</td>\n",
       "      <td>bias</td>\n",
       "      <td>http://www.unz.com/author/anatoly-karlin/topic...</td>\n",
       "      <td>Here I outline one of the core philosophies of...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Anatoly Karlin Archive</td>\n",
       "      <td>Anatoly Karlin, Sayed Hasan, A. Graham, Black ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Digital Philosophy, 9/11 Articles, Race/Crime ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>26150</td>\n",
       "      <td>washingtonsblog.com</td>\n",
       "      <td>bias</td>\n",
       "      <td>http://www.washingtonsblog.com/2012/08/because...</td>\n",
       "      <td>I Was Silent When They Came for You … So There...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>I Will Stand Up For You If You Stand Up For Me</td>\n",
       "      <td>Posted On, Sam Oconnel, President Ron Paul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Podcasts from Boiling Frogs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>29626</td>\n",
       "      <td>vdare.com</td>\n",
       "      <td>bias</td>\n",
       "      <td>http://www.vdare.com/posts/president-trump-sla...</td>\n",
       "      <td>In response to the Democrats closing the gover...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>President Trump Slams Democrats for Protecting...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['vdare.com', 'blog']</td>\n",
       "      <td>In response to the Democrats closing the gover...</td>\n",
       "      <td>Achievement Gap, GOP Share Of The White Vote, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>29669</td>\n",
       "      <td>vdare.com</td>\n",
       "      <td>bias</td>\n",
       "      <td>http://www.vdare.com/posts/inauguration-day-ri...</td>\n",
       "      <td>I got the following press release from the D.C...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Inauguration Day Riot (J20)–New Trials (Of 59)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['vdare.com', 'blog']</td>\n",
       "      <td>I got the following press release from the D.C...</td>\n",
       "      <td>Achievement Gap, GOP Share Of The White Vote, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>30562</td>\n",
       "      <td>americanlookout.com</td>\n",
       "      <td>bias</td>\n",
       "      <td>http://americanlookout.com/media-critic-the-pr...</td>\n",
       "      <td>Howard Kurtz is the media critic for FOX News....</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Media Critic: “The Press Is Falling Into The P...</td>\n",
       "      <td>Mike Lachance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News, Donald Trump, Media, Media Bias, Politics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               domain  type  \\\n",
       "12    1685     charismanews.com  bias   \n",
       "114  19639              unz.com  bias   \n",
       "167  26150  washingtonsblog.com  bias   \n",
       "190  29626            vdare.com  bias   \n",
       "191  29669            vdare.com  bias   \n",
       "196  30562  americanlookout.com  bias   \n",
       "\n",
       "                                                   url  \\\n",
       "12   https://charismanews.com/politics/69268-in-cal...   \n",
       "114  http://www.unz.com/author/anatoly-karlin/topic...   \n",
       "167  http://www.washingtonsblog.com/2012/08/because...   \n",
       "190  http://www.vdare.com/posts/president-trump-sla...   \n",
       "191  http://www.vdare.com/posts/inauguration-day-ri...   \n",
       "196  http://americanlookout.com/media-critic-the-pr...   \n",
       "\n",
       "                                               content  \\\n",
       "12   President Trump, right, with Rep. Robert Light...   \n",
       "114  Here I outline one of the core philosophies of...   \n",
       "167  I Was Silent When They Came for You … So There...   \n",
       "190  In response to the Democrats closing the gover...   \n",
       "191  I got the following press release from the D.C...   \n",
       "196  Howard Kurtz is the media critic for FOX News....   \n",
       "\n",
       "                     scraped_at                 inserted_at  \\\n",
       "12   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "114  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "167  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "190  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "191  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "196  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                     updated_at  \\\n",
       "12   2018-02-02 01:19:41.756664   \n",
       "114  2018-02-02 01:19:41.756664   \n",
       "167  2018-02-02 01:19:41.756664   \n",
       "190  2018-02-02 01:19:41.756664   \n",
       "191  2018-02-02 01:19:41.756664   \n",
       "196  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                 title  \\\n",
       "12   In Calling Out 'Fake News,' Donald Trump Engag...   \n",
       "114                             Anatoly Karlin Archive   \n",
       "167     I Will Stand Up For You If You Stand Up For Me   \n",
       "190  President Trump Slams Democrats for Protecting...   \n",
       "191  Inauguration Day Riot (J20)–New Trials (Of 59)...   \n",
       "196  Media Critic: “The Press Is Falling Into The P...   \n",
       "\n",
       "                                               authors  keywords  \\\n",
       "12                                 Hamilton Strategies       NaN   \n",
       "114  Anatoly Karlin, Sayed Hasan, A. Graham, Black ...       NaN   \n",
       "167         Posted On, Sam Oconnel, President Ron Paul       NaN   \n",
       "190                                                NaN       NaN   \n",
       "191                                                NaN       NaN   \n",
       "196                                      Mike Lachance       NaN   \n",
       "\n",
       "                                         meta_keywords  \\\n",
       "12   ['fake news', 'donald trump', 'president trump...   \n",
       "114                                               ['']   \n",
       "167                                               ['']   \n",
       "190                              ['vdare.com', 'blog']   \n",
       "191                              ['vdare.com', 'blog']   \n",
       "196                                               ['']   \n",
       "\n",
       "                                      meta_description  \\\n",
       "12   Stephen E. Strang, author of 'God and Donald T...   \n",
       "114                                                NaN   \n",
       "167                                                NaN   \n",
       "190  In response to the Democrats closing the gover...   \n",
       "191  I got the following press release from the D.C...   \n",
       "196                                                NaN   \n",
       "\n",
       "                                                  tags  summary  type_id  \n",
       "12                                                 NaN      NaN        0  \n",
       "114  Digital Philosophy, 9/11 Articles, Race/Crime ...      NaN        0  \n",
       "167                        Podcasts from Boiling Frogs      NaN        0  \n",
       "190  Achievement Gap, GOP Share Of The White Vote, ...      NaN        0  \n",
       "191  Achievement Gap, GOP Share Of The White Vote, ...      NaN        0  \n",
       "196    News, Donald Trump, Media, Media Bias, Politics      NaN        0  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.type == 'bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-6669bda1078e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontTok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, indexer)\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1729\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m                 \u001b[0mall_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "print(model.docvecs.most_similar(contTok[80]))\n",
    "model.docvecs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offers are getting into the six-figure range in pre-bidding for the NamesCon live auction\\n\\nHeadline: Bitcoin & Blockchain Searches Exceed Trump! Blockchain Stocks Are Next!\\n\\nIn a few weeks domain name investors from around the world will converge in Las Vegas for NamesCon, the largest event in the domain industry. One of the highlights of the conference is the live domain auction hosted by ROTD and pre-bidding has already started on NameJet, and like the title says, there are already six-figure bids.\\n\\nIf you’ve never been to a live domain auction I can tell you – it’s a rush that’s hard to explain, at least for a domain geek like me. Monte Cahn, one of the most respected domain industry veterans will be running the auction again this year and from what I’ve seen over the years, when Monte’s involved, a lot of the truly premium domains come out.\\n\\nSo I’m not incredibly surprised that we’re already starting to see six-figure bids come out in pre-bidding more than two weeks before the auction. The two domains with six-figure bids are:\\n\\nOther notable domains with strong interest in the pre-bidding phase are: pen.com, profile.com, shock.com, price.com, stop.com, cork.com, and the list goes on. I think it’s safe to say that some of the best one-word .COMs out there are going to hit the open market during the live auction at NamesCon this year.\\n\\nGiven that last year even two-word .COMs like MyWorld.com broke the $1M mark I think it’s safe to say this is likely going to be one of the most exciting live auctions yet. As usual I’ve submitted a handful of names to the auction and I’ll be interested to see which ones make the cut.\\n\\nOne strange thing I’ve learned about myself is that for some reason, when I’m in a live auction I end up with a rum and coke in my hand. Honestly I pretty much never drink rum and coke, it’s one of my least favorite drinks, but years ago I started drinking them during live auctions and now it’s become a bit of a tradition. If any of my readers are going to be at NamesCon, feel free to join me for one, maybe you can start a new tradition yourself!\\n\\nYou can see the full list of domains in pre-bidding for the NamesCon auction here.\\n\\nSource: http://morganlinton.com/offers-are-getting-into-the-six-figure-range-in-pre-bidding-for-the-namescon-live-auction/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
