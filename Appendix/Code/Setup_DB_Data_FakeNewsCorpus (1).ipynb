{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "# libraries we might not need\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the following code\n",
    "to use the whole document you only need one file specified by filepath for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>content_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>11593</td>\n",
       "      <td>washingtonexaminer.com</td>\n",
       "      <td>political</td>\n",
       "      <td>http://www.washingtonexaminer.com/us-support-f...</td>\n",
       "      <td>The United States has slashed its contribution...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>US support for UNRWA has gone on long enough</td>\n",
       "      <td>Michael Rubin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Blog Contributors', 'Refugees', 'Palestinian...</td>\n",
       "      <td>When the U.N. created UNRWA, it was meant to b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>39558</td>\n",
       "      <td>www.newsmax.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.newsmax.com/newsfront/bill-clinton...</td>\n",
       "      <td>Former U.S. President Bill Clinton on Monday c...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Bill Clinton Calls for Release of Reuters Jour...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['bill clinton', 'myanmar', 'calls', 'release'...</td>\n",
       "      <td>Former U.S. President Bill Clinton Calls for R...</td>\n",
       "      <td>Donald Trump, Russia, Trump Administration, Gu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>37351</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/alternative/2018/01/t...</td>\n",
       "      <td>The Real Story Behind Marijuana Prohibition\\r\\...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>The Real Story Behind Marijuana Prohibition</td>\n",
       "      <td>The Daily Sheeple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>25191</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/health/2017/04/audio-...</td>\n",
       "      <td>Audio Sensors Market Analysis by Current Indus...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Audio Sensors Market Analysis by Current Indus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>15468</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/science-and-technolog...</td>\n",
       "      <td>Headline: Bitcoin &amp; Blockchain Searches Exceed...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Yawning — Why Is It So Contagious and Why Shou...</td>\n",
       "      <td>Alton Parrish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1     id                  domain       type  \\\n",
       "67             67  11593  washingtonexaminer.com  political   \n",
       "249           249  39558         www.newsmax.com        NaN   \n",
       "230           230  37351       beforeitsnews.com       fake   \n",
       "161           161  25191       beforeitsnews.com       fake   \n",
       "91             91  15468       beforeitsnews.com       fake   \n",
       "\n",
       "                                                   url  \\\n",
       "67   http://www.washingtonexaminer.com/us-support-f...   \n",
       "249  https://www.newsmax.com/newsfront/bill-clinton...   \n",
       "230  http://beforeitsnews.com/alternative/2018/01/t...   \n",
       "161  http://beforeitsnews.com/health/2017/04/audio-...   \n",
       "91   http://beforeitsnews.com/science-and-technolog...   \n",
       "\n",
       "                                               content  \\\n",
       "67   The United States has slashed its contribution...   \n",
       "249  Former U.S. President Bill Clinton on Monday c...   \n",
       "230  The Real Story Behind Marijuana Prohibition\\r\\...   \n",
       "161  Audio Sensors Market Analysis by Current Indus...   \n",
       "91   Headline: Bitcoin & Blockchain Searches Exceed...   \n",
       "\n",
       "                     scraped_at                 inserted_at  \\\n",
       "67   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "249  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "230  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "161  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "91   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                     updated_at  \\\n",
       "67   2018-02-02 01:19:41.756664   \n",
       "249  2018-02-02 01:19:41.756664   \n",
       "230  2018-02-02 01:19:41.756664   \n",
       "161  2018-02-02 01:19:41.756664   \n",
       "91   2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                 title            authors  \\\n",
       "67        US support for UNRWA has gone on long enough      Michael Rubin   \n",
       "249  Bill Clinton Calls for Release of Reuters Jour...                NaN   \n",
       "230        The Real Story Behind Marijuana Prohibition  The Daily Sheeple   \n",
       "161  Audio Sensors Market Analysis by Current Indus...                NaN   \n",
       "91   Yawning — Why Is It So Contagious and Why Shou...      Alton Parrish   \n",
       "\n",
       "     keywords                                      meta_keywords  \\\n",
       "67        NaN  ['Blog Contributors', 'Refugees', 'Palestinian...   \n",
       "249       NaN  ['bill clinton', 'myanmar', 'calls', 'release'...   \n",
       "230       NaN                                               ['']   \n",
       "161       NaN                                               ['']   \n",
       "91        NaN                                               ['']   \n",
       "\n",
       "                                      meta_description  \\\n",
       "67   When the U.N. created UNRWA, it was meant to b...   \n",
       "249  Former U.S. President Bill Clinton Calls for R...   \n",
       "230                                                NaN   \n",
       "161                                                NaN   \n",
       "91                                                 NaN   \n",
       "\n",
       "                                                  tags  summary content_tokens  \n",
       "67                                                 NaN      NaN             []  \n",
       "249  Donald Trump, Russia, Trump Administration, Gu...      NaN             []  \n",
       "230                                                NaN      NaN             []  \n",
       "161                                                NaN      NaN             []  \n",
       "91                                                 NaN      NaN             []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# imports a random sample of size s from csv-file as a pandas dataframe\n",
    "# pandas using python 3.X uses utf-8 encoding\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random)\n",
    "filepath = '../Data_sample/FakeNewsCorpus_250.csv'\n",
    "#filepath = 'news_sample.csv' # <- overwrite for setup\n",
    "s = 250                    # desired sample size(seems to have slack ie. not exact)\n",
    "seed = 1                     # seed used by Pseudorandom number generator\n",
    "\n",
    "# init dataframe with specified values\n",
    "df = pd.read_csv(filepath, index_col = [0]).sample(n=s, random_state=seed)\n",
    "\n",
    "# visual output\n",
    "#print(df.shape, '<- size of dataframe \\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = pd.to_numeric(df['id'], errors = 'coerce', downcast = 'integer')\n",
    "df.drop_duplicates(subset = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['id']).set_index('id')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning for values out of bounds of DataBase requirements etc.\n",
    "df.index = df.index.astype(int)\n",
    "longAuthors = df[df['authors'].str.len() > 255].index\n",
    "df.drop(longAuthors, inplace = True)\n",
    "longTags = df[df['tags'].str.len() > 1000].drop_duplicates(subset = 'tags', keep = 'first').index\n",
    "df.drop(longTags, inplace = True)\n",
    "longMetaD = df[df['meta_description'].str.len() > 10000].index\n",
    "df.drop(longMetaD, inplace = True)\n",
    "df['authors'] = df['authors'].replace(np.nan, 'NoAuthor', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regexEmail = r\"[a-zA-Z_-]+@[a-zA-Z_-]+(\\.[a-zA-Z]{2,4}){1,3}\"\n",
    "df.content = df.content.replace(to_replace=regexEmail, value='<EMAIL>', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regexURL= r\"(?:https?:\\/\\/)?(?:www\\.)?([^@\\s]+\\.[a-zA-Z]{2,4})[^\\s]*\"\n",
    "df.content = df.content.replace(to_replace=regexURL, value='<URL>', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexDoubleSpace = r\"(\\s{2,})|\\n\"\n",
    "df.content = df.content.replace(to_replace=regexDoubleSpace, value=' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexDate = r\"(((19[7-9]\\d|20\\d{2})|(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|(nov|dec)(?:ember)?)|(([12][0-9])|(3[01])|(0?[1-9])))[\\/. \\-,\\n]){2,3}\"\n",
    "df.content = df.content.replace(to_replace=regexDate, value='<DATE>', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regexNum = r\"(\\s)\\$?(?:[\\d,.-])+\"\n",
    "df.content = df.content.replace(to_replace=regexNum, value='<NUM>', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data-tables: [name]-uniq / relational-tables: [name]_in\n",
    "creating csv-files for database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where to save all csv-files\n",
    "path = '../Data_git_ignore/clean_csv/'\n",
    "\n",
    "# create temporary dataframe and use article id as index \n",
    "out_df = pd.DataFrame({'id':df.index})\n",
    "out_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### types_uniq - data-table ###\n",
    "type_array = df.type.unique() # get array of unique types\n",
    "type_df = pd.DataFrame({'id': np.arange(type_array.size), 'name':type_array})\n",
    "\n",
    "# write file and free memory\n",
    "type_df.to_csv(path + 'type_clean.csv', index=False, header=True)\n",
    "#del type_array\n",
    "#del type_df # tmp delete later\n",
    "\n",
    "# create dict with type_name as key - [swap type with type_id]\n",
    "type_name_as_key_df = type_df.set_index('name')\n",
    "type_dict = type_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace type with tag id and create new column\n",
    "type_id = np.array([type_dict[key] for key in df['type'].to_numpy()])\n",
    "df['type_id'] =type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tags_uniq - data-table ###\n",
    "\n",
    "# creates list of list but formaly it is a pd.series of lists\n",
    "tags_series_of_lists = df.tags.dropna().str.split(', ') # -> ', ' not ','\n",
    "\n",
    "if not 'tags' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'tags', value = tags_series_of_lists)\n",
    "\n",
    "# flattern tags_series_of_lists to a set(ie. unique values only)\n",
    "tags_list = list(set([item for sublist in tags_series_of_lists for item in sublist]))\n",
    "\n",
    "# create dataframe\n",
    "tags_df = pd.DataFrame({'id': np.arange(len(tags_list)), 'name':tags_list})\n",
    "\n",
    "# write file and free memory\n",
    "tags_df.to_csv(path + 'tags_clean.csv', index=False, header=True)\n",
    "del tags_series_of_lists\n",
    "del tags_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### tags_in - relational-table ###\n",
    "\n",
    "# get all pairs of article_id and tags in a article (for all articles)\n",
    "articles_id_tags_name_pairs_df = out_df.tags.dropna().explode().drop_duplicates(keep = 'first')\n",
    "\n",
    "# split tags_name and articles_id\n",
    "articles_id_array = articles_id_tags_name_pairs_df.index.to_numpy()\n",
    "tags_name_array = articles_id_tags_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap tags with tags_id]\n",
    "tags_name_as_key_df = tags_df.set_index('name')\n",
    "tags_dict = tags_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace tags with tag id\n",
    "tags_id = np.array([tags_dict[key] for key in tags_name_array])\n",
    "\n",
    "# create dataframe\n",
    "tags_in_df = pd.DataFrame(data=articles_id_array, index=tags_id, columns=['article_id'])\n",
    "tags_in_df.index.name='tags_id'\n",
    "\n",
    "# write file and free memory\n",
    "tags_in_df.to_csv(path + 'tags_in.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### authors-uniq - data-table ###\n",
    "\n",
    "# creates list of list but formaly it is a pd.series of lists\n",
    "authors_series_of_lists = df.authors.str.split(',') # -> ',' not ', '\n",
    "\n",
    "if not 'authors' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'authors', value = authors_series_of_lists)\n",
    "\n",
    "# flattern authors_series_of_lists to a set(ie. unique values only)\n",
    "authors_list = list(set([item for sublist in authors_series_of_lists for item in sublist]))\n",
    "\n",
    "# create dataframe\n",
    "authors_df = pd.DataFrame({'id': np.arange(len(authors_list)), 'name':authors_list})\n",
    "\n",
    "# write file and free memory\n",
    "authors_df.to_csv(path + 'authors_clean.csv', index=False, header=True)\n",
    "#del authors_series_of_lists\n",
    "#del authors_list\n",
    "#del authors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors_in - relational-table ###\n",
    "#for i in out_df.index:\n",
    "    #out_df.author[i] = list(set(out_df.authors[i]))\n",
    "# get all pairs of article_id and authors in a article (for all articles)\n",
    "articles_id_authors_name_pairs_df = out_df.authors.dropna().explode()\n",
    "\n",
    "# split authors_name and articles_id\n",
    "articles_id_array = articles_id_authors_name_pairs_df.index.to_numpy()\n",
    "authors_name_array = articles_id_authors_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap authors with authors_id]\n",
    "authors_name_as_key_df = authors_df.set_index('name')\n",
    "authors_dict = authors_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace authors with tag id\n",
    "authors_id = np.array([authors_dict[key] for key in authors_name_array])\n",
    "\n",
    "# create dataframe\n",
    "authors_in_df = pd.DataFrame(data=articles_id_array, index=authors_id, columns=['article_id'])\n",
    "authors_in_df.index.name='authors_id'\n",
    "\n",
    "# write file and free memory\n",
    "authors_in_df.to_csv(path + 'authors_in.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### domains-uniq - data-table ###\n",
    "domain_array = df.domain.unique() # get array of unique domains\n",
    "domain_df = pd.DataFrame({'id': np.arange(domain_array.size), 'name':domain_array})\n",
    "\n",
    "# write file and free memory\n",
    "domain_df.to_csv(path + 'domain_name_clean.csv', index=False, header=True)\n",
    "#del domain_array\n",
    "#del domain_df\n",
    "\n",
    "# create dict with domain_name as key - [swap domain with domain_id]\n",
    "domain_name_as_key_df = domain_df.set_index('name')\n",
    "domain_dict = domain_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace domain with tag id and create new column\n",
    "domain_id = np.array([domain_dict[key] for key in df['domain'].to_numpy()])\n",
    "df['domain_id'] =domain_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta_keywords_uniq - data-table ###\n",
    "\n",
    "# use regex to remove string-padding\n",
    "regex = r\" *['\\\"\\[\\]]+\"\n",
    "meta_keywords_series = df.meta_keywords.replace(to_replace=regex, value='', regex=True).str.split(',')\n",
    "#meta_keywords_series = meta_keywords_series.replace(r'', np.NaN)\n",
    "\n",
    "if not 'meta_keywords' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'meta_keywords', value = meta_keywords_series)\n",
    "\n",
    "# create array of unique\n",
    "meta_keywords_set = meta_keywords_series.explode().unique()\n",
    "\n",
    "# create dataframe\n",
    "meta_keywords_df = pd.DataFrame({'id': np.arange(len(meta_keywords_set)), 'name':meta_keywords_set})\n",
    "\n",
    "# write file and free memory\n",
    "meta_keywords_df.to_csv(path + 'meta_keywords_clean.csv', index=False, header=True)\n",
    "#del meta_keywords_series\n",
    "#del meta_keywords_set\n",
    "#del meta_keywords_list\n",
    "#del meta_keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta_keywords_in - relational-table ###\n",
    "\n",
    "# get all pairs of article_id and meta_keywords in a article (for all articles)\n",
    "articles_id_meta_keywords_name_pairs_df = out_df.meta_keywords.dropna().explode()\n",
    "\n",
    "# split meta_keywords_name and articles_id\n",
    "articles_id_array = articles_id_meta_keywords_name_pairs_df.index.to_numpy()\n",
    "meta_keywords_name_array = articles_id_meta_keywords_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap meta_keywords with meta_keywords_id]\n",
    "meta_keywords_name_as_key_df = meta_keywords_df.set_index('name')\n",
    "meta_keywords_dict = meta_keywords_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace meta_keywords with tag id\n",
    "meta_keywords_id = np.array([meta_keywords_dict[key] for key in meta_keywords_name_array])\n",
    "\n",
    "# create dataframe\n",
    "meta_keywords_in_df = pd.DataFrame(data=articles_id_array, index=meta_keywords_id, columns=['article_id'])\n",
    "\n",
    "meta_keywords_in_df.index.name='meta_keywords_id'\n",
    "meta_keywords_in_df.reset_index(inplace = True)\n",
    "meta_keywords_in_df.drop_duplicates(subset = ['meta_keywords_id', 'article_id'], keep = 'first', inplace = True)\n",
    "# write file and free memory\n",
    "meta_keywords_in_df.to_csv(path + 'meta_keywords_in.csv', index=False, header=True)\n",
    "#meta_keywords_in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### article clean ###\n",
    "\n",
    "df[['domain_id', 'type_id', \"url\", \"content\", \"title\", \"meta_description\", \"scraped_at\",  \"updated_at\", \"inserted_at\"]].to_csv(path + 'article_clean.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48770</th>\n",
       "      <td>this morning s key headlines from generational...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96003</th>\n",
       "      <td>daily app fix april &lt;number&gt; th &lt;number&gt; risk&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16182</th>\n",
       "      <td>consciousness and unified physics are the keys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89922</th>\n",
       "      <td>do something&lt;NUM&gt; anything headline : bitcoin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106668</th>\n",
       "      <td>from conservapedia transfer pricing is the set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22752</th>\n",
       "      <td>americans sugar intake same as &lt;number&gt; years ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95890</th>\n",
       "      <td>torture news roundup : cheney ' s screwed top ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103383</th>\n",
       "      <td>this article is within the scope of wikiprojec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93431</th>\n",
       "      <td>recommended resources counterthink cartoons ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61029</th>\n",
       "      <td>missouri state troopers in riot gear stand in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content\n",
       "id                                                       \n",
       "48770   this morning s key headlines from generational...\n",
       "96003   daily app fix april <number> th <number> risk<...\n",
       "16182   consciousness and unified physics are the keys...\n",
       "89922   do something<NUM> anything headline : bitcoin ...\n",
       "106668  from conservapedia transfer pricing is the set...\n",
       "...                                                   ...\n",
       "22752   americans sugar intake same as <number> years ...\n",
       "95890   torture news roundup : cheney ' s screwed top ...\n",
       "103383  this article is within the scope of wikiprojec...\n",
       "93431   recommended resources counterthink cartoons ar...\n",
       "61029   missouri state troopers in riot gear stand in ...\n",
       "\n",
       "[495 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['type_id', 'content']].to_csv('d:/Personal/Desktop/downloads-tmp/' + 'learning-clean.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_description\n",
      "updated_at\n",
      "content\n",
      "authors\n",
      "scraped_at\n",
      "url\n",
      "meta_keywords\n",
      "summary\n",
      "domain\n",
      "title\n",
      "keywords\n",
      "tags\n",
      "type\n",
      "inserted_at\n",
      "source\n",
      "type_id\n",
      "domain_id\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns: \n",
    "    print(col) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
