{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "listContentRows = []\n",
    "\n",
    "with open('../news_sample.csv', encoding=\"utf8\", newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        listContentRows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uniq tags\n",
    "uniqTypes = []\n",
    "\n",
    "with open('type_clean.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"name\"])\n",
    "    for i in range (len(listContentRows)):\n",
    "        if (not (listContentRows[i]['type'] in uniqTypes)):\n",
    "            uniqTypes.append(listContentRows[i]['type'])\n",
    "            writer.writerow([len(uniqTypes) - 1, listContentRows[i]['type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uniq tags\n",
    "uniqTags = []\n",
    "\n",
    "for article in listContentRows:\n",
    "    tagsList = article[\"tags\"].split(\",\")\n",
    "    \n",
    "    for tags in tagsList:\n",
    "        if (tags != \"\"):\n",
    "            if (not (tags in uniqTags)):\n",
    "                uniqTags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "with open('tags_clean.csv', 'w', encoding=\"utf8\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"name\"])\n",
    "    for i in uniqTags:\n",
    "        writer.writerow([counter, i])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uniq authors\n",
    "uniqAuthors = []\n",
    "\n",
    "for article in listContentRows:\n",
    "    authorList = article[\"authors\"].split(\",\")\n",
    "    \n",
    "    for author in authorList:\n",
    "        if (author != \"\"):\n",
    "            if (not (author in uniqAuthors)):\n",
    "                uniqAuthors.append(author.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('authors_clean.csv', 'w', encoding=\"utf8\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"name\"])\n",
    "    counter = 0\n",
    "    for i in uniqAuthors:\n",
    "        writer.writerow([counter,i])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqDomains = []\n",
    "\n",
    "with open('domain_name_clean.csv', 'w', encoding=\"utf8\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\",\"name\"])\n",
    "    for i in range (len(listContentRows)):\n",
    "        if (not (listContentRows[i]['domain'] in uniqDomains)):\n",
    "            writer.writerow([len(uniqDomains),listContentRows[i]['domain']])\n",
    "            uniqDomains.append(listContentRows[i]['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_meta_keyword = []\n",
    "\n",
    "with open('meta_keywords_clean.csv', 'w', encoding=\"utf8\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\",\"name\"])\n",
    "    for i in listContentRows:\n",
    "        for meta_keyword in i[\"meta_keywords\"].split(\",\"):\n",
    "            if (meta_keyword == \"['']\"):\n",
    "                continue\n",
    "                \n",
    "            meta_keyword = re.sub(\"'|\\[|\\]\", \"\", meta_keyword.strip())\n",
    "            if (not (meta_keyword in uniq_meta_keyword)):\n",
    "                writer.writerow([len(uniq_meta_keyword),meta_keyword])\n",
    "                uniq_meta_keyword.append(meta_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### relational tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdFromName (name, dataset):\n",
    "    with open(dataset + '.csv', 'r', encoding=\"utf8\", newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for item in reader:\n",
    "            if (item['name'].strip() == name):\n",
    "                return int(item['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tags_in.csv', 'w', encoding=\"utf8\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"name\"])\n",
    "    for i in listContentRows:\n",
    "        for tag in i[\"tags\"].split(','):\n",
    "            if (tag == \"\"):\n",
    "                continue\n",
    "            tag = tag.strip()\n",
    "            writer.writerow([int(i[\"id\"]),int(getIdFromName(tag, 'tags_clean'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('author_in.csv', 'w', encoding=\"utf8\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"name\"])\n",
    "    for i in listContentRows:\n",
    "        for author in i[\"authors\"].split(','):\n",
    "            if (author == \"\"):\n",
    "                continue\n",
    "            author = author.strip()\n",
    "            writer.writerow([int(i[\"id\"]),int(getIdFromName(author, 'authors_clean'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meta_keywords_in.csv', 'w', encoding=\"utf8\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"name\"])\n",
    "    for i in listContentRows:\n",
    "        currentKeyWords = []\n",
    "        \n",
    "        for meta_keywords in i[\"meta_keywords\"].split(','):  \n",
    "            meta_keywords = re.sub(\"'|\\[|\\]\", \"\", meta_keywords.strip())\n",
    "\n",
    "            if (meta_keywords == \"['']\" or meta_keywords.strip() in currentKeyWords):\n",
    "                continue\n",
    "            currentKeyWords.append(meta_keywords.strip())\n",
    "            \n",
    "            writer.writerow([int(i[\"id\"]),int(getIdFromName(meta_keywords, 'meta_keywords_clean'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('article_clean.csv', 'w', encoding='utf8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"domain_id\", \"type_id\", \"url\", \"content\", \"title\", \"meta_description\", \"scraped_at\",  \"updated_at\", \"inserted_at\"])\n",
    "    for i in range (len(listContentRows)):\n",
    "        \n",
    "        writer.writerow([int(listContentRows[i]['id']), getIdFromName(listContentRows[i]['domain'], \"domain_name_clean\"), getIdFromName(listContentRows[i]['type'], \"type_clean\"), listContentRows[i]['url'], listContentRows[i]['content'], listContentRows[i]['title'], listContentRows[i]['meta_description'], listContentRows[i][\"scraped_at\"], listContentRows[i][\"updated_at\"], listContentRows[i][\"inserted_at\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "listContentRows = []\n",
    "\n",
    "with open('../news_sample.csv', encoding=\"utf8\", newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        listContentRows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uniq tags\n",
    "uniqTypes = []\n",
    "\n",
    "with open('type_clean.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"name\"])\n",
    "    for i in range (len(listContentRows)):\n",
    "        if (not (listContentRows[i]['type'] in uniqTypes)):\n",
    "            uniqTypes.append(listContentRows[i]['type'])\n",
    "            writer.writerow([len(uniqTypes) - 1, listContentRows[i]['type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never Hike Alone: A Friday the 13th Fan Film USA | 2017 | 54 min A fan tribute to Friday the 13th, Never Hike Alone follows an adventure\n",
      "\n",
      "Never Hike Alone: A Friday the 13th Fan Film USA | 2017 | 54 min\n",
      "\n",
      "\n",
      "\n",
      "A fan tribute to Friday the 13th, Never Hike Alone follows an adventure blogger, Kyle McLeod (Andrew Leighty), who uncovers the long lost remains of Camp Crystal Lake while on a solo backpacking trip.\n",
      "\n",
      "Ignoring the campfire tales from his childhood, Kyle's search turns deadly when he makes the grave mistake of crossing the path of Camp Blood’s legendary mass murderer, Jason Voorhees.\n",
      "\n",
      "Stranded in the forest with Jason on his heels, Kyle must push his survival skills to their limits if he hopes to survive the night.\n",
      "\n",
      "Otherwise, he'll wind up as another lost victim of the cursed camp.\n",
      "\n",
      "\n",
      "\n",
      "DIRECTED BY\n",
      "\n",
      "VINCENTE DISANTI\n",
      "\n",
      "\n",
      "\n",
      "SCREENPLAY BY\n",
      "\n",
      "VINCENTE DISANTI and NATHAN MCLEOD\n",
      "\n",
      "\n",
      "\n",
      "BASED ON CHARACTERS CREATED BY\n",
      "\n",
      "SEAN CUNNINGHAM\n",
      "\n",
      "VICTOR MILLER\n",
      "\n",
      "RON KURTZ\n",
      "\n",
      "\n",
      "\n",
      "STARRING:\n",
      "\n",
      "ANDREW LEIGHTY as KYLE MCLEOD\n",
      "\n",
      "VINCENTE DISANTI as JASON VOORHEES\n",
      "\n",
      "\n",
      "\n",
      "EXECUTIVE PRODUCERS\n",
      "\n",
      "BARRY JAY STITCH\n",
      "\n",
      "CAROLYN GAIR\n",
      "\n",
      "SARAH DISANTI\n",
      "\n",
      "\n",
      "\n",
      "PRODUCERS\n",
      "\n",
      "KYLE KLEIN\n",
      "\n",
      "DANIEL EVANS\n",
      "\n",
      "MATTHEW BARRETT\n",
      "\n",
      "\n",
      "\n",
      "CO-PRODUCERS\n",
      "\n",
      "STEVEN SQUILLANTE\n",
      "\n",
      "CHRISTOPHER THELLAS\n",
      "\n",
      "ALEXANDER THELLAS\n",
      "\n",
      "\n",
      "\n",
      "ASSOCIATE PRODUCERS\n",
      "\n",
      "CODY CAMERON\n",
      "\n",
      "CHRISTIAN ASCENCIO\n",
      "\n",
      "LYDIA ASCENCIO\n",
      "\n",
      "LYNN HOBSON\n",
      "\n",
      "RENE RIVAS\n",
      "\n",
      "MIKE WHELAN\n",
      "\n",
      "\n",
      "\n",
      "DIRECTORS OF PHOTOGRAPHY\n",
      "\n",
      "CHRISTOPHER THELLAS\n",
      "\n",
      "J.D.\n",
      "\n",
      "MARTZ\n",
      "\n",
      "\n",
      "\n",
      "PRODUCTION DESIGNER\n",
      "\n",
      "BRITTANY PORTER\n",
      "\n",
      "\n",
      "\n",
      "SPECIAL MAKE UP EFFECTS\n",
      "\n",
      "KELSEY BERK\n",
      "\n",
      "\n",
      "\n",
      "WARDROBE\n",
      "\n",
      "STEPHEN BOYD-MORALES\n",
      "\n",
      "\n",
      "\n",
      "STUNTS\n",
      "\n",
      "JESSICA BENNETT\n",
      "\n",
      "BRYAN FORREST as JASON VOORHEES\n",
      "\n",
      "DONNY NICHOLS as KYLE MCLEOD\n",
      "\n",
      "\n",
      "\n",
      "FILM EDITING BY\n",
      "\n",
      "LAWRENCE GAN\n",
      "\n",
      "SARAH DISANTI\n",
      "\n",
      "\n",
      "\n",
      "UNIT STILLS PHOTOGRAPHY\n",
      "\n",
      "ASHLY COVINGTON\n",
      "\n",
      "\n",
      "\n",
      "MUSIC BY\n",
      "\n",
      "RYAN PEREZ-DAPLE\n",
      "\n",
      "\n",
      "\n",
      "“Run Like Hell”\n",
      "\n",
      "Written by: Vincente DiSanti, Jake Gonsalves, Lenny Machado\n",
      "\n",
      "Performed by: Gonsalves, Machado\n",
      "\n",
      "Recorded at: Elm Street Studios, New Bedford, MA 2017\n",
      "\n",
      "Appears Courtesy of: Damn Enchiladas\n",
      "\n",
      "\n",
      "\n",
      "“Another Soul”\n",
      "\n",
      "Written and Performed by: Trevor Vaughan\n",
      "\n",
      "Appears on: Finally Salao Mix Tape\n",
      "\n",
      "Recorded at: The Colosseum, New Bedford, MA 2012\n",
      "\n",
      "Appears Courtesy of: Trevor Vaughan & Mass Glory Records\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for additional cast and crew visit: www.imdb.com/title/tt5719786/\n",
      "\n",
      "\n",
      "\n",
      "www.wompstompfilms.com\n",
      "\n",
      "\n",
      "\n",
      "DISCLAIMER\n",
      "\n",
      "\n",
      "\n",
      "THIS PRODUCTION IS THE EFFORT OF WOMP STOMP FILMS, LLC AND ITS VOLUNTEER ASSOCIATES.\n",
      "\n",
      "NEVER HIKE ALONE IS A PRODUCT OF FAN FICTION AND IN NO WAY AFFILIATED WITH, OR A REPRESENTATIVE OF THE VIACOM/PARAMOUNT FILM COMPANY, WARNER BROTHERS, NEW LINE CINEMA, HORROR INC, THE FRIDAY THE 13TH FILM PROPERTY, OR ITS CHARACTERS.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THIS PROJECT WAS CREATED BY FANS, FOR FANS, TO SHOW APPRECIATION FOR THE FRIDAY THE 13th FRANCHISE.\n",
      "\n",
      "AS OF FRIDAY THE 13th OCTOBER 2017, THIS PROJECT WILL BE MADE FREE AND AVAILABLE TO ALL WHO CARE TO WATCH IT.\n",
      "\n",
      "NO PROFIT SHALL BE MADE OFF ITS VIEWING.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THANK YOU TO ANY AND ALL WHO ASSISTED IN BRINGING THE PROJECT TO LIFE.\n",
      "\n",
      "WOMP STOMP FILMS IS FOREVER GRATEFUL FOR YOUR TIME AND DEDICATION TO THE PROJECT.\n",
      "\n",
      "\n",
      "\n",
      "THANK YOU FOR WATCHING.\n"
     ]
    }
   ],
   "source": [
    "with open('article_clean.csv', 'r', encoding='utf8', newline='') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if (row[\"id\"] == \"700\"):\n",
    "            print(row[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
