{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "# libraries we might not need\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the following code\n",
    "to use the whole document you only need one file specified by filepath for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 2.94 s\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                       meta_description  \\\n3660  World View: Both Greece and European Leaders S...   \n7278                                                NaN   \n4317                                                NaN   \n1932                                                NaN   \n5321                                                NaN   \n\n                      updated_at  \\\n3660  2018-02-02 01:19:41.756664   \n7278  2018-02-02 01:19:41.756664   \n4317  2018-02-02 01:19:41.756664   \n1932  2018-02-02 01:19:41.756664   \n5321  2018-02-02 01:19:41.756664   \n\n                                                content  \\\n3660  this morning s key headlines from generational...   \n7278  daily app fix april <number> th <number> risk ...   \n4317  consciousness and unified physics are the keys...   \n1932  do something , anything headline : bitcoin blo...   \n5321  from conservapedia transfer pricing is the set...   \n\n                            authors                  scraped_at      id  \\\n3660                John J. Xenakis  2018-01-25 20:13:50.426130   48770   \n7278                            NaN  2018-01-25 20:13:50.426130   96003   \n4317                   Waking Times  2018-01-25 16:17:44.789555   16182   \n1932  Humble Student Of The Markets  2018-01-25 20:13:50.426130   89922   \n5321                            NaN  2018-01-25 20:13:50.426130  106668   \n\n                                                    url  \\\n3660  http://www.breitbart.com/national-security/201...   \n7278  http://beforeitsnews.com/science-and-technolog...   \n4317  http://beforeitsnews.com/alternative/2016/07/c...   \n1932  http://beforeitsnews.com/financial-markets/201...   \n5321      http://www.conservapedia.com/Transfer_Pricing   \n\n                                          meta_keywords  summary  \\\n3660  ['Alexis Tsipras', 'Angela Merkel', 'Britain',...      NaN   \n7278                                               ['']      NaN   \n4317                                               ['']      NaN   \n1932                                               ['']      NaN   \n5321                                               ['']      NaN   \n\n                 domain                                              title  \\\n3660      breitbart.com  World View: Both Greece and European Leaders S...   \n7278  beforeitsnews.com  Daily App Fix April 20th 2013 – RISK, Connect ...   \n4317  beforeitsnews.com  Consciousness and Unified Physics are the Keys...   \n1932  beforeitsnews.com                            Do something, anything!   \n5321  conservapedia.com                                   Transfer Pricing   \n\n      keywords                                               tags       type  \\\n3660       NaN  Tunisia, Generational Dynamics, Britain, Alexi...  political   \n7278       NaN                                                NaN       fake   \n4317       NaN                                                NaN       fake   \n1932       NaN                                                NaN       fake   \n5321       NaN                                                NaN       bias   \n\n                     inserted_at  source  \n3660  2018-02-02 01:19:41.756632     NaN  \n7278  2018-02-02 01:19:41.756632     NaN  \n4317  2018-02-02 01:19:41.756632     NaN  \n1932  2018-02-02 01:19:41.756632     NaN  \n5321  2018-02-02 01:19:41.756632     NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meta_description</th>\n      <th>updated_at</th>\n      <th>content</th>\n      <th>authors</th>\n      <th>scraped_at</th>\n      <th>id</th>\n      <th>url</th>\n      <th>meta_keywords</th>\n      <th>summary</th>\n      <th>domain</th>\n      <th>title</th>\n      <th>keywords</th>\n      <th>tags</th>\n      <th>type</th>\n      <th>inserted_at</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3660</th>\n      <td>World View: Both Greece and European Leaders S...</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>this morning s key headlines from generational...</td>\n      <td>John J. Xenakis</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>48770</td>\n      <td>http://www.breitbart.com/national-security/201...</td>\n      <td>['Alexis Tsipras', 'Angela Merkel', 'Britain',...</td>\n      <td>NaN</td>\n      <td>breitbart.com</td>\n      <td>World View: Both Greece and European Leaders S...</td>\n      <td>NaN</td>\n      <td>Tunisia, Generational Dynamics, Britain, Alexi...</td>\n      <td>political</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7278</th>\n      <td>NaN</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>daily app fix april &lt;number&gt; th &lt;number&gt; risk ...</td>\n      <td>NaN</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>96003</td>\n      <td>http://beforeitsnews.com/science-and-technolog...</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>beforeitsnews.com</td>\n      <td>Daily App Fix April 20th 2013 – RISK, Connect ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>fake</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4317</th>\n      <td>NaN</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>consciousness and unified physics are the keys...</td>\n      <td>Waking Times</td>\n      <td>2018-01-25 16:17:44.789555</td>\n      <td>16182</td>\n      <td>http://beforeitsnews.com/alternative/2016/07/c...</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>beforeitsnews.com</td>\n      <td>Consciousness and Unified Physics are the Keys...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>fake</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1932</th>\n      <td>NaN</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>do something , anything headline : bitcoin blo...</td>\n      <td>Humble Student Of The Markets</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>89922</td>\n      <td>http://beforeitsnews.com/financial-markets/201...</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>beforeitsnews.com</td>\n      <td>Do something, anything!</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>fake</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5321</th>\n      <td>NaN</td>\n      <td>2018-02-02 01:19:41.756664</td>\n      <td>from conservapedia transfer pricing is the set...</td>\n      <td>NaN</td>\n      <td>2018-01-25 20:13:50.426130</td>\n      <td>106668</td>\n      <td>http://www.conservapedia.com/Transfer_Pricing</td>\n      <td>['']</td>\n      <td>NaN</td>\n      <td>conservapedia.com</td>\n      <td>Transfer Pricing</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>bias</td>\n      <td>2018-02-02 01:19:41.756632</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "%%time\n",
    "# imports a random sample of size s from csv-file as a pandas dataframe\n",
    "# pandas using python 3.X uses utf-8 encoding\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random)\n",
    "filepath = 'd:/Personal/Desktop/downloads-tmp/clean-100k.csv'\n",
    "#filepath = 'news_sample.csv' # <- overwrite for setup\n",
    "s = 5000                    # desired sample size(seems to have slack ie. not exact)\n",
    "seed = 1                     # seed used by Pseudorandom number generator\n",
    "\n",
    "# init dataframe with specified values\n",
    "df = pd.read_csv(filepath, index_col = [0]).sample(n=s, random_state=seed)\n",
    "\n",
    "# visual output\n",
    "#print(df.shape, '<- size of dataframe \\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = pd.to_numeric(df['id'], errors = 'coerce', downcast = 'integer')\n",
    "df.drop_duplicates(subset = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(500, 15)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df = df.dropna(subset=['id']).set_index('id')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning for values out of bounds of DataBase requirements etc.\n",
    "df.index = df.index.astype(int)\n",
    "longAuthors = df[df['authors'].str.len() > 255].index\n",
    "df.drop(longAuthors, inplace = True)\n",
    "longTags = df[df['tags'].str.len() > 1000].drop_duplicates(subset = 'tags', keep = 'first').index\n",
    "df.drop(longTags, inplace = True)\n",
    "longMetaD = df[df['meta_description'].str.len() > 10000].index\n",
    "df.drop(longMetaD, inplace = True)\n",
    "df['authors'] = df['authors'].replace(np.nan, 'NoAuthor', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 38 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "regexEmail = r\"[a-zA-Z_-]+@[a-zA-Z_-]+(\\.[a-zA-Z]{2,4}){1,3}\"\n",
    "df.content = df.content.replace(to_replace=regexEmail, value='<EMAIL>', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 179 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "regexURL= r\"(?:https?:\\/\\/)?(?:www\\.)?([^@\\s]+\\.[a-zA-Z]{2,4})[^\\s]*\"\n",
    "df.content = df.content.replace(to_replace=regexURL, value='<URL>', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexDoubleSpace = r\"(\\s{2,})|\\n\"\n",
    "df.content = df.content.replace(to_replace=regexDoubleSpace, value=' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexDate = r\"(((19[7-9]\\d|20\\d{2})|(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|(nov|dec)(?:ember)?)|(([12][0-9])|(3[01])|(0?[1-9])))[\\/. \\-,\\n]){2,3}\"\n",
    "df.content = df.content.replace(to_replace=regexDate, value='<DATE>', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 31 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "regexNum = r\"(\\s)\\$?(?:[\\d,.-])+\"\n",
    "df.content = df.content.replace(to_replace=regexNum, value='<NUM>', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data-tables: [name]-uniq / relational-tables: [name]_in\n",
    "creating csv-files for database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where to save all csv-files\n",
    "path = 'Database_CSV_IN/'\n",
    "\n",
    "# create temporary dataframe and use article id as index \n",
    "out_df = pd.DataFrame({'id':df.index})\n",
    "out_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### types_uniq - data-table ###\n",
    "type_array = df.type.unique() # get array of unique types\n",
    "type_df = pd.DataFrame({'id': np.arange(type_array.size), 'name':type_array})\n",
    "\n",
    "# write file and free memory\n",
    "type_df.to_csv(path + 'type_clean.csv', index=False, header=True)\n",
    "#del type_array\n",
    "#del type_df # tmp delete later\n",
    "\n",
    "# create dict with type_name as key - [swap type with type_id]\n",
    "type_name_as_key_df = type_df.set_index('name')\n",
    "type_dict = type_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace type with tag id and create new column\n",
    "type_id = np.array([type_dict[key] for key in df['type'].to_numpy()])\n",
    "df['type_id'] =type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tags_uniq - data-table ###\n",
    "\n",
    "# creates list of list but formaly it is a pd.series of lists\n",
    "tags_series_of_lists = df.tags.dropna().str.split(', ') # -> ', ' not ','\n",
    "\n",
    "if not 'tags' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'tags', value = tags_series_of_lists)\n",
    "\n",
    "# flattern tags_series_of_lists to a set(ie. unique values only)\n",
    "tags_list = list(set([item for sublist in tags_series_of_lists for item in sublist]))\n",
    "\n",
    "# create dataframe\n",
    "tags_df = pd.DataFrame({'id': np.arange(len(tags_list)), 'name':tags_list})\n",
    "\n",
    "# write file and free memory\n",
    "tags_df.to_csv(path + 'tags_clean.csv', index=False, header=True)\n",
    "del tags_series_of_lists\n",
    "del tags_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### tags_in - relational-table ###\n",
    "\n",
    "# get all pairs of article_id and tags in a article (for all articles)\n",
    "articles_id_tags_name_pairs_df = out_df.tags.dropna().explode().drop_duplicates(keep = 'first')\n",
    "\n",
    "# split tags_name and articles_id\n",
    "articles_id_array = articles_id_tags_name_pairs_df.index.to_numpy()\n",
    "tags_name_array = articles_id_tags_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap tags with tags_id]\n",
    "tags_name_as_key_df = tags_df.set_index('name')\n",
    "tags_dict = tags_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace tags with tag id\n",
    "tags_id = np.array([tags_dict[key] for key in tags_name_array])\n",
    "\n",
    "# create dataframe\n",
    "tags_in_df = pd.DataFrame(data=articles_id_array, index=tags_id, columns=['article_id'])\n",
    "tags_in_df.index.name='tags_id'\n",
    "\n",
    "# write file and free memory\n",
    "tags_in_df.to_csv(path + 'tags_in.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### authors-uniq - data-table ###\n",
    "\n",
    "# creates list of list but formaly it is a pd.series of lists\n",
    "authors_series_of_lists = df.authors.str.split(',') # -> ',' not ', '\n",
    "\n",
    "if not 'authors' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'authors', value = authors_series_of_lists)\n",
    "\n",
    "# flattern authors_series_of_lists to a set(ie. unique values only)\n",
    "authors_list = list(set([item for sublist in authors_series_of_lists for item in sublist]))\n",
    "\n",
    "# create dataframe\n",
    "authors_df = pd.DataFrame({'id': np.arange(len(authors_list)), 'name':authors_list})\n",
    "\n",
    "# write file and free memory\n",
    "authors_df.to_csv(path + 'authors_clean.csv', index=False, header=True)\n",
    "#del authors_series_of_lists\n",
    "#del authors_list\n",
    "#del authors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors_in - relational-table ###\n",
    "#for i in out_df.index:\n",
    "    #out_df.author[i] = list(set(out_df.authors[i]))\n",
    "# get all pairs of article_id and authors in a article (for all articles)\n",
    "articles_id_authors_name_pairs_df = out_df.authors.dropna().explode()\n",
    "\n",
    "# split authors_name and articles_id\n",
    "articles_id_array = articles_id_authors_name_pairs_df.index.to_numpy()\n",
    "authors_name_array = articles_id_authors_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap authors with authors_id]\n",
    "authors_name_as_key_df = authors_df.set_index('name')\n",
    "authors_dict = authors_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace authors with tag id\n",
    "authors_id = np.array([authors_dict[key] for key in authors_name_array])\n",
    "\n",
    "# create dataframe\n",
    "authors_in_df = pd.DataFrame(data=articles_id_array, index=authors_id, columns=['article_id'])\n",
    "authors_in_df.index.name='authors_id'\n",
    "\n",
    "# write file and free memory\n",
    "authors_in_df.to_csv(path + 'authors_in.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### domains-uniq - data-table ###\n",
    "domain_array = df.domain.unique() # get array of unique domains\n",
    "domain_df = pd.DataFrame({'id': np.arange(domain_array.size), 'name':domain_array})\n",
    "\n",
    "# write file and free memory\n",
    "domain_df.to_csv(path + 'domain_name_clean.csv', index=False, header=True)\n",
    "#del domain_array\n",
    "#del domain_df\n",
    "\n",
    "# create dict with domain_name as key - [swap domain with domain_id]\n",
    "domain_name_as_key_df = domain_df.set_index('name')\n",
    "domain_dict = domain_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace domain with tag id and create new column\n",
    "domain_id = np.array([domain_dict[key] for key in df['domain'].to_numpy()])\n",
    "df['domain_id'] =domain_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta_keywords_uniq - data-table ###\n",
    "\n",
    "# use regex to remove string-padding\n",
    "regex = r\" *['\\\"\\[\\]]+\"\n",
    "meta_keywords_series = df.meta_keywords.replace(to_replace=regex, value='', regex=True).str.split(',')\n",
    "#meta_keywords_series = meta_keywords_series.replace(r'', np.NaN)\n",
    "\n",
    "if not 'meta_keywords' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'meta_keywords', value = meta_keywords_series)\n",
    "\n",
    "# create array of unique\n",
    "meta_keywords_set = meta_keywords_series.explode().unique()\n",
    "\n",
    "# create dataframe\n",
    "meta_keywords_df = pd.DataFrame({'id': np.arange(len(meta_keywords_set)), 'name':meta_keywords_set})\n",
    "\n",
    "# write file and free memory\n",
    "meta_keywords_df.to_csv(path + 'meta_keywords_clean.csv', index=False, header=True)\n",
    "#del meta_keywords_series\n",
    "#del meta_keywords_set\n",
    "#del meta_keywords_list\n",
    "#del meta_keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta_keywords_in - relational-table ###\n",
    "\n",
    "# get all pairs of article_id and meta_keywords in a article (for all articles)\n",
    "articles_id_meta_keywords_name_pairs_df = out_df.meta_keywords.dropna().explode()\n",
    "\n",
    "# split meta_keywords_name and articles_id\n",
    "articles_id_array = articles_id_meta_keywords_name_pairs_df.index.to_numpy()\n",
    "meta_keywords_name_array = articles_id_meta_keywords_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap meta_keywords with meta_keywords_id]\n",
    "meta_keywords_name_as_key_df = meta_keywords_df.set_index('name')\n",
    "meta_keywords_dict = meta_keywords_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace meta_keywords with tag id\n",
    "meta_keywords_id = np.array([meta_keywords_dict[key] for key in meta_keywords_name_array])\n",
    "\n",
    "# create dataframe\n",
    "meta_keywords_in_df = pd.DataFrame(data=articles_id_array, index=meta_keywords_id, columns=['article_id'])\n",
    "\n",
    "meta_keywords_in_df.index.name='meta_keywords_id'\n",
    "meta_keywords_in_df.reset_index(inplace = True)\n",
    "meta_keywords_in_df.drop_duplicates(subset = ['meta_keywords_id', 'article_id'], keep = 'first', inplace = True)\n",
    "# write file and free memory\n",
    "meta_keywords_in_df.to_csv(path + 'meta_keywords_in.csv', index=False, header=True)\n",
    "#meta_keywords_in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### article clean ###\n",
    "\n",
    "df[['domain_id', 'type_id', \"url\", \"content\", \"title\", \"meta_description\", \"scraped_at\",  \"updated_at\", \"inserted_at\"]].to_csv(path + 'article_clean.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                  content\nid                                                       \n48770   this morning s key headlines from generational...\n96003   daily app fix april <number> th <number> risk<...\n16182   consciousness and unified physics are the keys...\n89922   do something<NUM> anything headline : bitcoin ...\n106668  from conservapedia transfer pricing is the set...\n...                                                   ...\n22752   americans sugar intake same as <number> years ...\n95890   torture news roundup : cheney ' s screwed top ...\n103383  this article is within the scope of wikiprojec...\n93431   recommended resources counterthink cartoons ar...\n61029   missouri state troopers in riot gear stand in ...\n\n[495 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>48770</th>\n      <td>this morning s key headlines from generational...</td>\n    </tr>\n    <tr>\n      <th>96003</th>\n      <td>daily app fix april &lt;number&gt; th &lt;number&gt; risk&lt;...</td>\n    </tr>\n    <tr>\n      <th>16182</th>\n      <td>consciousness and unified physics are the keys...</td>\n    </tr>\n    <tr>\n      <th>89922</th>\n      <td>do something&lt;NUM&gt; anything headline : bitcoin ...</td>\n    </tr>\n    <tr>\n      <th>106668</th>\n      <td>from conservapedia transfer pricing is the set...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22752</th>\n      <td>americans sugar intake same as &lt;number&gt; years ...</td>\n    </tr>\n    <tr>\n      <th>95890</th>\n      <td>torture news roundup : cheney ' s screwed top ...</td>\n    </tr>\n    <tr>\n      <th>103383</th>\n      <td>this article is within the scope of wikiprojec...</td>\n    </tr>\n    <tr>\n      <th>93431</th>\n      <td>recommended resources counterthink cartoons ar...</td>\n    </tr>\n    <tr>\n      <th>61029</th>\n      <td>missouri state troopers in riot gear stand in ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>495 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "df[['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['type_id', 'content']].to_csv('d:/Personal/Desktop/downloads-tmp/' + 'learning-clean.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "meta_description\nupdated_at\ncontent\nauthors\nscraped_at\nurl\nmeta_keywords\nsummary\ndomain\ntitle\nkeywords\ntags\ntype\ninserted_at\nsource\ntype_id\ndomain_id\n"
    }
   ],
   "source": [
    "for col in df.columns: \n",
    "    print(col) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}