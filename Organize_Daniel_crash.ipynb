{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "# libraries we might not need\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the following code\n",
    "to use the whole document you only need one file specified by filepath for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 16) <- size of dataframe \n",
      "\n",
      "CPU times: user 32.9 s, sys: 18.6 s, total: 51.5 s\n",
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6803</td>\n",
       "      <td>322758</td>\n",
       "      <td>zerohedge.com</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>https://www.zerohedge.com/news/2014-12-17/new-...</td>\n",
       "      <td>Submitted by Mike Krieger via Liberty Blitzkri...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>59% Of Americans Support Post-9/11 Torture – P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>\"By an almost 2-1 margin, or 59-to-31 percent,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9359</td>\n",
       "      <td>1017129</td>\n",
       "      <td>shadowproof.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://shadowproof.com/2012/09/18/oregons-mar...</td>\n",
       "      <td>Measure 80, which would legalize marijuana for...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Oregon’s Marijuana Legalization Measure Traili...</td>\n",
       "      <td>Jon Walker, Jonathan Walker Grew Up In New Jer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4476</td>\n",
       "      <td>577340</td>\n",
       "      <td>us.blastingnews.com</td>\n",
       "      <td>satire</td>\n",
       "      <td>http://us.blastingnews.com/news/2017/05/photo/...</td>\n",
       "      <td>This website uses profiling (non technical) co...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Photogallery - Donald Trump 'gaining weight' o...</td>\n",
       "      <td>Blasting News, P. Ghose, M. Singh, W. Camille,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Donald Trump', 'Russian Scandal', 'Gaining W...</td>\n",
       "      <td>Article's photos Donald Trump 'gaining weight'...</td>\n",
       "      <td>4062 followers Duggar Family, 3756 followers S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>695601</td>\n",
       "      <td>breitbart.com</td>\n",
       "      <td>political</td>\n",
       "      <td>http://www.breitbart.com/author/joseph-c-phill...</td>\n",
       "      <td>It is said that when Alexander the Great visit...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Joseph C. Phillips, Author at Breitbart</td>\n",
       "      <td>Joseph C. Phillips</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Joseph C. Phillips, Author at Breitbart - Page...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1150037</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>https://www.express.co.uk/sport/football/61716...</td>\n",
       "      <td>Barca, along with Arsenal and Liverpool, have ...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Arsenal and Liverpool target to snub bargain £...</td>\n",
       "      <td>Joe Short</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>EZEQUIEL LAVEZZI will not be joining Barcelona...</td>\n",
       "      <td>Wenger would likely be tempted by a cheap deal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id               domain        type  \\\n",
       "6803   322758        zerohedge.com  conspiracy   \n",
       "9359  1017129      shadowproof.com     unknown   \n",
       "4476   577340  us.blastingnews.com      satire   \n",
       "988    695601        breitbart.com   political   \n",
       "155   1150037        express.co.uk       rumor   \n",
       "\n",
       "                                                    url  \\\n",
       "6803  https://www.zerohedge.com/news/2014-12-17/new-...   \n",
       "9359  https://shadowproof.com/2012/09/18/oregons-mar...   \n",
       "4476  http://us.blastingnews.com/news/2017/05/photo/...   \n",
       "988   http://www.breitbart.com/author/joseph-c-phill...   \n",
       "155   https://www.express.co.uk/sport/football/61716...   \n",
       "\n",
       "                                                content  \\\n",
       "6803  Submitted by Mike Krieger via Liberty Blitzkri...   \n",
       "9359  Measure 80, which would legalize marijuana for...   \n",
       "4476  This website uses profiling (non technical) co...   \n",
       "988   It is said that when Alexander the Great visit...   \n",
       "155   Barca, along with Arsenal and Liverpool, have ...   \n",
       "\n",
       "                      scraped_at                 inserted_at  \\\n",
       "6803  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "9359  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "4476  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "988   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "155   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                      updated_at  \\\n",
       "6803  2018-02-02 01:19:41.756664   \n",
       "9359  2018-02-02 01:19:41.756664   \n",
       "4476  2018-02-02 01:19:41.756664   \n",
       "988   2018-02-02 01:19:41.756664   \n",
       "155   2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                  title  \\\n",
       "6803  59% Of Americans Support Post-9/11 Torture – P...   \n",
       "9359  Oregon’s Marijuana Legalization Measure Traili...   \n",
       "4476  Photogallery - Donald Trump 'gaining weight' o...   \n",
       "988             Joseph C. Phillips, Author at Breitbart   \n",
       "155   Arsenal and Liverpool target to snub bargain £...   \n",
       "\n",
       "                                                authors  keywords  \\\n",
       "6803                                                NaN       NaN   \n",
       "9359  Jon Walker, Jonathan Walker Grew Up In New Jer...       NaN   \n",
       "4476  Blasting News, P. Ghose, M. Singh, W. Camille,...       NaN   \n",
       "988                                  Joseph C. Phillips       NaN   \n",
       "155                                           Joe Short       NaN   \n",
       "\n",
       "                                          meta_keywords  \\\n",
       "6803                                               ['']   \n",
       "9359                                               ['']   \n",
       "4476  ['Donald Trump', 'Russian Scandal', 'Gaining W...   \n",
       "988                                                ['']   \n",
       "155                                                ['']   \n",
       "\n",
       "                                       meta_description  \\\n",
       "6803  \"By an almost 2-1 margin, or 59-to-31 percent,...   \n",
       "9359                                                NaN   \n",
       "4476  Article's photos Donald Trump 'gaining weight'...   \n",
       "988   Joseph C. Phillips, Author at Breitbart - Page...   \n",
       "155   EZEQUIEL LAVEZZI will not be joining Barcelona...   \n",
       "\n",
       "                                                   tags  summary  source  \n",
       "6803                                                NaN      NaN     NaN  \n",
       "9359                                                NaN      NaN     NaN  \n",
       "4476  4062 followers Duggar Family, 3756 followers S...      NaN     NaN  \n",
       "988                                                 NaN      NaN     NaN  \n",
       "155   Wenger would likely be tempted by a cheap deal...      NaN     NaN  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# imports a random sample of size s from csv-file as a pandas dataframe\n",
    "# pandas using python 3.X uses utf-8 encoding\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random)\n",
    "filepath = '1mioraw.csv'\n",
    "#filepath = 'news_sample.csv' # <- overwrite for setup\n",
    "s = 1000000                    # desired sample size(seems to have slack ie. not exact)\n",
    "seed = 1                     # seed used by Pseudorandom number generator\n",
    "\n",
    "# init dataframe with specified values\n",
    "df = pd.read_csv(filepath, index_col = [0]).sample(n=s, random_state=seed)\n",
    "\n",
    "# visual output\n",
    "#print(df.shape, '<- size of dataframe \\n')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = pd.to_numeric(df['id'], errors = 'coerce', downcast = 'integer')\n",
    "df.drop_duplicates(subset = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999934, 15)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['id']).set_index('id')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning for values out of bounds of DataBase requirements etc.\n",
    "df.index = df.index.astype(int)\n",
    "longAuthors = df[df['authors'].str.len() > 255].index\n",
    "df.drop(longAuthors, inplace = True)\n",
    "longTags = df[df['tags'].str.len() > 1000].drop_duplicates(subset = 'tags', keep = 'first').index\n",
    "df.drop(longTags, inplace = True)\n",
    "longMetaD = df[df['meta_description'].str.len() > 10000].index\n",
    "df.drop(longMetaD, inplace = True)\n",
    "df['authors'] = df['authors'].replace(np.nan, 'NoAuthor', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regexEmail = r\"[a-zA-Z_-]+@[a-zA-Z_-]+(\\.[a-zA-Z]{2,4}){1,3}\"\n",
    "df.content = df.content.replace(to_replace=regexEmail, value='<EMAIL>', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regexURL= r\"(?:https?:\\/\\/)?(?:www\\.)?([^@\\s]+\\.[a-zA-Z]{2,4})[^\\s]*\"\n",
    "df.content = df.content.replace(to_replace=regexURL, value='<URL>', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexDoubleSpace = r\"(\\s{2,})|\\n\"\n",
    "df.content = df.content.replace(to_replace=regexDoubleSpace, value=' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexDate = r\"(((19[7-9]\\d|20\\d{2})|(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|(nov|dec)(?:ember)?)|(([12][0-9])|(3[01])|(0?[1-9])))[\\/. \\-,\\n]){2,3}\"\n",
    "df.content = df.content.replace(to_replace=regexDate, value='<DATE>', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regexNum = r\"(\\s)\\$?(?:[\\d,.-])+\"\n",
    "df.content = df.content.replace(to_replace=regexNum, value='<NUM>', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data-tables: [name]-uniq / relational-tables: [name]_in\n",
    "creating csv-files for database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where to save all csv-files\n",
    "path = 'Database_CSV_IN/'\n",
    "\n",
    "# create temporary dataframe and use article id as index \n",
    "out_df = pd.DataFrame({'id':df.index})\n",
    "out_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### types_uniq - data-table ###\n",
    "type_array = df.type.unique() # get array of unique types\n",
    "type_df = pd.DataFrame({'id': np.arange(type_array.size), 'name':type_array})\n",
    "\n",
    "# write file and free memory\n",
    "type_df.to_csv(path + 'type_clean.csv', index=False, header=True)\n",
    "#del type_array\n",
    "#del type_df # tmp delete later\n",
    "\n",
    "# create dict with type_name as key - [swap type with type_id]\n",
    "type_name_as_key_df = type_df.set_index('name')\n",
    "type_dict = type_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace type with tag id and create new column\n",
    "type_id = np.array([type_dict[key] for key in df['type'].to_numpy()])\n",
    "df['type_id'] =type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tags_uniq - data-table ###\n",
    "\n",
    "# creates list of list but formaly it is a pd.series of lists\n",
    "tags_series_of_lists = df.tags.dropna().str.split(', ') # -> ', ' not ','\n",
    "\n",
    "if not 'tags' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'tags', value = tags_series_of_lists)\n",
    "\n",
    "# flattern tags_series_of_lists to a set(ie. unique values only)\n",
    "tags_list = list(set([item for sublist in tags_series_of_lists for item in sublist]))\n",
    "\n",
    "# create dataframe\n",
    "tags_df = pd.DataFrame({'id': np.arange(len(tags_list)), 'name':tags_list})\n",
    "\n",
    "# write file and free memory\n",
    "tags_df.to_csv(path + 'tags_clean.csv', index=False, header=True)\n",
    "del tags_series_of_lists\n",
    "del tags_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### tags_in - relational-table ###\n",
    "\n",
    "# get all pairs of article_id and tags in a article (for all articles)\n",
    "articles_id_tags_name_pairs_df = out_df.tags.dropna().explode().drop_duplicates(keep = 'first')\n",
    "\n",
    "# split tags_name and articles_id\n",
    "articles_id_array = articles_id_tags_name_pairs_df.index.to_numpy()\n",
    "tags_name_array = articles_id_tags_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap tags with tags_id]\n",
    "tags_name_as_key_df = tags_df.set_index('name')\n",
    "tags_dict = tags_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace tags with tag id\n",
    "tags_id = np.array([tags_dict[key] for key in tags_name_array])\n",
    "\n",
    "# create dataframe\n",
    "tags_in_df = pd.DataFrame(data=articles_id_array, index=tags_id, columns=['article_id'])\n",
    "tags_in_df.index.name='tags_id'\n",
    "\n",
    "# write file and free memory\n",
    "tags_in_df.to_csv(path + 'tags_in.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### authors-uniq - data-table ###\n",
    "\n",
    "# creates list of list but formaly it is a pd.series of lists\n",
    "authors_series_of_lists = df.authors.str.split(',') # -> ',' not ', '\n",
    "\n",
    "if not 'authors' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'authors', value = authors_series_of_lists)\n",
    "\n",
    "# flattern authors_series_of_lists to a set(ie. unique values only)\n",
    "authors_list = list(set([item for sublist in authors_series_of_lists for item in sublist]))\n",
    "\n",
    "# create dataframe\n",
    "authors_df = pd.DataFrame({'id': np.arange(len(authors_list)), 'name':authors_list})\n",
    "\n",
    "# write file and free memory\n",
    "authors_df.to_csv(path + 'authors_clean.csv', index=False, header=True)\n",
    "#del authors_series_of_lists\n",
    "#del authors_list\n",
    "#del authors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors_in - relational-table ###\n",
    "#for i in out_df.index:\n",
    "    #out_df.author[i] = list(set(out_df.authors[i]))\n",
    "# get all pairs of article_id and authors in a article (for all articles)\n",
    "articles_id_authors_name_pairs_df = out_df.authors.dropna().explode()\n",
    "\n",
    "# split authors_name and articles_id\n",
    "articles_id_array = articles_id_authors_name_pairs_df.index.to_numpy()\n",
    "authors_name_array = articles_id_authors_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap authors with authors_id]\n",
    "authors_name_as_key_df = authors_df.set_index('name')\n",
    "authors_dict = authors_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace authors with tag id\n",
    "authors_id = np.array([authors_dict[key] for key in authors_name_array])\n",
    "\n",
    "# create dataframe\n",
    "authors_in_df = pd.DataFrame(data=articles_id_array, index=authors_id, columns=['article_id'])\n",
    "authors_in_df.index.name='authors_id'\n",
    "\n",
    "# write file and free memory\n",
    "authors_in_df.to_csv(path + 'authors_in.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### domains-uniq - data-table ###\n",
    "domain_array = df.domain.unique() # get array of unique domains\n",
    "domain_df = pd.DataFrame({'id': np.arange(domain_array.size), 'name':domain_array})\n",
    "\n",
    "# write file and free memory\n",
    "domain_df.to_csv(path + 'domain_name_clean.csv', index=False, header=True)\n",
    "#del domain_array\n",
    "#del domain_df\n",
    "\n",
    "# create dict with domain_name as key - [swap domain with domain_id]\n",
    "domain_name_as_key_df = domain_df.set_index('name')\n",
    "domain_dict = domain_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace domain with tag id and create new column\n",
    "domain_id = np.array([domain_dict[key] for key in df['domain'].to_numpy()])\n",
    "df['domain_id'] =domain_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta_keywords_uniq - data-table ###\n",
    "\n",
    "# use regex to remove string-padding\n",
    "regex = r\" *['\\\"\\[\\]]+\"\n",
    "meta_keywords_series = df.meta_keywords.replace(to_replace=regex, value='', regex=True).str.split(',')\n",
    "#meta_keywords_series = meta_keywords_series.replace(r'', np.NaN)\n",
    "\n",
    "if not 'meta_keywords' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'meta_keywords', value = meta_keywords_series)\n",
    "\n",
    "# create array of unique\n",
    "meta_keywords_set = meta_keywords_series.explode().unique()\n",
    "\n",
    "# create dataframe\n",
    "meta_keywords_df = pd.DataFrame({'id': np.arange(len(meta_keywords_set)), 'name':meta_keywords_set})\n",
    "\n",
    "# write file and free memory\n",
    "meta_keywords_df.to_csv(path + 'meta_keywords_clean.csv', index=False, header=True)\n",
    "#del meta_keywords_series\n",
    "#del meta_keywords_set\n",
    "#del meta_keywords_list\n",
    "#del meta_keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta_keywords_in - relational-table ###\n",
    "\n",
    "# get all pairs of article_id and meta_keywords in a article (for all articles)\n",
    "articles_id_meta_keywords_name_pairs_df = out_df.meta_keywords.dropna().explode()\n",
    "\n",
    "# split meta_keywords_name and articles_id\n",
    "articles_id_array = articles_id_meta_keywords_name_pairs_df.index.to_numpy()\n",
    "meta_keywords_name_array = articles_id_meta_keywords_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap meta_keywords with meta_keywords_id]\n",
    "meta_keywords_name_as_key_df = meta_keywords_df.set_index('name')\n",
    "meta_keywords_dict = meta_keywords_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace meta_keywords with tag id\n",
    "meta_keywords_id = np.array([meta_keywords_dict[key] for key in meta_keywords_name_array])\n",
    "\n",
    "# create dataframe\n",
    "meta_keywords_in_df = pd.DataFrame(data=articles_id_array, index=meta_keywords_id, columns=['article_id'])\n",
    "\n",
    "meta_keywords_in_df.index.name='meta_keywords_id'\n",
    "meta_keywords_in_df.reset_index(inplace = True)\n",
    "meta_keywords_in_df.drop_duplicates(subset = ['meta_keywords_id', 'article_id'], keep = 'first', inplace = True)\n",
    "# write file and free memory\n",
    "meta_keywords_in_df.to_csv(path + 'meta_keywords_in.csv', index=False, header=True)\n",
    "#meta_keywords_in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### article clean ###\n",
    "\n",
    "df[['domain_id', 'type_id', \"url\", \"content\", \"title\", \"meta_description\", \"scraped_at\",  \"updated_at\", \"inserted_at\"]].to_csv(path + 'article_clean.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
