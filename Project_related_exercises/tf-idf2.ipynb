{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execQuery(query):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"root\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\",\n",
    "                                      database = \"postgres\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        record = cursor.fetchall()\n",
    "        return record\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        connection = False\n",
    "        print (\"Error while connecting to PostgreSQL\", error)\n",
    "    finally:\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Executed query and closed connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesContent = execQuery(\"\"\"Select content\n",
    "from article\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection = psycopg2.connect(user = \"postgres\",\n",
    "#                                      password = \"root\",\n",
    "#                                      host = \"localhost\",\n",
    "#                                      port = \"5432\",\n",
    "#                                      database = \"postgres\")\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random) \n",
    "filepath = '/home/daniel/OneDrive/KUuni/DataScience/Python/small.csv'\n",
    "#filepath = 'news_sample.csv' # <- overwrite for setup\n",
    "s = 250                    # desired sample size(seems to have slack ie. not exact)\n",
    "seed = 1                     # seed used by Pseudorandom number generator\n",
    "\n",
    "df = pd.read_csv(filepath, index_col = [0]).sample(n=s, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content = df['content'].str.lower()\n",
    "series_content_tokens = df_content.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_content_tokens = series_content_tokens.explode()\n",
    "df_content_tokens = series_content_tokens.to_frame()\n",
    "df_content_tokens = df_content_tokens.reset_index().rename(columns={'index':'content_index'})\n",
    "series_content_tokens = df_content_tokens.groupby('content_index')['content'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "content_index  content     \n0              the             0.084715\n               .               0.040516\n               of              0.036832\n               to              0.029466\n               in              0.025783\n                                 ...   \n249            unacceptable    0.007752\n               wa              0.007752\n               wednesday       0.007752\n               will            0.007752\n               yangon          0.007752\nName: content, Length: 76994, dtype: float64"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "series_content_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "content               !   #         $         %         &         '        ''  \\\ncontent_index                                                                   \n0              0.007366 NaN  0.001842       NaN       NaN       NaN       NaN   \n1              0.008969 NaN       NaN  0.004484  0.004484       NaN       NaN   \n2                   NaN NaN       NaN       NaN  0.001912       NaN       NaN   \n3                   NaN NaN       NaN       NaN       NaN       NaN       NaN   \n4                   NaN NaN       NaN       NaN       NaN       NaN       NaN   \n...                 ...  ..       ...       ...       ...       ...       ...   \n245            0.001922 NaN  0.001281  0.004484  0.000641       NaN       NaN   \n246            0.002985 NaN       NaN       NaN  0.002985       NaN       NaN   \n247                 NaN NaN       NaN       NaN       NaN       NaN  0.033708   \n248            0.002016 NaN       NaN       NaN       NaN  0.002016  0.020161   \n249                 NaN NaN       NaN       NaN       NaN       NaN  0.007752   \n\ncontent        'come  'd  'editing  ...   …  …..  …and  …liberate  …record  \\\ncontent_index                       ...                                      \n0                NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n1                NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n2                NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n3                NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n4                NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n...              ...  ..       ...  ...  ..  ...   ...        ...      ...   \n245              NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n246              NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n247              NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n248              NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n249              NaN NaN       NaN  ... NaN  NaN   NaN        NaN      NaN   \n\ncontent        …researchers  €1100  €7.2bn   →  ≥8  \ncontent_index                                       \n0                       NaN    NaN     NaN NaN NaN  \n1                       NaN    NaN     NaN NaN NaN  \n2                       NaN    NaN     NaN NaN NaN  \n3                       NaN    NaN     NaN NaN NaN  \n4                       NaN    NaN     NaN NaN NaN  \n...                     ...    ...     ...  ..  ..  \n245                     NaN    NaN     NaN NaN NaN  \n246                     NaN    NaN     NaN NaN NaN  \n247                     NaN    NaN     NaN NaN NaN  \n248                     NaN    NaN     NaN NaN NaN  \n249                     NaN    NaN     NaN NaN NaN  \n\n[250 rows x 17928 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>content</th>\n      <th>!</th>\n      <th>#</th>\n      <th>$</th>\n      <th>%</th>\n      <th>&amp;</th>\n      <th>'</th>\n      <th>''</th>\n      <th>'come</th>\n      <th>'d</th>\n      <th>'editing</th>\n      <th>...</th>\n      <th>…</th>\n      <th>…..</th>\n      <th>…and</th>\n      <th>…liberate</th>\n      <th>…record</th>\n      <th>…researchers</th>\n      <th>€1100</th>\n      <th>€7.2bn</th>\n      <th>→</th>\n      <th>≥8</th>\n    </tr>\n    <tr>\n      <th>content_index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.007366</td>\n      <td>NaN</td>\n      <td>0.001842</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.008969</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.004484</td>\n      <td>0.004484</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.001912</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>0.001922</td>\n      <td>NaN</td>\n      <td>0.001281</td>\n      <td>0.004484</td>\n      <td>0.000641</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>0.002985</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002985</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.033708</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>0.002016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002016</td>\n      <td>0.020161</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.007752</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>250 rows × 17928 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# create TF\n",
    "TF = series_content_tokens.unstack(level=1)\n",
    "TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0007521289603870707"
     },
     "metadata": {},
     "execution_count": 341
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "df_1 = pd.DataFrame({'sent': [\n",
    "                     'the sky is blue',\n",
    "                     'blue not is sky the']})\n",
    "\n",
    "series_1 = df_1['sent'].str.split()\n",
    "\n",
    "# used to to tokenize\n",
    "    # # Tokenize and generate count vectors\n",
    "    # word_vec = series_of_lists_of_tokens.str.split().apply(pd.value_counts).fillna(0)\n",
    "\n",
    "\n",
    "def tf_idf(series_of_lists_of_tokens):\n",
    "    \"\"\" Returns tf-idf matrix(dataframe) for a given df-column/series with list of tokens for each row. \"\"\"\n",
    "\n",
    "    # Generate count vectors\n",
    "    word_vec = series_of_lists_of_tokens.apply(pd.value_counts).fillna(0) # intuitive but slow\n",
    "\n",
    "\n",
    "    # Compute term frequencies\n",
    "        ### Assuming from http://www.tfidf.com/ -> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "    tf = word_vec.divide(np.sum(word_vec, axis='columns'), axis='index')\n",
    "\n",
    "    # Compute inverse document frequencies\n",
    "        ### Slides seems wrong s.18: https://docs.google.com/presentation/d/1rXhewxbmLX7mzDcy8aucI8OLIikfrLLiz8ocN5GS06I/edit#slide=id.g77508fe5d3_0_19\n",
    "        ### Assuming from http://www.tfidf.com/ -> log_e(Total number of documents / Number of documents with term t in it)\n",
    "    idf = np.log(len(tf) / word_vec[word_vec > 0].count()) \n",
    "\n",
    "    # Compute TF-IDF vectors and return\n",
    "    tfidf = np.multiply(tf, idf.to_frame().T)\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "# print(tf)\n",
    "# print(idf)\n",
    "# stemming = PorterStemmer()\n",
    "series_content_tokens = df_1['sent'].str.split()\n",
    "\n",
    "# def tokenize(text):\n",
    "#     list_tokens = nltk.word_tokenize(text)\n",
    "#     a = [stemming.stem(word) for word in list_tokens]\n",
    "#     return a\n",
    "\n",
    "# df_tokens = series_content_tokens.explode().to_frame().reset_index().rename(columns={'index':'content_index'})\n",
    "# df_tokens.groupby('content_index')[df_tokens.columns[1]].value_counts(normalize=True).unstack(level=1)\n",
    "\n",
    "\n",
    "tf_idf(series_content_tokens)\n",
    "# df_tokens.apply(tuple).value_counts()\n",
    "# df_1['sent'].str.split().groupby('name')['sent'].value_counts().unstack().fillna(0)\n",
    "\n",
    "pd.DataFrame(series_content_tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9c3c2739340c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorpus_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "corpus_index = [n for n in corpus]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfs.T.todense(), index=feature_names, columns=corpus_index)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "This        3\nis          2\nfirst       1\nsecond      1\nthe         2\nsentence    2\nthird       1\ndtype: int64\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_1 = pd.DataFrame({'sent': \n",
    "                    ['This This is is first', \n",
    "                     'This is the second sentence',\n",
    "                     'This the third sentence']})\n",
    "\n",
    "# Tokenize and generate count vectors\n",
    "word_vec = df_1.sent.str.split().apply(pd.value_counts).fillna(0)\n",
    "\n",
    "# Compute term frequencies\n",
    "tf = word_vec.divide(np.sum(word_vec, axis='columns'), axis='index')\n",
    "\n",
    "# Compute inverse document frequencies\n",
    "idf = np.log(len(tf) / word_vec[word_vec > 0].count()) \n",
    "\n",
    "# Compute TF-IDF vectors\n",
    "tfidf = np.multiply(tf, idf.to_frame().T)\n",
    "\n",
    "print(word_vec[word_vec > 0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   This is the first sentence  This is the second sentence  \\\n0                        26.0                          0.0   \n1                         0.0                         27.0   \n2                         0.0                          0.0   \n\n   This is the third sentence  \n0                         0.0  \n1                         0.0  \n2                        26.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>This is the first sentence</th>\n      <th>This is the second sentence</th>\n      <th>This is the third sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>27.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>26.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "df = pd.DataFrame({'docId': [1,2,3], \n",
    "               'sent': ['This is the first sentence', \n",
    "                        'This is the second sentence',\n",
    "                        'This is the third sentence']})\n",
    "\n",
    "# use 'value_counts' to get counts of items in list\n",
    "tf = df.sent.apply(pd.value_counts).fillna(0) \n",
    "# tf = df.sent.str.split().apply(pd.value_counts).fillna(0)\n",
    "\n",
    "# add one to numerator and denominator just incase a term isn't in any document\n",
    "# maximum value is log(N) and minimum value is zero\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "scipy.sparse.csr.csr_matrix"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'docId': [1,2,3], \n",
    "               'sent': ['This is the first sentence','This is the second sentence', 'This is the third sentence']})\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df['sent'])\n",
    " \n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('DS': conda)",
   "language": "python",
   "name": "python37764bitdsconda217f41063fe24cf89ccdf8aa73f962cf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}