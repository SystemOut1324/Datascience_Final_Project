{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Image class from PIL package  \n",
    "from PIL import Image  \n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ER Diagram \n",
    "\n",
    "![ER Diagram](https://i.imgur.com/naRsMrw.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have created and imported our schema to our python notebook as an image. We have made the decision to exclude 'meta_keyword', 'type, 'domain', 'authors' and 'tag' as \n",
    "attributes of 'article', and instead have their own tables. This was done, to create the necessary entity-relations. For example, in the table 'type' there are 11 types, that each has their own id because 'type' is a one-to-many relation, and that id\n",
    "is saved in the 'article' instance. 'tag' on the other hand is a many-to-many relation, and we have therefore created a relation table that saves the tags associated with an article. In this \n",
    "relation table the id of the article and the tag is saved together. The other one-to-many and many-to-many relations works as described above.The SQL-code that was used to create the database can be found in the file \"SQL_database.sql\". \n",
    "\n",
    "The code used to separate the database and clean the data can be seen in the appendix. Where the clean part replaces dates, emails, etc with \\<DATE>, \\<EMAIL>, etc.  the organize part  separates the articles into different csv-files which we populate our database with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execQuery(query):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"detminkode\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\",\n",
    "                                      database = \"postgres\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        record = cursor.fetchall()\n",
    "        return record\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        connection = False\n",
    "        print (\"Error while connecting to PostgreSQL\", error)\n",
    "    finally:\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Executed query and closed connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(991691,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"Select Count(id)\n",
    "from article\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve task 3.1, we have created two queries, one with and one without INNER JOIN. We save our 'scraped_at' dates as 'Dates', to utilize the >= operator to find the articles that\n",
    "has been scraped after 15. Jan 2018. We also isolated the articles with type_id = 4, as this is the id for the articles classified as 'reliable'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT domain_name.domain_name FROM domain_name\n",
    "WHERE domain_id in (SELECT article.domain_id FROM article\n",
    "WHERE article.type_id = '4' and scraped_at >= '2018-01-15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without INNER JOIN above, and without it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT DISTINCT domain_name.domain_name FROM domain_name\n",
    "INNER JOIN article\n",
    "ON article.domain_id = domain_name.domain_id\n",
    "WHERE article.type_id = '4' and scraped_at >= '2018-01-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('christianpost.com',), ('consortiumnews.com',), ('nutritionfacts.org',)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT DISTINCT domain_name.domain_name FROM domain_name\n",
    "INNER JOIN article\n",
    "ON article.domain_id = domain_name.domain_id\n",
    "WHERE article.type_id = '12' and scraped_at >= '2018-01-15'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the query below we find the author(s) with the most articles classified as fake news. This is done by finding all the authors for the articels that have been classified as fake, then count the authors, and lastly group all the authors that have the same number of fake articels. This is to make sure that if two or more authors have shared the most artikels. Then we order them and take first element which will be the largest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT array_agg(authorname), counter\n",
    "FROM (SELECT COUNT(author_in.authorid) as counter, author.authorname\n",
    "    FROM author_in\n",
    "    INNER JOIN author\n",
    "    ON author.authorid = author_in.authorid\n",
    "    WHERE Author.authorname != 'NoAuthor' AND author_in.id in (SELECT article.id FROM article WHERE article.type_id = 7)\n",
    "    GROUP BY author_in.authorid, author.authorname) AS authors\n",
    "GROUP BY counter\n",
    "ORDER BY counter DESC\n",
    "limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['John Rolls'], 1142)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT array_agg(authorname), counter\n",
    "FROM (SELECT COUNT(author_in.authorid) as counter, author.authorname\n",
    "    FROM author_in\n",
    "    INNER JOIN author\n",
    "    ON author.authorid = author_in.authorid\n",
    "    WHERE Author.authorname != 'NoAuthor' AND author_in.id in (SELECT article.id FROM article WHERE article.type_id = 7)\n",
    "    GROUP BY author_in.authorid, author.authorname) AS authors\n",
    "GROUP BY counter\n",
    "ORDER BY counter DESC\n",
    "limit 1 \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we used to count the pairs, was to compare 3 articles (a, b, c) and if they had the same meta_tags, to then create the pairs (a, b) and (a, c).\n",
    "We do not include the pair (b, c), as this information can be deduced by the other pairs. Therefore the number of pairs are Count(*) - 1, as can be seen \n",
    "on line 2 in the query. The way it works, is by collapsing all the meta_tags with the articles, so the meta_tags get saved as a list instead of all the \n",
    "different rows of data. We do this with the aggregate function 'array_agg'. After this we collapse the article id's that has the same list of meta_tags\n",
    "using the same 'array_agg' fuction, and if the length of the list is more than one then there are one or more pairs. Lastly, we then sum all the pairs,\n",
    "which returns the total number of article pairs with the same meta_tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT sum(pairAmount)\n",
    "FROM (SELECT count(*) - 1 as pairAmount\n",
    "\tFROM (SELECT id, array_agg(meta_keywords.meta_keyword) AS Meta_Keyword_ids\n",
    "\t\t\tFROM meta_keywords_in\n",
    "\t\t\tINNER JOIN meta_keywords\n",
    "\t\t\tON meta_keywords.meta_keyword_id = meta_keywords_in.meta_keyword_id\n",
    "\t\t\tWHERE meta_keywords.meta_keyword_id != '0'\n",
    "\t\t\tGROUP BY meta_keywords_in.id\n",
    "\t\t ) meta_keywords_in\n",
    "\tGROUP BY Meta_Keyword_ids\n",
    "\tHAVING count(*) > 1) AS counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Decimal('24773'),)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT sum(pairAmount)\n",
    "FROM (SELECT count(*) - 1 as pairAmount\n",
    "\tFROM (SELECT id, array_agg(meta_keywords.meta_keyword) AS Meta_Keyword_ids\n",
    "\t\t\tFROM meta_keywords_in\n",
    "\t\t\tINNER JOIN meta_keywords\n",
    "\t\t\tON meta_keywords.meta_keyword_id = meta_keywords_in.meta_keyword_id\n",
    "\t\t\tWHERE meta_keywords.meta_keyword_id != '0'\n",
    "\t\t\tGROUP BY meta_keywords_in.id\n",
    "\t\t ) meta_keywords_in\n",
    "\tGROUP BY Meta_Keyword_ids\n",
    "\tHAVING count(*) > 1) AS counter\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataexploration Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a hypothesis that fake or other type that we would consider to cluster with fake type\n",
    "would have a short content. We test this hypothesis in this query and what the query does is it calculates the length of all the articles and then it sums this length of all of the same types and then this sum is divided by the amount of articles of that specific type. The way it divides by only the specific types id is because we group by type_id, so both sum and count works within the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT sum(length(content))/COUNT(id) as AvgLen, type\n",
    "from article\n",
    "Inner join type\n",
    "ON article.type_id = type.type_id\n",
    "GROUP BY type\n",
    "ORDER BY AvgLen ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1510, 'satire'),\n",
       " (1629, 'rumor'),\n",
       " (1914, 'unreliable'),\n",
       " (2399, 'conspiracy'),\n",
       " (2459, None),\n",
       " (2460, 'clickbait'),\n",
       " (3060, 'junksci'),\n",
       " (3072, 'fake'),\n",
       " (3501, 'political'),\n",
       " (3543, 'unknown'),\n",
       " (3667, 'bias'),\n",
       " (4365, 'reliable'),\n",
       " (8426, 'hate')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT sum(length(content))/COUNT(id) as AvgLen, type\n",
    "from article\n",
    "Inner join type\n",
    "ON article.type_id = type.type_id\n",
    "GROUP BY type\n",
    "ORDER BY AvgLen ASC \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataexploration Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to see which meta_keywords tag are most frequently appiering, on articles with the type fake. This is because we had a theoty that the meta_keywords might play a big role in classifying wether or not an articel is fake or not. The query below will give all the meta_keywords that appier more than once, in decinting order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT COUNT(meta_keywords_in.meta_keyword_id), meta_keywords.meta_keyword\n",
    "FROM meta_keywords_in\n",
    "inner join\n",
    "article\n",
    "ON article.id = meta_keywords_in.id and article.type_id = 7\n",
    "inner join \n",
    "meta_keywords\n",
    "ON meta_keywords.meta_keyword_id = meta_keywords_in.meta_keyword_id\n",
    "GROUP BY meta_keywords.meta_keyword\n",
    "Having COUNT(meta_keywords_in.meta_keyword_id) > 1\n",
    "ORDER BY count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(123512, None),\n",
       " (5, 'texas'),\n",
       " (3, 'donald trump'),\n",
       " (3, 'Kfc'),\n",
       " (2, 'Clinton Foundation'),\n",
       " (2, 'president obama'),\n",
       " (2, 'secession'),\n",
       " (2, 'spontaneous combustion'),\n",
       " (2, 'trump'),\n",
       " (2, 'hillary clinton'),\n",
       " (2, 'Kentucky Fried Chicken'),\n",
       " (2, 'fox news'),\n",
       " (2, 'fried chicken')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT COUNT(meta_keywords_in.meta_keyword_id), meta_keywords.meta_keyword\n",
    "FROM meta_keywords_in\n",
    "inner join\n",
    "article\n",
    "ON article.id = meta_keywords_in.id and article.type_id = 7\n",
    "inner join \n",
    "meta_keywords\n",
    "ON meta_keywords.meta_keyword_id = meta_keywords_in.meta_keyword_id\n",
    "GROUP BY meta_keywords.meta_keyword\n",
    "Having COUNT(meta_keywords_in.meta_keyword_id) > 1\n",
    "ORDER BY count DESC\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataexploration Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made this query to see if we could seperate the different article types by comparing the average\n",
    "length of their tags as an indicator of their complexity.\n",
    "\n",
    "In this query we calculate the average length of tags by type. It works by inner joining tags_in and article on article.id which makes sure we get the matching article/tags, then inner joining this with type ON type_id so we can select type in the first line. From this Join it then selects the length of each tag in the tags table and then sums this up by grouping by type, and this is then divided by the amount of articles of that type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT sum(length(tags.tag)) / count(*) as Avg, type.type\n",
    "FROM tags_in\n",
    "INNER JOIN article\n",
    "ON article.id = tags_in.id\n",
    "inner join tags\n",
    "ON tags.tag_id = tags_in.tag_id\n",
    "inner join type\n",
    "ON type.type_id = article.type_id\n",
    "GROUP BY type.type\n",
    "ORDER BY Avg DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(43, 'rumor'),\n",
       " (18, None),\n",
       " (17, 'bias'),\n",
       " (17, 'hate'),\n",
       " (16, 'fake'),\n",
       " (16, 'political'),\n",
       " (16, 'conspiracy'),\n",
       " (16, 'unknown'),\n",
       " (16, 'clickbait'),\n",
       " (15, 'unreliable'),\n",
       " (14, 'junksci'),\n",
       " (12, 'satire'),\n",
       " (12, 'reliable')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT sum(length(tags.tag)) / count(*) as Avg, type.type\n",
    "FROM tags_in\n",
    "INNER JOIN article\n",
    "ON article.id = tags_in.id\n",
    "inner join tags\n",
    "ON tags.tag_id = tags_in.tag_id\n",
    "inner join type\n",
    "ON type.type_id = article.type_id\n",
    "GROUP BY type.type\n",
    "ORDER BY Avg DESC\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I this task we had to make a web crawler that could scrape information from \"Politics and Conflict\" on Wikinews and based on our group number we would should only select a subset of all articles. This was given to us in the form of some python code to generate a string with the beginning letter for articles within our subset of articles.\n",
    "\n",
    "The first step we took was to figure out what kind of task we were given before we decided on a given tool.\n",
    "\n",
    "The first few observations we made were regarding how the webpage indexed its articles such that we could make our crawling logic.\n",
    "\n",
    "The first observation we made was that each \"entry point\" for a given letter was easy to get as it was just https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=[letter], where the letter was at the end of the url. on a given entry point there are links to categories and articles however there are a maximum 200 pages, even if there are more articles with a given starting letter, so we had to follow links to find all articles. A sticking point arose when we looked at \"indexing\"-urls after our entry points as they had no information of what letter/entry point we were coming from as they where of the form https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&pagefrom=[start article]+[end article], such that the url only described by article tiles what other articles where on that indexing page. This mean that our tool had to be able to follow links and have knowledge of what page it was coming from as it could not just use information on a page as well as jumping to all articles on an indexing page.\n",
    "\n",
    "Next we looked at a few articles and they seemed to have a general structure such that locating them on a page within HTML would be somewhat doable so we postponed the actual scraping part for later.\n",
    "\n",
    "Because of the requirements based on our initial assesment of the problem and our look at python web scraping tools we choose Scrapy as it is a feature rich tool made for making web crawlers and as we had some requirements that where non trivial ie. traversal logic more complicated then get all links and so on we choose it.\n",
    "\n",
    "When implementing our scraper, we encountered a lot obstacles along the way, amplified by the fact that we assigned tasks such that people with less explerience in an given subject had to do it for our assignment.\n",
    "\n",
    "We started by reading documentation while watching and reading tutorials as we building a dummy spider for a smaller part of the problem to get the basics of Scrapy right as well as understading HTML-markup node navigation. We made use of xpath to locate nodes within HTML. A snapshot of our Scrapy class we made is given below along with its output:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 8) <- shape of wikinews-dataset(row, col)\n",
      "(3680,) <- num of unique articles - seems to have the same number of rows no duplicates\n",
      "437 <- num of nan entries out of -> 29003 \n",
      "nan rate of 0.01506740681998414 %\n",
      "216 nan elements from sources_url column\n",
      "210 nan elements from about_sources_wiki_url column\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3680 entries, 0 to 3679\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   article_url             3680 non-null   object\n",
      " 1   categories              3680 non-null   object\n",
      " 2   content                 3678 non-null   object\n",
      " 3   publish_date            3671 non-null   object\n",
      " 4   scraped_at              3680 non-null   object\n",
      " 5   sources_url             3464 non-null   object\n",
      " 6   about_sources_wiki_url  3470 non-null   object\n",
      " 7   title                   3680 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 230.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "filepath = '/home/daniel/OneDrive/KUuni/DataScience/Python/DS_5/wiki_news_nr_12.csv'\n",
    "\n",
    "# we read in the file\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# print shape -< how many rows and elements it has\n",
    "print(df.shape, \"<- shape of wikinews-dataset(row, col)\")\n",
    "\n",
    "# how many unique urls did we get(optimally as many as the rows in our dataframe)\n",
    "unique_articles = df['article_url'].unique().shape\n",
    "print(unique_articles, \"<- num of unique articles - seems to have the same number of rows no duplicates\")\n",
    "\n",
    "# how many fields without daata ie. nan out of all fiels\n",
    "df_nan_elms = df.isna().sum().sum()\n",
    "df_not_nan_elms = df.notna().sum().sum()\n",
    "print( df_nan_elms, \"<- num of nan entries out of ->\",  df_not_nan_elms ,\n",
    "     \"\\nnan rate of\", df_nan_elms/df_not_nan_elms, \"%\")\n",
    "# majority comes form articles not having sources or source wiki pages\n",
    "print(df['sources_url'].isna().sum(), \"nan elements from sources_url column\")\n",
    "print( df['about_sources_wiki_url'].isna().sum(), \"nan elements from about_sources_wiki_url column\\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test spider below was used to see of we could iterate over a list of article-URLs if we could locate them on a navigation page. This early spider only has 2 parts; one for iterating over URLs and the other for getting article scraping data. The spider above worked fine, but the next step being creating the navigation logic became the greatest challenge, caused by a simple thing. We wrote the spider logic for another website as it was simpler and then adapted it to wikinews, however when we interchanged our other website with wikinews no files where generated when we selected to get output. After debugging we came across an Error about robot.txt which tunrns out is used when accesing websites from non-intruductionary-tutorials as it is a policy obayed by all never Scrapy spider by default if a website dosen't allow certain kinds of scrapers. After this we changed our spider to not obey the robot.txt but read https://en.wikipedia.org/robots.txt instead, and implimented some restrictions on the spider. Below the test spider we have included some of the settings we enabled to scrape more responsibly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "``` Test spider - bash\n",
    "class testSpider(scrapy.Spider):\n",
    "    name = \"test\"\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'https://en.wikinews.org/wiki/A_policeman_is_killed_and_another_one_is_tortured_in_MST_camp,_in_Brazil',\n",
    "\n",
    "            'https://en.wikinews.org/wiki/African_Union_refuses_to_arrest_Sudan%27s_President_for_war_crimes',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # get article content\n",
    "    def parse(self, response):\n",
    "        for info in response.xpath('//div[@id=\"content\"]'):\n",
    "            yield {\n",
    "                'title': info.xpath('//*[@id=\"firstHeading\"]/text()').get(),\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "``` Settings - python\n",
    "AUTOTHROTTLE_ENABLED = True\n",
    "\n",
    "# The initial download delay\n",
    "AUTOTHROTTLE_START_DELAY = 5\n",
    "\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "AUTOTHROTTLE_MAX_DELAY = 30\n",
    "\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "\n",
    "AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "HTTPCACHE_ENABLED = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final spider became somewhat complicated for a first spider and crawls without any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "``` python\n",
    "import string\n",
    "import re\n",
    "import scrapy\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, Compose\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from ..items import articleItem # location of item - used for scraped data structure\n",
    "\n",
    "# creating urls for chars based on group _nr - change group_nr to generate start_urls\n",
    "group_nr = 12 # <- change to get correct article set\n",
    "urls =[]\n",
    "for char in \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr % 23:group_nr % 23+10]:\n",
    "    urls.append('https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=' + char)\n",
    "print(*urls, sep='\\n')\n",
    "\n",
    "# main spider\n",
    "class wikiSpider(scrapy.Spider):\n",
    "    name = \"wiki\"\n",
    "\n",
    "    # start urls for scraping\n",
    "    def start_requests(self):\n",
    "\n",
    "        # urls used to spawn spider-instances\n",
    "        global urls\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # Set the maximum depth\n",
    "    maxdepth = 10\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\" Main method that parse downloaded pages. \"\"\"\n",
    "        # Set defaults for the first page that won't have any meta information\n",
    "        start_url = ''\n",
    "        from_url = ''\n",
    "        from_text = ''\n",
    "        depth = 0\n",
    "        # Extract the meta information from the response, if any\n",
    "        if 'start'  in response.meta: start_url = response.meta['start']\n",
    "        if 'from'   in response.meta: from_url  = response.meta['from']\n",
    "        if 'text'   in response.meta: from_text = response.meta['text']\n",
    "        if 'depth'  in response.meta: depth     = response.meta['depth']\n",
    "        \n",
    "        # set start url for crawler\n",
    "        if depth == 0:\n",
    "            start_url = response.url\n",
    "\n",
    "        # get all article links\n",
    "        if start_url[-1] == response.xpath('//div[@id=\"mw-pages\"]/div/div/div[1]/h3/text()').get(): # chek that current letter is on page\n",
    "\n",
    "            # change xpath to: ('//div[@id=\"mw-pages\"]/div/div/div[1]/ul/li[1]/a/@href') <- 1   page\n",
    "            # change xpath to: ('//div[@id=\"mw-pages\"]/div/div/div[1]/ul/li/a/@href')    <- 200 pages\n",
    "            articles = response.xpath('//div[@id=\"mw-pages\"]/div/div/div[1]/ul/li/a/@href').getall()\n",
    "            for a in articles:\n",
    "                url = urljoin(response.url, a)\n",
    "                yield scrapy.Request(url, callback=self.parse_article)\n",
    "\n",
    "        ### DEBUG printing - used for locating spider behavior ###\n",
    "        print(\"### DEBUG DUMP STEP:\", depth, response.url, '<-', from_url, from_text, \"END ###\",\n",
    "              \"### DEBUG DUMP start_url:\", start_url[-1], response.xpath('//div[@id=\"mw-pages\"]/div/div/div[1]/h3/text()').get(),\"char_page END ###\")\n",
    "\n",
    "        # get nex_page only if maximum depth has not be reached and current char is still on page\n",
    "        if depth < self.maxdepth and start_url[-1] == response.xpath('//div[@id=\"mw-pages\"]/div/div/div[1]/h3/text()').get():\n",
    "            next_page = response.xpath('//div[@id=\"mw-pages\"]/a[2]') # location of next link\n",
    "            next_page_text = next_page.xpath(\"text()\").get()\n",
    "            next_page_link = next_page.xpath(\"@href\").get()\n",
    "            print(\"### DEBUG DUMP next_page:\", next_page, \"END ###\")\n",
    "\n",
    "            if next_page_link is not None:\n",
    "                request = response.follow(next_page_link, callback=self.parse)\n",
    "                # Meta information: URL of the current page\n",
    "                request.meta['from'] = response.url\n",
    "                # Meta information: text of the link\n",
    "                request.meta['text'] = next_page_text\n",
    "                # Meta information: depth of the link\n",
    "                request.meta['depth'] = depth + 1\n",
    "                # Meta information: start page for current crawler\n",
    "                request.meta['start'] = start_url\n",
    "                yield request\n",
    "\n",
    "    # get article content - using scrapy itemLoader and Items\n",
    "    def parse_article(self, response):\n",
    "        l = ItemLoader(item=articleItem(), response=response) # create itemloader l - following is adding to Fields\n",
    "        l.add_xpath('title',        '//*[@id=\"firstHeading\"]/text()')\n",
    "        l.add_xpath('publish_date', '//div[@id=\"catlinks\"]/div[@id=\"mw-normal-catlinks\"]/ul/li/a/text()',re='(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)[\\s,]*(?:\\d{1,2})[\\s,]*(?:\\d{4})')\n",
    "        l.add_xpath('content',      '//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/p/text()|//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/p/child::a/text()|//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/ul/li/text()', Join(' '))\n",
    "        l.add_xpath('categories',   '//div[@id=\"catlinks\"]/div[@id=\"mw-normal-catlinks\"]/ul/li/a/text()')\n",
    "        l.add_xpath('sources_url',  '//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/ul/li/span/a/@href')\n",
    "        l.add_xpath('about_sources_wiki_url', '//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/ul/li/span/i/span/a/@href')\n",
    "        l.add_value('article_url', response.request.url)\n",
    "        l.add_value('scraped_at', (datetime.today().strftime('%Y-%m-%d')) )\n",
    "        yield l.load_item() # could use return/yield - no idea what changesw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea for the final Spider is that we generate based on group_nr our entry-point websites and generate a list of all article urls to follow afterwards we follow all these links we follow a link to the next 200 link-page and start again. By using metadata parameters we can inform the spider about where it has been where it is going and how deep it has gone. The last part about the spider to talk about is the function 'parse_article' where we make use of scrapy's item containers which help us deal with missing data in the case of a broken link or other unforeseen circumstances.\n",
    "\n",
    "The datafields we ended up collecting were:\n",
    "\n",
    "'article_url'\n",
    "\n",
    "'title' = title of the article inside the page\n",
    "\n",
    "'categories' = categories assigned to the article\n",
    "\n",
    "'content' = main text of article\n",
    "\n",
    "'publish_date'\n",
    "\n",
    "'scraped_at' = date of scraping by our spider\n",
    "\n",
    "'sources_url' = urls for all individual pages used as a source\n",
    "\n",
    "'about_sources_wiki_url' = url to wikipage about a given source ie. BBC\n",
    "\n",
    "\n",
    "We felt that these would be of use for further analyses for another group as well as being general enough that most articles would have an entry for all fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for cleaning and organizing the raw article data into what we save in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "# libraries we might not need\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the following code\n",
    "to use the whole document you only need one file specified by filepath for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 16) <- size of dataframe \n",
      "\n",
      "CPU times: user 32.9 s, sys: 18.6 s, total: 51.5 s\n",
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6803</td>\n",
       "      <td>322758</td>\n",
       "      <td>zerohedge.com</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>https://www.zerohedge.com/news/2014-12-17/new-...</td>\n",
       "      <td>Submitted by Mike Krieger via Liberty Blitzkri...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>59% Of Americans Support Post-9/11 Torture – P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>\"By an almost 2-1 margin, or 59-to-31 percent,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9359</td>\n",
       "      <td>1017129</td>\n",
       "      <td>shadowproof.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://shadowproof.com/2012/09/18/oregons-mar...</td>\n",
       "      <td>Measure 80, which would legalize marijuana for...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Oregon’s Marijuana Legalization Measure Traili...</td>\n",
       "      <td>Jon Walker, Jonathan Walker Grew Up In New Jer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4476</td>\n",
       "      <td>577340</td>\n",
       "      <td>us.blastingnews.com</td>\n",
       "      <td>satire</td>\n",
       "      <td>http://us.blastingnews.com/news/2017/05/photo/...</td>\n",
       "      <td>This website uses profiling (non technical) co...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Photogallery - Donald Trump 'gaining weight' o...</td>\n",
       "      <td>Blasting News, P. Ghose, M. Singh, W. Camille,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Donald Trump', 'Russian Scandal', 'Gaining W...</td>\n",
       "      <td>Article's photos Donald Trump 'gaining weight'...</td>\n",
       "      <td>4062 followers Duggar Family, 3756 followers S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>695601</td>\n",
       "      <td>breitbart.com</td>\n",
       "      <td>political</td>\n",
       "      <td>http://www.breitbart.com/author/joseph-c-phill...</td>\n",
       "      <td>It is said that when Alexander the Great visit...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Joseph C. Phillips, Author at Breitbart</td>\n",
       "      <td>Joseph C. Phillips</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Joseph C. Phillips, Author at Breitbart - Page...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1150037</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>https://www.express.co.uk/sport/football/61716...</td>\n",
       "      <td>Barca, along with Arsenal and Liverpool, have ...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Arsenal and Liverpool target to snub bargain £...</td>\n",
       "      <td>Joe Short</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>EZEQUIEL LAVEZZI will not be joining Barcelona...</td>\n",
       "      <td>Wenger would likely be tempted by a cheap deal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id               domain        type  \\\n",
       "6803   322758        zerohedge.com  conspiracy   \n",
       "9359  1017129      shadowproof.com     unknown   \n",
       "4476   577340  us.blastingnews.com      satire   \n",
       "988    695601        breitbart.com   political   \n",
       "155   1150037        express.co.uk       rumor   \n",
       "\n",
       "                                                    url  \\\n",
       "6803  https://www.zerohedge.com/news/2014-12-17/new-...   \n",
       "9359  https://shadowproof.com/2012/09/18/oregons-mar...   \n",
       "4476  http://us.blastingnews.com/news/2017/05/photo/...   \n",
       "988   http://www.breitbart.com/author/joseph-c-phill...   \n",
       "155   https://www.express.co.uk/sport/football/61716...   \n",
       "\n",
       "                                                content  \\\n",
       "6803  Submitted by Mike Krieger via Liberty Blitzkri...   \n",
       "9359  Measure 80, which would legalize marijuana for...   \n",
       "4476  This website uses profiling (non technical) co...   \n",
       "988   It is said that when Alexander the Great visit...   \n",
       "155   Barca, along with Arsenal and Liverpool, have ...   \n",
       "\n",
       "                      scraped_at                 inserted_at  \\\n",
       "6803  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "9359  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "4476  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "988   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "155   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                      updated_at  \\\n",
       "6803  2018-02-02 01:19:41.756664   \n",
       "9359  2018-02-02 01:19:41.756664   \n",
       "4476  2018-02-02 01:19:41.756664   \n",
       "988   2018-02-02 01:19:41.756664   \n",
       "155   2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                  title  \\\n",
       "6803  59% Of Americans Support Post-9/11 Torture – P...   \n",
       "9359  Oregon’s Marijuana Legalization Measure Traili...   \n",
       "4476  Photogallery - Donald Trump 'gaining weight' o...   \n",
       "988             Joseph C. Phillips, Author at Breitbart   \n",
       "155   Arsenal and Liverpool target to snub bargain £...   \n",
       "\n",
       "                                                authors  keywords  \\\n",
       "6803                                                NaN       NaN   \n",
       "9359  Jon Walker, Jonathan Walker Grew Up In New Jer...       NaN   \n",
       "4476  Blasting News, P. Ghose, M. Singh, W. Camille,...       NaN   \n",
       "988                                  Joseph C. Phillips       NaN   \n",
       "155                                           Joe Short       NaN   \n",
       "\n",
       "                                          meta_keywords  \\\n",
       "6803                                               ['']   \n",
       "9359                                               ['']   \n",
       "4476  ['Donald Trump', 'Russian Scandal', 'Gaining W...   \n",
       "988                                                ['']   \n",
       "155                                                ['']   \n",
       "\n",
       "                                       meta_description  \\\n",
       "6803  \"By an almost 2-1 margin, or 59-to-31 percent,...   \n",
       "9359                                                NaN   \n",
       "4476  Article's photos Donald Trump 'gaining weight'...   \n",
       "988   Joseph C. Phillips, Author at Breitbart - Page...   \n",
       "155   EZEQUIEL LAVEZZI will not be joining Barcelona...   \n",
       "\n",
       "                                                   tags  summary  source  \n",
       "6803                                                NaN      NaN     NaN  \n",
       "9359                                                NaN      NaN     NaN  \n",
       "4476  4062 followers Duggar Family, 3756 followers S...      NaN     NaN  \n",
       "988                                                 NaN      NaN     NaN  \n",
       "155   Wenger would likely be tempted by a cheap deal...      NaN     NaN  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# imports a random sample of size s from csv-file as a pandas dataframe\n",
    "# pandas using python 3.X uses utf-8 encoding\n",
    "\n",
    "# usage: specify file location, sample size and seed(used by random)\n",
    "filepath = '1mioraw.csv'\n",
    "#filepath = 'news_sample.csv' # <- overwrite for setup\n",
    "s = 1000000                    # desired sample size(seems to have slack ie. not exact)\n",
    "seed = 1                     # seed used by Pseudorandom number generator\n",
    "\n",
    "# init dataframe with specified values\n",
    "df = pd.read_csv(filepath, index_col = [0]).sample(n=s, random_state=seed)\n",
    "\n",
    "# visual output\n",
    "#print(df.shape, '<- size of dataframe \\n')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = pd.to_numeric(df['id'], errors = 'coerce', downcast = 'integer')\n",
    "df.drop_duplicates(subset = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999934, 15)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['id']).set_index('id')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning for values out of bounds of DataBase requirements etc.\n",
    "df.index = df.index.astype(int)\n",
    "longAuthors = df[df['authors'].str.len() > 255].index\n",
    "df.drop(longAuthors, inplace = True)\n",
    "longTags = df[df['tags'].str.len() > 1000].drop_duplicates(subset = 'tags', keep = 'first').index\n",
    "df.drop(longTags, inplace = True)\n",
    "longMetaD = df[df['meta_description'].str.len() > 10000].index\n",
    "df.drop(longMetaD, inplace = True)\n",
    "df['authors'] = df['authors'].replace(np.nan, 'NoAuthor', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regexEmail = r\"[a-zA-Z_-]+@[a-zA-Z_-]+(\\.[a-zA-Z]{2,4}){1,3}\"\n",
    "df.content = df.content.replace(to_replace=regexEmail, value='<EMAIL>', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regexURL= r\"(?:https?:\\/\\/)?(?:www\\.)?([^@\\s]+\\.[a-zA-Z]{2,4})[^\\s]*\"\n",
    "df.content = df.content.replace(to_replace=regexURL, value='<URL>', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexDoubleSpace = r\"(\\s{2,})|\\n\"\n",
    "df.content = df.content.replace(to_replace=regexDoubleSpace, value=' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexDate = r\"(((19[7-9]\\d|20\\d{2})|(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|(nov|dec)(?:ember)?)|(([12][0-9])|(3[01])|(0?[1-9])))[\\/. \\-,\\n]){2,3}\"\n",
    "df.content = df.content.replace(to_replace=regexDate, value='<DATE>', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regexNum = r\"(\\s)\\$?(?:[\\d,.-])+\"\n",
    "df.content = df.content.replace(to_replace=regexNum, value='<NUM>', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data-tables: [name]-uniq / relational-tables: [name]_in\n",
    "creating csv-files for database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where to save all csv-files\n",
    "path = 'database_csv_in2/'\n",
    "\n",
    "# create temporary dataframe and use article id as index \n",
    "out_df = pd.DataFrame({'id':df.index})\n",
    "out_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### types_uniq - data-table ###\n",
    "type_array = df.type.unique() # get array of unique types\n",
    "type_df = pd.DataFrame({'id': np.arange(type_array.size), 'name':type_array})\n",
    "\n",
    "# write file and free memory\n",
    "type_df.to_csv(path + 'type_clean.csv', index=False, header=True)\n",
    "#del type_array\n",
    "#del type_df # tmp delete later\n",
    "\n",
    "# create dict with type_name as key - [swap type with type_id]\n",
    "type_name_as_key_df = type_df.set_index('name')\n",
    "type_dict = type_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace type with tag id and create new column\n",
    "type_id = np.array([type_dict[key] for key in df['type'].to_numpy()])\n",
    "df['type_id'] =type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tags_uniq - data-table ###\n",
    "\n",
    "# creates list of list but formaly it is a pd.series of lists\n",
    "tags_series_of_lists = df.tags.dropna().str.split(', ') # -> ', ' not ','\n",
    "\n",
    "if not 'tags' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'tags', value = tags_series_of_lists)\n",
    "\n",
    "# flattern tags_series_of_lists to a set(ie. unique values only)\n",
    "tags_list = list(set([item for sublist in tags_series_of_lists for item in sublist]))\n",
    "\n",
    "# create dataframe\n",
    "tags_df = pd.DataFrame({'id': np.arange(len(tags_list)), 'name':tags_list})\n",
    "\n",
    "# write file and free memory\n",
    "tags_df.to_csv(path + 'tags_clean.csv', index=False, header=True)\n",
    "del tags_series_of_lists\n",
    "del tags_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tags_in - relational-table ###\n",
    "\n",
    "# get all pairs of article_id and tags in a article (for all articles)\n",
    "articles_id_tags_name_pairs_df = out_df.tags.dropna().explode().drop_duplicates(keep = 'first')\n",
    "\n",
    "# split tags_name and articles_id\n",
    "articles_id_array = articles_id_tags_name_pairs_df.index.to_numpy()\n",
    "tags_name_array = articles_id_tags_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap tags with tags_id]\n",
    "tags_name_as_key_df = tags_df.set_index('name')\n",
    "tags_dict = tags_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace tags with tag id\n",
    "tags_id = np.array([tags_dict[key] for key in tags_name_array])\n",
    "\n",
    "# create dataframe\n",
    "tags_in_df = pd.DataFrame(data=articles_id_array, index=tags_id, columns=['article_id'])\n",
    "tags_in_df.index.name='tags_id'\n",
    "\n",
    "# write file and free memory\n",
    "tags_in_df.to_csv(path + 'tags_in.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### authors-uniq - data-table ###\n",
    "\n",
    "# creates list of list but formaly it is a pd.series of lists\n",
    "authors_series_of_lists = df.authors.str.split(',') # -> ',' not ', '\n",
    "\n",
    "if not 'authors' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'authors', value = authors_series_of_lists)\n",
    "\n",
    "# flattern authors_series_of_lists to a set(ie. unique values only)\n",
    "authors_list = list(set([item for sublist in authors_series_of_lists for item in sublist]))\n",
    "\n",
    "# create dataframe\n",
    "authors_df = pd.DataFrame({'id': np.arange(len(authors_list)), 'name':authors_list})\n",
    "\n",
    "# write file and free memory\n",
    "authors_df.to_csv(path + 'authors_clean.csv', index=False, header=True)\n",
    "#del authors_series_of_lists\n",
    "#del authors_list\n",
    "#del authors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors_in - relational-table ###\n",
    "#for i in out_df.index:\n",
    "    #out_df.author[i] = list(set(out_df.authors[i]))\n",
    "# get all pairs of article_id and authors in a article (for all articles)\n",
    "articles_id_authors_name_pairs_df = out_df.authors.dropna().explode()\n",
    "\n",
    "# split authors_name and articles_id\n",
    "articles_id_array = articles_id_authors_name_pairs_df.index.to_numpy()\n",
    "authors_name_array = articles_id_authors_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap authors with authors_id]\n",
    "authors_name_as_key_df = authors_df.set_index('name')\n",
    "authors_dict = authors_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace authors with tag id\n",
    "authors_id = np.array([authors_dict[key] for key in authors_name_array])\n",
    "\n",
    "# create dataframe\n",
    "authors_in_df = pd.DataFrame(data=articles_id_array, index=authors_id, columns=['article_id'])\n",
    "authors_in_df.index.name='authors_id'\n",
    "\n",
    "# write file and free memory\n",
    "authors_in_df.to_csv(path + 'authors_in.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### domains-uniq - data-table ###\n",
    "domain_array = df.domain.unique() # get array of unique domains\n",
    "domain_df = pd.DataFrame({'id': np.arange(domain_array.size), 'name':domain_array})\n",
    "\n",
    "# write file and free memory\n",
    "domain_df.to_csv(path + 'domain_name_clean.csv', index=False, header=True)\n",
    "#del domain_array\n",
    "#del domain_df\n",
    "\n",
    "# create dict with domain_name as key - [swap domain with domain_id]\n",
    "domain_name_as_key_df = domain_df.set_index('name')\n",
    "domain_dict = domain_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace domain with tag id and create new column\n",
    "domain_id = np.array([domain_dict[key] for key in df['domain'].to_numpy()])\n",
    "df['domain_id'] =domain_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta_keywords_uniq - data-table ###\n",
    "\n",
    "# use regex to remove string-padding\n",
    "regex = r\" *['\\\"\\[\\]]+\"\n",
    "meta_keywords_series = df.meta_keywords.replace(to_replace=regex, value='', regex=True).str.split(',')\n",
    "#meta_keywords_series = meta_keywords_series.replace(r'', np.NaN)\n",
    "\n",
    "if not 'meta_keywords' in out_df: ### tmp need another method ###\n",
    "    out_df.insert(0,column = 'meta_keywords', value = meta_keywords_series)\n",
    "\n",
    "# create array of unique\n",
    "meta_keywords_set = meta_keywords_series.explode().unique()\n",
    "\n",
    "# create dataframe\n",
    "meta_keywords_df = pd.DataFrame({'id': np.arange(len(meta_keywords_set)), 'name':meta_keywords_set})\n",
    "\n",
    "# write file and free memory\n",
    "meta_keywords_df.to_csv(path + 'meta_keywords_clean.csv', index=False, header=True)\n",
    "#del meta_keywords_series\n",
    "#del meta_keywords_set\n",
    "#del meta_keywords_list\n",
    "#del meta_keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta_keywords_in - relational-table ###\n",
    "\n",
    "# get all pairs of article_id and meta_keywords in a article (for all articles)\n",
    "articles_id_meta_keywords_name_pairs_df = out_df.meta_keywords.dropna().explode()\n",
    "\n",
    "# split meta_keywords_name and articles_id\n",
    "articles_id_array = articles_id_meta_keywords_name_pairs_df.index.to_numpy()\n",
    "meta_keywords_name_array = articles_id_meta_keywords_name_pairs_df.to_numpy()\n",
    "\n",
    "# create dict with tag_name as key - [swap meta_keywords with meta_keywords_id]\n",
    "meta_keywords_name_as_key_df = meta_keywords_df.set_index('name')\n",
    "meta_keywords_dict = meta_keywords_name_as_key_df['id'].to_dict()\n",
    "\n",
    "# replace meta_keywords with tag id\n",
    "meta_keywords_id = np.array([meta_keywords_dict[key] for key in meta_keywords_name_array])\n",
    "\n",
    "# create dataframe\n",
    "meta_keywords_in_df = pd.DataFrame(data=articles_id_array, index=meta_keywords_id, columns=['article_id'])\n",
    "\n",
    "meta_keywords_in_df.index.name='meta_keywords_id'\n",
    "meta_keywords_in_df.reset_index(inplace = True)\n",
    "meta_keywords_in_df.drop_duplicates(subset = ['meta_keywords_id', 'article_id'], keep = 'first', inplace = True)\n",
    "# write file and free memory\n",
    "meta_keywords_in_df.to_csv(path + 'meta_keywords_in.csv', index=False, header=True)\n",
    "#meta_keywords_in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### article clean ###\n",
    "\n",
    "df[['domain_id', 'type_id', \"url\", \"content\", \"title\", \"meta_description\", \"scraped_at\",  \"updated_at\", \"inserted_at\"]].to_csv(path + 'article_clean.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
